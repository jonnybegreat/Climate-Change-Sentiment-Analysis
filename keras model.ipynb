{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "\n",
    "#modules for n-gram model\n",
    "#warnings ot surpressed\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import models\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K #backend to use outside metrics on n-gram model\n",
    "\n",
    "#Bi-LSTM model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('Train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(sentence):\n",
    "#     tokens = nltk.word_tokenize(sentence)  # now using tweet tokenizer\n",
    "    tknzr = TweetTokenizer()\n",
    "    tokens = tknzr.tokenize(sentence)\n",
    "    tokens = [token for token in tokens if (token not in stopwords and len(token) > 1)]\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    return (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>It's not like we lack evidence of anthropogeni...</td>\n",
       "      <td>126103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @RawStory: Researchers say we have three ye...</td>\n",
       "      <td>698562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>#TodayinMaker# WIRED : 2016 was a pivotal year...</td>\n",
       "      <td>573736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @SoyNovioDeTodas: It's 2016, and a racist, ...</td>\n",
       "      <td>466954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>Worth a read whether you do or don't believe i...</td>\n",
       "      <td>425577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15814</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @ezlusztig: They took down the material on ...</td>\n",
       "      <td>22001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15815</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @washingtonpost: How climate change could b...</td>\n",
       "      <td>17856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15816</th>\n",
       "      <td>0</td>\n",
       "      <td>notiven: RT: nytimesworld :What does Trump act...</td>\n",
       "      <td>384248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15817</th>\n",
       "      <td>-1</td>\n",
       "      <td>RT @sara8smiles: Hey liberals the climate chan...</td>\n",
       "      <td>819732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15818</th>\n",
       "      <td>0</td>\n",
       "      <td>RT @Chet_Cannon: .@kurteichenwald's 'climate c...</td>\n",
       "      <td>806319</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15818 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentiment                                            message  tweetid\n",
       "1              1  It's not like we lack evidence of anthropogeni...   126103\n",
       "2              2  RT @RawStory: Researchers say we have three ye...   698562\n",
       "3              1  #TodayinMaker# WIRED : 2016 was a pivotal year...   573736\n",
       "4              1  RT @SoyNovioDeTodas: It's 2016, and a racist, ...   466954\n",
       "5              1  Worth a read whether you do or don't believe i...   425577\n",
       "...          ...                                                ...      ...\n",
       "15814          1  RT @ezlusztig: They took down the material on ...    22001\n",
       "15815          2  RT @washingtonpost: How climate change could b...    17856\n",
       "15816          0  notiven: RT: nytimesworld :What does Trump act...   384248\n",
       "15817         -1  RT @sara8smiles: Hey liberals the climate chan...   819732\n",
       "15818          0  RT @Chet_Cannon: .@kurteichenwald's 'climate c...   806319\n",
       "\n",
       "[15818 rows x 3 columns]"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def clean_funct(text):\n",
    "    tweet = text\n",
    "    \n",
    "    def lemm(word):\n",
    "        result = wordnet.morphy(word)\n",
    "        if result is None:\n",
    "            return word\n",
    "        else: return result\n",
    "    \n",
    "    token = TweetTokenizer.tokenize(text = tweet)\n",
    "    token = [t for t in token if (token not in  stop and token not in stop2 and len(token) > 1)]\n",
    "    token = [lemm(t) for t in token]\n",
    "    return token\n",
    "\n",
    "    \n",
    "def tokenizer(text3):\n",
    "    text_nonum = re.sub(r'\\d+', '', text3)\n",
    "    text_nopunct = \"\".join([char.lower() for char in text_nonum if char not in string.punctuation]) \n",
    "    text_no_doublespace = re.sub('\\s+', ' ', text_nopunct).strip()\n",
    "    token= word_tokenize(text_no_doublespace)\n",
    "    return token\n",
    "'''\n",
    "#create mention list\n",
    "def m_fin(text):\n",
    "    mentions = []\n",
    "    men_f = re.compile(r'@([a-zA-Z0-9-z#_]+)')\n",
    "    for word in text:\n",
    "         mentions = men_f.findall(text)\n",
    "    return mentions\n",
    "\n",
    "#create hashtag list\n",
    "def hash_finder(tweet):\n",
    "    hashes = []\n",
    "    finder = re.compile(r'#([a-zA-Z0-9-z#_]+)')\n",
    "    for word in tweet:\n",
    "        hashes = finder.findall(tweet)\n",
    "    return hashes\n",
    "\n",
    "#create url list\n",
    "def url_fr(text):\n",
    "    url = re.compile(r\"([https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]\\.[^\\s]{2,}])\")\n",
    "    for link in text:\n",
    "        found = url.findall(text)\n",
    "    return found\n",
    "\n",
    "#precleaning\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]\\.[^\\s]{2,})\", \"\", text)\n",
    "\n",
    "    # remove numbers\n",
    "    text_nonum = re.sub(r'\\d+', '', text)\n",
    "    # remove \n",
    "    text_nomen = re.sub(r'(@[a-zA-Z0-9-z#_]+)','', text_nonum)\n",
    "    text_nohash = re.sub(r'(#[a-zA-Z0-9-z#_]+)', '', text_nomen)\n",
    "    # remove punctuations and convert characters to lower case\n",
    "    text_nopunct = \"\".join([char.lower() for char in text_nohash if char not in string.punctuation]) \n",
    "    # substitute multiple whitespace with single whitespace\n",
    "    # Also, removes leading and trailing whitespaces\n",
    "    text_no_doublespace = re.sub('\\s+', ' ', text_nopunct).strip()\n",
    "    return text_no_doublespace\n",
    "\n",
    "#lemmetize\n",
    "def lemm(word):\n",
    "        result = wordnet.morphy(word)\n",
    "        if result is None:\n",
    "            return word\n",
    "        else: return result\n",
    "#remove stop words\n",
    "def tokenizer_2(tokenized):\n",
    "    tokenized = word_tokenize(tokenized)\n",
    "    tokenized = [t for t in tokenized if (t not in  stop and t not in stop2 and len(t) > 1)]\n",
    "    tokenized = [lemm(t) for t in tokenized]\n",
    "    return tokenized\n",
    "\n",
    "#tweet word length\n",
    "def counter(text):\n",
    "    # remove numbers\n",
    "    count = len(text)\n",
    "    return count\n",
    "\n",
    "train['mentions'] = train['message'].apply(m_fin)\n",
    "train['url'] = train['message'].apply(url_fr)\n",
    "train['len'] = train['message'].apply(counter)\n",
    "train['tags'] = train['message'].apply(hash_finder)\n",
    "train['cleaned'] = train['message'].apply(clean_text)\n",
    "train['tokens'] = train['cleaned'].apply(tokenizer_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = Tokenizer(num_words = 20000, split = ' ',  oov_token='<unw>', filters = ' ')\n",
    "token.fit_on_texts(train['tokens'].values)\n",
    "x = token.texts_to_sequences(train['cleaned'].values)\n",
    "x = pad_sequences(x, 18)#selected 52 because I havent checked the max words in tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,  6222,    18, ...,   444,     1,     1],\n",
       "       [    0,     0,     0, ...,  1251,     5,     6],\n",
       "       [    0,     0,     4, ...,     1,     1,   663],\n",
       "       ...,\n",
       "       [13868,     4, 13869, ...,  2435,     1,     1],\n",
       "       [  318,     1,     1, ...,  3359,     1,  6202],\n",
       "       [    0,     0,     0, ...,  4301,     1, 13870]])"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embeddings with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict = {}\n",
    "\n",
    "\n",
    "with open(\"glove.twitter.27B.50d.txt\", 'r') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.array(values[1:])\n",
    "        embeddings_dict[word] = vector\n",
    "\n",
    "\n",
    "embedding_path = \"glove.twitter.27B.50d.txt\" \n",
    "\n",
    "def get_word2vec(file_path):\n",
    "    file = open(embedding_path, \"r\", encoding = 'utf-8')\n",
    "    if (file):\n",
    "        word2vec = dict()\n",
    "        split = file.read().splitlines()\n",
    "        for line in split:\n",
    "            key = line.split(' ',1)[0] # the first word is the key\n",
    "            value = np.array([float(val) for val in line.split(' ')[1:]])\n",
    "            word2vec[key] = value\n",
    "        return (word2vec)\n",
    "w2v = get_word2vec(embedding_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "class MeanVect(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(next(iter(word2vec.values())))\n",
    "        \n",
    "    # pass a word list\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in (X)\n",
    "        ])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.pop('tweetid')\n",
    "train.pop('message')\n",
    "#trial. DONT USE ON LIST!!\n",
    "t_vector = TfidfVectorizer(ngram_range=(1, 2),\n",
    "                            \n",
    "                           strip_accents = 'unicode',\n",
    "                           decode_error = 'replace',\n",
    "                           analyzer = 'word',\n",
    "                           min_df = .1, \n",
    "                           max_df = .50,\n",
    "                           stop_words = stop)\n",
    "\n",
    "token_vect=  t_vector.fit_transform(train['tokens'].values)\n",
    "hash_vect = t_vector.fit_transform(train['tags'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['message'] = train['message'].apply(lambda x: ' '.join(x.lower() for x in x.split()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15819\n"
     ]
    }
   ],
   "source": [
    "#Number of samples\n",
    "sample_tot = len(train.index.values)\n",
    "print(sample_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, -1}\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "#Number of classes\n",
    "sent_val = set(train['sentiment'].values)\n",
    "sent_count = len(set(train['sentiment'].values))\n",
    "print(sent_val)\n",
    "print(sent_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment 0 observations :2353\n",
      "Sentiment 1 observations :8530\n",
      "Sentiment 2 observations :3640\n",
      "Sentiment -1 observations :1296\n"
     ]
    }
   ],
   "source": [
    "#number of samples per class\n",
    "for i in set(train['sentiment'].values):\n",
    "    count = 0\n",
    "    for x in train['sentiment']:\n",
    "        if x == i:\n",
    "            count+=1\n",
    "    print(\"Sentiment \"+ str(i)+' '+ \"observations :\"+ str(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.0\n"
     ]
    }
   ],
   "source": [
    "#median words per tweet\n",
    "def word_count(text):\n",
    "    num_words = [len(s.split()) for s in text]\n",
    "    return np.median(num_words)\n",
    "words = word_count(train['message'])\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#distr. tweet lengths\n",
    "def sample_plotter(text2):\n",
    "    plt.hist([len(s) for s in text2], 50)\n",
    "    plt.xlabel('tweet length')\n",
    "    plt.ylabel('nuber of tweets')\n",
    "    plt.title('Sample length distribution')\n",
    "    plt.show()\n",
    "\n",
    "sample_plotter(train['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>PolySciMajor EPA chief doesn't think carbon di...</td>\n",
       "      <td>625221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>It's not like we lack evidence of anthropogeni...</td>\n",
       "      <td>126103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @RawStory: Researchers say we have three ye...</td>\n",
       "      <td>698562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>#TodayinMaker# WIRED : 2016 was a pivotal year...</td>\n",
       "      <td>573736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @SoyNovioDeTodas: It's 2016, and a racist, ...</td>\n",
       "      <td>466954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  tweetid\n",
       "0          1  PolySciMajor EPA chief doesn't think carbon di...   625221\n",
       "1          1  It's not like we lack evidence of anthropogeni...   126103\n",
       "2          2  RT @RawStory: Researchers say we have three ye...   698562\n",
       "3          1  #TodayinMaker# WIRED : 2016 was a pivotal year...   573736\n",
       "4          1  RT @SoyNovioDeTodas: It's 2016, and a racist, ...   466954"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "879.0\n"
     ]
    }
   ],
   "source": [
    "#word sample ratios\n",
    "word_sample_ratio = sample_tot/words\n",
    "print(round(word_sample_ratio, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentiment prep\n",
    "def adder(text):\n",
    "    num = int(text)\n",
    "    num = num + 1\n",
    "    return num\n",
    "train['sentiment'] = train['sentiment'].apply(adder)\n",
    "train['sentiment'] = train['sentiment'].replace(3, 2)\n",
    "train['sentiment']\n",
    "y = train['sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop words\n",
    "stop2 = text.ENGLISH_STOP_WORDS\n",
    "stop = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.iloc[:, 1].values\n",
    "y = train.iloc[:, 0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "f = open(os.path.join('../glove-global-vectors-for-word-representation', 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test and train\n",
    "from sklearn.model_selection import train_test_split\n",
    "X, validation_x, Y, validation_y = train_test_split(x, y, test_size=0.30, shuffle = True, random_state=32)\n",
    "train_text, test_text, train_val, test_val = train_test_split(X, Y, test_size=0.20, shuffle = True, random_state=32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-346-d76d820bda0b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"glove.twitter.27B.100d.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\codecs.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, errors)\u001b[0m\n\u001b[0;32m    307\u001b[0m     \u001b[0mbyte\u001b[0m \u001b[0msequences\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m     \"\"\"\n\u001b[1;32m--> 309\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'strict'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    310\u001b[0m         \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m         \u001b[1;31m# undecoded input that is kept between calls to decode()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "embeddings_dict = {}\n",
    "\n",
    "\n",
    "with open(\"glove.twitter.27B.100d.txt\", 'r', encoding = 'utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.array(values[1:])\n",
    "        embeddings_dict[word] = vector\n",
    "\n",
    "\n",
    "embedding_path = \"glove.twitter.27B.100d.txt\" \n",
    "\n",
    "def get_word2vec(file_path):\n",
    "    file = open(embedding_path, \"r\", encoding = 'utf-8')\n",
    "    if (file):\n",
    "        word2vec = dict()\n",
    "        split = file.read().splitlines()\n",
    "        for line in split:\n",
    "            key = line.split(' ',1)[0] # the first word is the key\n",
    "            value = np.array([float(val) for val in line.split(' ')[1:]])\n",
    "            word2vec[key] = value\n",
    "        return (word2vec)\n",
    "w2v = get_word2vec(embedding_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1193514 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open(\"glove.twitter.27B.100d.txt\", 'r', encoding = 'utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13870 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = token.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = min(20000, len(word_index)) + 1\n",
    "em_mat = np.zeros((num_words, 100))\n",
    "# for each word in out tokenizer lets try to find that work in our w2v model\n",
    "for word, i in word_index.items():\n",
    "    if i > 20000:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # we found the word - add that words vector to the matrix\n",
    "        em_mat[i] = embedding_vector\n",
    "    else:\n",
    "        # doesn't exist, assign a random vector\n",
    "        em_mat[i] = np.random.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-320-0a2b837f9cb0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m                            \u001b[0mmax_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m.50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m                            stop_words = stop)\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mx_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt_vector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mx_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt_vector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1838\u001b[0m         \"\"\"\n\u001b[0;32m   1839\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1840\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1841\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1198\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m-> 1199\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m   1200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1201\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1109\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     67\u001b[0m     \"\"\"\n\u001b[0;32m     68\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maccent_function\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccent_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "#tokenizing into uni+bi-grams and vectorizing\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "\n",
    "t_vector = TfidfVectorizer(ngram_range=(1, 2),\n",
    "                            \n",
    "                           strip_accents = 'unicode',\n",
    "                           decode_error = 'replace',\n",
    "                           analyzer = 'word',\n",
    "                           min_df = .1, \n",
    "                           max_df = .50,\n",
    "                           stop_words = stop)\n",
    "x_train = t_vector.fit_transform(train_text)\n",
    "x_val = t_vector.transform(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting top 20 000 Features for n-gram model\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "b_vect = SelectKBest(f_classif, k = min(20000, train_text.shape[1]))\n",
    "b_vect.fit(train_text, train_val)\n",
    "train_text = b_vect.transform(train_text).astype('float32')\n",
    "x_train = train_text\n",
    "test_text = b_vect.transform(test_text).astype('float32')\n",
    "x_val = test_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.000e+00, 0.000e+00, 0.000e+00, ..., 3.000e+00, 1.460e+02,\n",
       "        1.000e+00],\n",
       "       [4.000e+00, 7.710e+02, 7.710e+02, ..., 7.710e+02, 7.710e+02,\n",
       "        2.744e+03],\n",
       "       [1.000e+00, 1.000e+00, 3.100e+01, ..., 1.000e+00, 1.300e+01,\n",
       "        2.467e+03],\n",
       "       ...,\n",
       "       [1.000e+00, 4.880e+02, 1.000e+00, ..., 1.000e+00, 4.460e+02,\n",
       "        1.000e+00],\n",
       "       [2.524e+03, 3.566e+03, 1.000e+00, ..., 1.000e+00, 3.630e+02,\n",
       "        4.789e+03],\n",
       "       [0.000e+00, 0.000e+00, 0.000e+00, ..., 1.000e+00, 2.000e+00,\n",
       "        3.000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.000e+00, 7.070e+02, 1.570e+02, ..., 1.000e+00, 1.000e+00,\n",
       "        1.000e+00],\n",
       "       [0.000e+00, 0.000e+00, 4.000e+00, ..., 1.000e+00, 3.200e+01,\n",
       "        7.000e+00],\n",
       "       [0.000e+00, 0.000e+00, 0.000e+00, ..., 1.000e+00, 1.000e+00,\n",
       "        3.150e+02],\n",
       "       ...,\n",
       "       [1.000e+00, 1.800e+01, 1.000e+00, ..., 2.000e+00, 3.000e+00,\n",
       "        4.275e+03],\n",
       "       [0.000e+00, 0.000e+00, 0.000e+00, ..., 1.000e+00, 3.000e+00,\n",
       "        1.000e+00],\n",
       "       [1.000e+00, 2.000e+00, 3.000e+00, ..., 1.000e+00, 4.845e+03,\n",
       "        1.000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building  multilayer perceptron\n",
    "#not optimized\n",
    "from keras.layers import Embedding, Bidirectional, LSTM\n",
    "from keras.initializers import Constant\n",
    "drop_rate = 0.2\n",
    "#layers = 2\n",
    "clasif = models.Sequential()\n",
    "clasif.add(Embedding(num_words,\n",
    "                     100,\n",
    "                     embeddings_initializer =Constant(em_mat),\n",
    "                     input_length = 18,\n",
    "                     trainable=True))\n",
    "clasif.add(Dropout(rate = drop_rate, input_shape = x_train.shape[1:]))\n",
    "#clasif.add(Bidirectional(LSTM(64, return_sequences = True)))\n",
    "#clasif.add(Bidirectional(LSTM(32)))\n",
    "clasif.add(Dropout(rate = 0.1))\n",
    "clasif.add(Dense(units = 3,activation = 'softmax'))\n",
    "\n",
    "\n",
    "#for lvl in range(layers - 1):\n",
    "#    clasif.add(Dense(units = 3, activation = 'relu'))\n",
    "#    clasif.add(Dropout(rate = 0.1))\n",
    "#clasif.add(Dense(units = 3,activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_13 to have 3 dimensions, but got array with shape (8858, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-378-2bae087ef99c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Logs once per epoch.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m             )\n\u001b[0;32m     34\u001b[0m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1152\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1153\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1154\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1156\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    619\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 621\u001b[1;33m                 exception_prefix='target')\n\u001b[0m\u001b[0;32m    622\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    623\u001b[0m             \u001b[1;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    133\u001b[0m                         \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    136\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking target: expected dense_13 to have 3 dimensions, but got array with shape (8858, 1)"
     ]
    }
   ],
   "source": [
    "#N-gram model training and validation. Haven't used balance library\n",
    "#Metrics calc for metrics not available in Keras. Funcs from Stackoverflow\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "\n",
    "optimizer = Adam(lr = 1e-3)\n",
    "clasif.compile(optimizer, loss = 'sparse_categorical_crossentropy', metrics = ['acc', f1_m, precision_m, recall_m])\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)]\n",
    "history = clasif.fit(\n",
    "            x_train,\n",
    "            train_val,\n",
    "            epochs=1000,\n",
    "            batch_size = 128,\n",
    "            callbacks=callbacks,\n",
    "            validation_data=(x_val, test_val),\n",
    "            verbose=2,  # Logs once per epoch.\n",
    "            )\n",
    "history = history.history\n",
    "print('Validation accuracy: {acc}, loss: {loss}, f1_score: {f1}'.format(acc=history['val_acc'][-1], loss=history['val_loss'][-1], f1 = history['val_f1_m']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 1.0702214\ttotal: 101ms\tremaining: 8s\n",
      "1:\tlearn: 1.0376314\ttotal: 203ms\tremaining: 7.93s\n",
      "2:\tlearn: 1.0216949\ttotal: 309ms\tremaining: 7.93s\n",
      "3:\tlearn: 0.9984164\ttotal: 407ms\tremaining: 7.73s\n",
      "4:\tlearn: 0.9822195\ttotal: 516ms\tremaining: 7.73s\n",
      "5:\tlearn: 0.9643367\ttotal: 617ms\tremaining: 7.61s\n",
      "6:\tlearn: 0.9485583\ttotal: 719ms\tremaining: 7.5s\n",
      "7:\tlearn: 0.9308092\ttotal: 830ms\tremaining: 7.47s\n",
      "8:\tlearn: 0.9194750\ttotal: 940ms\tremaining: 7.41s\n",
      "9:\tlearn: 0.9089587\ttotal: 1.05s\tremaining: 7.35s\n",
      "10:\tlearn: 0.8988127\ttotal: 1.16s\tremaining: 7.27s\n",
      "11:\tlearn: 0.8864199\ttotal: 1.26s\tremaining: 7.16s\n",
      "12:\tlearn: 0.8749474\ttotal: 1.37s\tremaining: 7.04s\n",
      "13:\tlearn: 0.8646275\ttotal: 1.47s\tremaining: 6.93s\n",
      "14:\tlearn: 0.8558497\ttotal: 1.57s\tremaining: 6.8s\n",
      "15:\tlearn: 0.8438211\ttotal: 1.67s\tremaining: 6.69s\n",
      "16:\tlearn: 0.8334711\ttotal: 1.77s\tremaining: 6.58s\n",
      "17:\tlearn: 0.8257156\ttotal: 1.88s\tremaining: 6.49s\n",
      "18:\tlearn: 0.8171806\ttotal: 1.99s\tremaining: 6.38s\n",
      "19:\tlearn: 0.8054038\ttotal: 2.1s\tremaining: 6.3s\n",
      "20:\tlearn: 0.7996220\ttotal: 2.21s\tremaining: 6.2s\n",
      "21:\tlearn: 0.7863950\ttotal: 2.31s\tremaining: 6.09s\n",
      "22:\tlearn: 0.7765539\ttotal: 2.42s\tremaining: 5.99s\n",
      "23:\tlearn: 0.7672739\ttotal: 2.52s\tremaining: 5.87s\n",
      "24:\tlearn: 0.7572051\ttotal: 2.63s\tremaining: 5.79s\n",
      "25:\tlearn: 0.7448611\ttotal: 2.74s\tremaining: 5.69s\n",
      "26:\tlearn: 0.7319638\ttotal: 2.85s\tremaining: 5.59s\n",
      "27:\tlearn: 0.7185345\ttotal: 2.96s\tremaining: 5.49s\n",
      "28:\tlearn: 0.7089436\ttotal: 3.06s\tremaining: 5.39s\n",
      "29:\tlearn: 0.6975957\ttotal: 3.16s\tremaining: 5.27s\n",
      "30:\tlearn: 0.6865572\ttotal: 3.27s\tremaining: 5.16s\n",
      "31:\tlearn: 0.6754036\ttotal: 3.37s\tremaining: 5.05s\n",
      "32:\tlearn: 0.6645585\ttotal: 3.48s\tremaining: 4.95s\n",
      "33:\tlearn: 0.6566328\ttotal: 3.58s\tremaining: 4.84s\n",
      "34:\tlearn: 0.6472628\ttotal: 3.68s\tremaining: 4.74s\n",
      "35:\tlearn: 0.6376971\ttotal: 3.79s\tremaining: 4.63s\n",
      "36:\tlearn: 0.6310792\ttotal: 3.89s\tremaining: 4.53s\n",
      "37:\tlearn: 0.6248423\ttotal: 4s\tremaining: 4.42s\n",
      "38:\tlearn: 0.6158097\ttotal: 4.1s\tremaining: 4.31s\n",
      "39:\tlearn: 0.6073942\ttotal: 4.2s\tremaining: 4.2s\n",
      "40:\tlearn: 0.6016807\ttotal: 4.3s\tremaining: 4.09s\n",
      "41:\tlearn: 0.5962331\ttotal: 4.4s\tremaining: 3.98s\n",
      "42:\tlearn: 0.5887278\ttotal: 4.51s\tremaining: 3.88s\n",
      "43:\tlearn: 0.5838801\ttotal: 4.61s\tremaining: 3.77s\n",
      "44:\tlearn: 0.5777760\ttotal: 4.71s\tremaining: 3.67s\n",
      "45:\tlearn: 0.5716173\ttotal: 4.82s\tremaining: 3.56s\n",
      "46:\tlearn: 0.5672466\ttotal: 4.92s\tremaining: 3.45s\n",
      "47:\tlearn: 0.5579329\ttotal: 5.02s\tremaining: 3.35s\n",
      "48:\tlearn: 0.5519310\ttotal: 5.12s\tremaining: 3.24s\n",
      "49:\tlearn: 0.5432315\ttotal: 5.22s\tremaining: 3.13s\n",
      "50:\tlearn: 0.5351628\ttotal: 5.33s\tremaining: 3.03s\n",
      "51:\tlearn: 0.5295345\ttotal: 5.43s\tremaining: 2.93s\n",
      "52:\tlearn: 0.5229646\ttotal: 5.54s\tremaining: 2.82s\n",
      "53:\tlearn: 0.5198007\ttotal: 5.64s\tremaining: 2.72s\n",
      "54:\tlearn: 0.5132723\ttotal: 5.74s\tremaining: 2.61s\n",
      "55:\tlearn: 0.5077573\ttotal: 5.85s\tremaining: 2.5s\n",
      "56:\tlearn: 0.5041519\ttotal: 5.95s\tremaining: 2.4s\n",
      "57:\tlearn: 0.4989832\ttotal: 6.05s\tremaining: 2.29s\n",
      "58:\tlearn: 0.4948315\ttotal: 6.15s\tremaining: 2.19s\n",
      "59:\tlearn: 0.4874866\ttotal: 6.26s\tremaining: 2.08s\n",
      "60:\tlearn: 0.4844062\ttotal: 6.36s\tremaining: 1.98s\n",
      "61:\tlearn: 0.4804072\ttotal: 6.47s\tremaining: 1.88s\n",
      "62:\tlearn: 0.4750318\ttotal: 6.57s\tremaining: 1.77s\n",
      "63:\tlearn: 0.4732490\ttotal: 6.68s\tremaining: 1.67s\n",
      "64:\tlearn: 0.4691007\ttotal: 6.78s\tremaining: 1.56s\n",
      "65:\tlearn: 0.4633874\ttotal: 6.88s\tremaining: 1.46s\n",
      "66:\tlearn: 0.4594472\ttotal: 6.99s\tremaining: 1.36s\n",
      "67:\tlearn: 0.4536594\ttotal: 7.1s\tremaining: 1.25s\n",
      "68:\tlearn: 0.4474583\ttotal: 7.2s\tremaining: 1.15s\n",
      "69:\tlearn: 0.4437885\ttotal: 7.31s\tremaining: 1.04s\n",
      "70:\tlearn: 0.4398065\ttotal: 7.42s\tremaining: 941ms\n",
      "71:\tlearn: 0.4352160\ttotal: 7.52s\tremaining: 836ms\n",
      "72:\tlearn: 0.4310377\ttotal: 7.63s\tremaining: 732ms\n",
      "73:\tlearn: 0.4274707\ttotal: 7.74s\tremaining: 628ms\n",
      "74:\tlearn: 0.4234668\ttotal: 7.85s\tremaining: 523ms\n",
      "75:\tlearn: 0.4202832\ttotal: 7.96s\tremaining: 419ms\n",
      "76:\tlearn: 0.4166371\ttotal: 8.08s\tremaining: 315ms\n",
      "77:\tlearn: 0.4139684\ttotal: 8.19s\tremaining: 210ms\n",
      "78:\tlearn: 0.4093979\ttotal: 8.31s\tremaining: 105ms\n",
      "79:\tlearn: 0.4062195\ttotal: 8.42s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('randomoversampler', RandomOverSampler(random_state=32)),\n",
       "                ('catboostclassifier',\n",
       "                 <catboost.core.CatBoostClassifier object at 0x0000028E22617908>)])"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.over_sampling import KMeansSMOTE, ADASYN, SMOTE, BorderlineSMOTE, SVMSMOTE, SMOTENC, RandomOverSampler\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from catboost import CatBoostClassifier\n",
    "pipeline = make_pipeline(RandomOverSampler(random_state=32), CatBoostClassifier(depth=9, \n",
    "                                                                                bootstrap_type= 'Bayesian', \n",
    "                                                                                loss_function = 'MultiClass', \n",
    "                                                                                iterations=80, learning_rate=0.4, l2_leaf_reg=26))\n",
    "pipeline.fit( x_train,\n",
    "            train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5986455981941309\n",
      "0.42956292127270784\n",
      "0.46846756140042295\n",
      "0.4300050909468606\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "y_pred2 = pipeline.predict(test_text)\n",
    "print(accuracy_score(test_val, y_pred2))\n",
    "print(precision_score(test_val, y_pred2, average = 'macro'))\n",
    "print(recall_score(test_val, y_pred2, average='macro'))\n",
    "print(f1_score(test_val, y_pred2, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29179049632463167"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = clasif.predict_classes(validation_x)\n",
    "score = f1_score(validation_y, y_pred, average='macro')\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>polyscimajor epa chief doesn't think carbon di...</td>\n",
       "      <td>625221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>it's not like we lack evidence of anthropogeni...</td>\n",
       "      <td>126103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>rt @rawstory: researchers say we have three ye...</td>\n",
       "      <td>698562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>#todayinmaker# wired : 2016 was a pivotal year...</td>\n",
       "      <td>573736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>rt @soynoviodetodas: it's 2016, and a racist, ...</td>\n",
       "      <td>466954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  tweetid\n",
       "0          1  polyscimajor epa chief doesn't think carbon di...   625221\n",
       "1          1  it's not like we lack evidence of anthropogeni...   126103\n",
       "2          2  rt @rawstory: researchers say we have three ye...   698562\n",
       "3          1  #todayinmaker# wired : 2016 was a pivotal year...   573736\n",
       "4          1  rt @soynoviodetodas: it's 2016, and a racist, ...   466954"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrain[\\'cleaned\\'] = train[\\'message\\'].str.split()\\nfor text in train[\\'message\\']:\\n    text_nonum = re.sub(r\\'\\\\d+\\', \\'\\', text)\\n    text_nopunct = \"\".join([char.lower() for char in text_nonum if char not in string.punctuation]) \\n    text_no_doublespace = re.sub(\\'\\\\s+\\', \\' \\', text_nopunct).strip()\\n    train[\\'message\\'] = text_no_doublespace\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Lowercaase, remove punct and numbers\n",
    "'''\n",
    "import string\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    # remove numbers\n",
    "    text_nonum = re.sub(r'\\d+', '', text)\n",
    "    # remove punctuations and convert characters to lower case\n",
    "    text_nopunct = \"\".join([char.lower() for char in text_nonum if char not in string.punctuation]) \n",
    "    # substitute multiple whitespace with single whitespace\n",
    "    # Also, removes leading and trailing whitespaces\n",
    "    text_no_doublespace = re.sub('\\s+', ' ', text_nopunct).strip()\n",
    "    return text_no_doublespace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['message'] = train['message'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>polyscimajor epa chief doesnt think carbon dio...</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>its not like we lack evidence of anthropogenic...</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>rt rawstory researchers say we have three year...</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>todayinmaker wired was a pivotal year in the w...</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>rt soynoviodetodas its and a racist sexist cli...</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  count\n",
       "0          1  polyscimajor epa chief doesnt think carbon dio...    128\n",
       "1          1  its not like we lack evidence of anthropogenic...     61\n",
       "2          2  rt rawstory researchers say we have three year...    125\n",
       "3          1  todayinmaker wired was a pivotal year in the w...     83\n",
       "4          1  rt soynoviodetodas its and a racist sexist cli...    109"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def counter(text):\n",
    "    # remove numbers\n",
    "    count = len(text)\n",
    "    return count\n",
    "train['count'] = train['message'].apply(counter)\n",
    "train.pop('tweetid')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "def tokenizer(text3):\n",
    "    tokenized = word_tokenize(text3)\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['tokenized'] = train[\"message\"].apply(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = train['message']\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), \n",
    "                                   min_df = 2, \n",
    "                                   max_df = .95,\n",
    "                                  stop_words = stop2)\n",
    "\n",
    "x_train = tfidf_vectorizer.fit_transform(texts) #features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15819, 22505)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15819, 100)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD \n",
    "lsa = TruncatedSVD(n_components=100, \n",
    "                   n_iter=10, \n",
    "                   random_state=3)\n",
    "\n",
    "x_train = lsa.fit_transform(x_train)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df = 1, ngram_range = (1,1), \n",
    "                             stop_words = stop2)\n",
    "tfidf = vectorizer.fit_transform(text4)\n",
    "return tfidf\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner_text(text2):\n",
    "    tweet = [w for w in text2 if w not in list(stop2)]\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['tokey'] = train['tokenized'].apply(cleaner_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['tokey2'] = train['tokenized'].apply(vect_funct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_trainer, X_tester, y_trainer, y_tester = train_test_split(x_train, y_train, test_size=0.20, shuffle = True, random_state=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "'''Classifiers'''\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "'''Metrics/Evaluation'''\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc, confusion_matrix\n",
    "from scipy import interp\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 1.2422782\ttotal: 651ms\tremaining: 51.4s\n",
      "1:\tlearn: 1.1629784\ttotal: 1.42s\tremaining: 55.3s\n",
      "2:\tlearn: 1.1074635\ttotal: 2.17s\tremaining: 55.7s\n",
      "3:\tlearn: 1.0648235\ttotal: 2.87s\tremaining: 54.6s\n",
      "4:\tlearn: 1.0378812\ttotal: 3.65s\tremaining: 54.7s\n",
      "5:\tlearn: 1.0103706\ttotal: 4.41s\tremaining: 54.3s\n",
      "6:\tlearn: 0.9892448\ttotal: 5.15s\tremaining: 53.7s\n",
      "7:\tlearn: 0.9673001\ttotal: 5.84s\tremaining: 52.6s\n",
      "8:\tlearn: 0.9526841\ttotal: 6.54s\tremaining: 51.6s\n",
      "9:\tlearn: 0.9375668\ttotal: 7.26s\tremaining: 50.9s\n",
      "10:\tlearn: 0.9256460\ttotal: 7.95s\tremaining: 49.9s\n",
      "11:\tlearn: 0.9127509\ttotal: 8.65s\tremaining: 49s\n",
      "12:\tlearn: 0.9020729\ttotal: 9.34s\tremaining: 48.1s\n",
      "13:\tlearn: 0.8913362\ttotal: 10s\tremaining: 47.3s\n",
      "14:\tlearn: 0.8830024\ttotal: 10.7s\tremaining: 46.4s\n",
      "15:\tlearn: 0.8742813\ttotal: 11.4s\tremaining: 45.6s\n",
      "16:\tlearn: 0.8646963\ttotal: 12.1s\tremaining: 44.8s\n",
      "17:\tlearn: 0.8560204\ttotal: 12.8s\tremaining: 44.1s\n",
      "18:\tlearn: 0.8476133\ttotal: 13.5s\tremaining: 43.2s\n",
      "19:\tlearn: 0.8417029\ttotal: 14.2s\tremaining: 42.5s\n",
      "20:\tlearn: 0.8351504\ttotal: 14.8s\tremaining: 41.7s\n",
      "21:\tlearn: 0.8284220\ttotal: 15.5s\tremaining: 41s\n",
      "22:\tlearn: 0.8212577\ttotal: 16.3s\tremaining: 40.3s\n",
      "23:\tlearn: 0.8149352\ttotal: 16.9s\tremaining: 39.5s\n",
      "24:\tlearn: 0.8087724\ttotal: 17.6s\tremaining: 38.8s\n",
      "25:\tlearn: 0.8028464\ttotal: 18.3s\tremaining: 38s\n",
      "26:\tlearn: 0.7961789\ttotal: 19s\tremaining: 37.3s\n",
      "27:\tlearn: 0.7909302\ttotal: 19.6s\tremaining: 36.5s\n",
      "28:\tlearn: 0.7848341\ttotal: 20.4s\tremaining: 35.8s\n",
      "29:\tlearn: 0.7783815\ttotal: 21.1s\tremaining: 35.1s\n",
      "30:\tlearn: 0.7737742\ttotal: 21.7s\tremaining: 34.4s\n",
      "31:\tlearn: 0.7675558\ttotal: 22.4s\tremaining: 33.6s\n",
      "32:\tlearn: 0.7628484\ttotal: 23.1s\tremaining: 32.9s\n",
      "33:\tlearn: 0.7565845\ttotal: 23.8s\tremaining: 32.2s\n",
      "34:\tlearn: 0.7518001\ttotal: 24.5s\tremaining: 31.5s\n",
      "35:\tlearn: 0.7470587\ttotal: 25.2s\tremaining: 30.8s\n",
      "36:\tlearn: 0.7433492\ttotal: 25.9s\tremaining: 30.1s\n",
      "37:\tlearn: 0.7391890\ttotal: 26.5s\tremaining: 29.3s\n",
      "38:\tlearn: 0.7338764\ttotal: 27.2s\tremaining: 28.6s\n",
      "39:\tlearn: 0.7299091\ttotal: 27.9s\tremaining: 27.9s\n",
      "40:\tlearn: 0.7251977\ttotal: 28.6s\tremaining: 27.2s\n",
      "41:\tlearn: 0.7206750\ttotal: 29.3s\tremaining: 26.5s\n",
      "42:\tlearn: 0.7150616\ttotal: 30s\tremaining: 25.8s\n",
      "43:\tlearn: 0.7098647\ttotal: 30.7s\tremaining: 25.1s\n",
      "44:\tlearn: 0.7052080\ttotal: 31.4s\tremaining: 24.4s\n",
      "45:\tlearn: 0.7027490\ttotal: 32.1s\tremaining: 23.7s\n",
      "46:\tlearn: 0.6983037\ttotal: 32.7s\tremaining: 23s\n",
      "47:\tlearn: 0.6932460\ttotal: 33.4s\tremaining: 22.3s\n",
      "48:\tlearn: 0.6895075\ttotal: 34.1s\tremaining: 21.6s\n",
      "49:\tlearn: 0.6858952\ttotal: 34.8s\tremaining: 20.9s\n",
      "50:\tlearn: 0.6807459\ttotal: 35.5s\tremaining: 20.2s\n",
      "51:\tlearn: 0.6750865\ttotal: 36.2s\tremaining: 19.5s\n",
      "52:\tlearn: 0.6707766\ttotal: 36.9s\tremaining: 18.8s\n",
      "53:\tlearn: 0.6660760\ttotal: 37.7s\tremaining: 18.1s\n",
      "54:\tlearn: 0.6615995\ttotal: 38.3s\tremaining: 17.4s\n",
      "55:\tlearn: 0.6581678\ttotal: 39s\tremaining: 16.7s\n",
      "56:\tlearn: 0.6542275\ttotal: 39.7s\tremaining: 16s\n",
      "57:\tlearn: 0.6517558\ttotal: 40.4s\tremaining: 15.3s\n",
      "58:\tlearn: 0.6479573\ttotal: 41.1s\tremaining: 14.6s\n",
      "59:\tlearn: 0.6450791\ttotal: 41.8s\tremaining: 13.9s\n",
      "60:\tlearn: 0.6416379\ttotal: 42.4s\tremaining: 13.2s\n",
      "61:\tlearn: 0.6375806\ttotal: 43.1s\tremaining: 12.5s\n",
      "62:\tlearn: 0.6354562\ttotal: 43.8s\tremaining: 11.8s\n",
      "63:\tlearn: 0.6305288\ttotal: 44.5s\tremaining: 11.1s\n",
      "64:\tlearn: 0.6285698\ttotal: 45.2s\tremaining: 10.4s\n",
      "65:\tlearn: 0.6257434\ttotal: 45.9s\tremaining: 9.74s\n",
      "66:\tlearn: 0.6221721\ttotal: 46.6s\tremaining: 9.04s\n",
      "67:\tlearn: 0.6191036\ttotal: 47.3s\tremaining: 8.35s\n",
      "68:\tlearn: 0.6165460\ttotal: 48s\tremaining: 7.66s\n",
      "69:\tlearn: 0.6124504\ttotal: 48.7s\tremaining: 6.96s\n",
      "70:\tlearn: 0.6088004\ttotal: 49.4s\tremaining: 6.26s\n",
      "71:\tlearn: 0.6053620\ttotal: 50.1s\tremaining: 5.57s\n",
      "72:\tlearn: 0.6020743\ttotal: 50.8s\tremaining: 4.87s\n",
      "73:\tlearn: 0.5994745\ttotal: 51.5s\tremaining: 4.18s\n",
      "74:\tlearn: 0.5964681\ttotal: 52.2s\tremaining: 3.48s\n",
      "75:\tlearn: 0.5929916\ttotal: 52.9s\tremaining: 2.78s\n",
      "76:\tlearn: 0.5905118\ttotal: 53.6s\tremaining: 2.09s\n",
      "77:\tlearn: 0.5878024\ttotal: 54.3s\tremaining: 1.39s\n",
      "78:\tlearn: 0.5843553\ttotal: 54.9s\tremaining: 695ms\n",
      "79:\tlearn: 0.5810704\ttotal: 55.6s\tremaining: 0us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:249: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>accuracy_score</th>\n",
       "      <th>precision_score</th>\n",
       "      <th>recall_score</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.696903</td>\n",
       "      <td>0.692982</td>\n",
       "      <td>0.505798</td>\n",
       "      <td>0.54071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>0.678887</td>\n",
       "      <td>0.640671</td>\n",
       "      <td>0.499413</td>\n",
       "      <td>0.529377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>K Nearest Neighbor</td>\n",
       "      <td>0.605879</td>\n",
       "      <td>0.510207</td>\n",
       "      <td>0.502339</td>\n",
       "      <td>0.505625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.541719</td>\n",
       "      <td>0.430299</td>\n",
       "      <td>0.282851</td>\n",
       "      <td>0.252387</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           model_name accuracy_score precision_score recall_score  f1_score\n",
       "2       Random Forest       0.696903        0.692982     0.505798   0.54071\n",
       "0            CatBoost       0.678887        0.640671     0.499413  0.529377\n",
       "3  K Nearest Neighbor       0.605879        0.510207     0.502339  0.505625\n",
       "1                 SVC       0.541719        0.430299     0.282851  0.252387"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Preliminary model evaluation using default parameters\n",
    "\n",
    "#Creating a dict of the models\n",
    "model_dict = {\"CatBoost\" : CatBoostClassifier(depth=9,\n",
    "                                              bootstrap_type= 'Bayesian', \n",
    "                                              loss_function = 'MultiClass', \n",
    "                                              iterations=80, learning_rate=0.4, \n",
    "                                              l2_leaf_reg=26),\n",
    "              \"SVC\": SVC(max_iter = 100, kernel =  'linear' ),\n",
    "              'Random Forest': RandomForestClassifier(random_state=32),\n",
    "              'K Nearest Neighbor': KNeighborsClassifier()}\n",
    "\n",
    "\n",
    "\n",
    "#Function to get the scores for each model in a df\n",
    "def model_score_df(model_dict):   \n",
    "    model_name, ac_score_list, p_score_list, r_score_list, f1_score_list = [], [], [], [], []\n",
    "    for k,v in model_dict.items():   \n",
    "        model_name.append(k)\n",
    "        v.fit(X_trainer, y_trainer)\n",
    "        y_pred = v.predict(X_tester)\n",
    "        ac_score_list.append(accuracy_score(y_tester, y_pred))\n",
    "        p_score_list.append(precision_score(y_tester, y_pred, average='macro'))\n",
    "        r_score_list.append(recall_score(y_tester, y_pred, average='macro'))\n",
    "        f1_score_list.append(f1_score(y_tester, y_pred, average='macro'))\n",
    "        model_comparison_df = pd.DataFrame([model_name, ac_score_list, p_score_list, r_score_list, f1_score_list]).T\n",
    "        model_comparison_df.columns = ['model_name', 'accuracy_score', 'precision_score', 'recall_score', 'f1_score']\n",
    "        model_comparison_df = model_comparison_df.sort_values(by='f1_score', ascending=False)\n",
    "    return model_comparison_df\n",
    "\n",
    "model_score_df(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imbalanced-learn\n",
      "  Downloading imbalanced_learn-0.7.0-py3-none-any.whl (167 kB)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn>=0.23 in c:\\programdata\\anaconda3\\lib\\site-packages (from imbalanced-learn) (0.23.1)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in c:\\programdata\\anaconda3\\lib\\site-packages (from imbalanced-learn) (0.15.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.18.1)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn>=0.23->imbalanced-learn) (2.1.0)\n",
      "Installing collected packages: imbalanced-learn\n",
      "Successfully installed imbalanced-learn-0.7.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 1.2901965\ttotal: 724ms\tremaining: 57.2s\n",
      "1:\tlearn: 1.2176739\ttotal: 1.5s\tremaining: 58.6s\n",
      "2:\tlearn: 1.1640547\ttotal: 2.23s\tremaining: 57.2s\n",
      "3:\tlearn: 1.1205991\ttotal: 2.95s\tremaining: 56.1s\n",
      "4:\tlearn: 1.0851678\ttotal: 3.67s\tremaining: 55s\n",
      "5:\tlearn: 1.0547703\ttotal: 4.38s\tremaining: 54.1s\n",
      "6:\tlearn: 1.0258817\ttotal: 5.12s\tremaining: 53.4s\n",
      "7:\tlearn: 1.0036852\ttotal: 5.83s\tremaining: 52.4s\n",
      "8:\tlearn: 0.9882438\ttotal: 6.52s\tremaining: 51.5s\n",
      "9:\tlearn: 0.9647969\ttotal: 7.25s\tremaining: 50.8s\n",
      "10:\tlearn: 0.9507624\ttotal: 7.95s\tremaining: 49.9s\n",
      "11:\tlearn: 0.9309737\ttotal: 8.67s\tremaining: 49.2s\n",
      "12:\tlearn: 0.9151370\ttotal: 9.39s\tremaining: 48.4s\n",
      "13:\tlearn: 0.8982678\ttotal: 10.1s\tremaining: 47.7s\n",
      "14:\tlearn: 0.8830705\ttotal: 10.8s\tremaining: 47s\n",
      "15:\tlearn: 0.8697233\ttotal: 11.5s\tremaining: 46.2s\n",
      "16:\tlearn: 0.8522023\ttotal: 12.3s\tremaining: 45.5s\n",
      "17:\tlearn: 0.8431209\ttotal: 13s\tremaining: 44.6s\n",
      "18:\tlearn: 0.8340374\ttotal: 13.6s\tremaining: 43.7s\n",
      "19:\tlearn: 0.8248791\ttotal: 14.3s\tremaining: 43s\n",
      "20:\tlearn: 0.8139810\ttotal: 15s\tremaining: 42.2s\n",
      "21:\tlearn: 0.8033320\ttotal: 15.7s\tremaining: 41.5s\n",
      "22:\tlearn: 0.7911523\ttotal: 16.5s\tremaining: 40.8s\n",
      "23:\tlearn: 0.7826090\ttotal: 17.2s\tremaining: 40s\n",
      "24:\tlearn: 0.7705775\ttotal: 17.9s\tremaining: 39.5s\n",
      "25:\tlearn: 0.7610800\ttotal: 18.7s\tremaining: 38.8s\n",
      "26:\tlearn: 0.7518574\ttotal: 19.4s\tremaining: 38.1s\n",
      "27:\tlearn: 0.7418407\ttotal: 20.1s\tremaining: 37.4s\n",
      "28:\tlearn: 0.7334242\ttotal: 20.8s\tremaining: 36.7s\n",
      "29:\tlearn: 0.7257432\ttotal: 21.5s\tremaining: 35.9s\n",
      "30:\tlearn: 0.7157681\ttotal: 22.3s\tremaining: 35.2s\n",
      "31:\tlearn: 0.7072431\ttotal: 23s\tremaining: 34.4s\n",
      "32:\tlearn: 0.6978530\ttotal: 23.7s\tremaining: 33.7s\n",
      "33:\tlearn: 0.6881179\ttotal: 24.4s\tremaining: 33.1s\n",
      "34:\tlearn: 0.6779755\ttotal: 25.2s\tremaining: 32.4s\n",
      "35:\tlearn: 0.6674615\ttotal: 25.9s\tremaining: 31.7s\n",
      "36:\tlearn: 0.6609447\ttotal: 26.7s\tremaining: 31s\n",
      "37:\tlearn: 0.6555557\ttotal: 27.5s\tremaining: 30.4s\n",
      "38:\tlearn: 0.6470749\ttotal: 28.2s\tremaining: 29.7s\n",
      "39:\tlearn: 0.6391158\ttotal: 29s\tremaining: 29s\n",
      "40:\tlearn: 0.6339052\ttotal: 29.7s\tremaining: 28.3s\n",
      "41:\tlearn: 0.6228268\ttotal: 30.5s\tremaining: 27.6s\n",
      "42:\tlearn: 0.6142621\ttotal: 31.3s\tremaining: 27s\n",
      "43:\tlearn: 0.6065780\ttotal: 32.2s\tremaining: 26.3s\n",
      "44:\tlearn: 0.6005540\ttotal: 32.9s\tremaining: 25.6s\n",
      "45:\tlearn: 0.5953947\ttotal: 33.7s\tremaining: 24.9s\n",
      "46:\tlearn: 0.5878845\ttotal: 34.4s\tremaining: 24.2s\n",
      "47:\tlearn: 0.5787865\ttotal: 35.2s\tremaining: 23.4s\n",
      "48:\tlearn: 0.5733231\ttotal: 35.9s\tremaining: 22.7s\n",
      "49:\tlearn: 0.5686790\ttotal: 36.9s\tremaining: 22.2s\n",
      "50:\tlearn: 0.5637786\ttotal: 38s\tremaining: 21.6s\n",
      "51:\tlearn: 0.5588173\ttotal: 39.1s\tremaining: 21s\n",
      "52:\tlearn: 0.5514186\ttotal: 40s\tremaining: 20.4s\n",
      "53:\tlearn: 0.5440257\ttotal: 40.9s\tremaining: 19.7s\n",
      "54:\tlearn: 0.5364694\ttotal: 42.1s\tremaining: 19.1s\n",
      "55:\tlearn: 0.5321941\ttotal: 43.1s\tremaining: 18.5s\n",
      "56:\tlearn: 0.5272488\ttotal: 44.1s\tremaining: 17.8s\n",
      "57:\tlearn: 0.5195688\ttotal: 45.1s\tremaining: 17.1s\n",
      "58:\tlearn: 0.5151694\ttotal: 46.1s\tremaining: 16.4s\n",
      "59:\tlearn: 0.5111632\ttotal: 47s\tremaining: 15.7s\n",
      "60:\tlearn: 0.5061255\ttotal: 48s\tremaining: 14.9s\n",
      "61:\tlearn: 0.5009951\ttotal: 48.9s\tremaining: 14.2s\n",
      "62:\tlearn: 0.4954117\ttotal: 49.8s\tremaining: 13.4s\n",
      "63:\tlearn: 0.4930867\ttotal: 50.7s\tremaining: 12.7s\n",
      "64:\tlearn: 0.4895812\ttotal: 51.4s\tremaining: 11.9s\n",
      "65:\tlearn: 0.4848664\ttotal: 52.2s\tremaining: 11.1s\n",
      "66:\tlearn: 0.4827727\ttotal: 53s\tremaining: 10.3s\n",
      "67:\tlearn: 0.4779212\ttotal: 53.8s\tremaining: 9.49s\n",
      "68:\tlearn: 0.4752761\ttotal: 54.5s\tremaining: 8.69s\n",
      "69:\tlearn: 0.4730648\ttotal: 55.3s\tremaining: 7.89s\n",
      "70:\tlearn: 0.4680976\ttotal: 56.2s\tremaining: 7.13s\n",
      "71:\tlearn: 0.4639773\ttotal: 57.1s\tremaining: 6.35s\n",
      "72:\tlearn: 0.4584293\ttotal: 57.9s\tremaining: 5.55s\n",
      "73:\tlearn: 0.4542367\ttotal: 58.8s\tremaining: 4.76s\n",
      "74:\tlearn: 0.4498599\ttotal: 59.6s\tremaining: 3.97s\n",
      "75:\tlearn: 0.4460113\ttotal: 1m\tremaining: 3.18s\n",
      "76:\tlearn: 0.4400022\ttotal: 1m 1s\tremaining: 2.39s\n",
      "77:\tlearn: 0.4362980\ttotal: 1m 2s\tremaining: 1.6s\n",
      "78:\tlearn: 0.4316460\ttotal: 1m 3s\tremaining: 800ms\n",
      "79:\tlearn: 0.4268849\ttotal: 1m 4s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('randomoversampler', RandomOverSampler(random_state=32)),\n",
       "                ('catboostclassifier',\n",
       "                 <catboost.core.CatBoostClassifier object at 0x000002091C577908>)])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.over_sampling import KMeansSMOTE, ADASYN, SMOTE, BorderlineSMOTE, SVMSMOTE, SMOTENC, RandomOverSampler\n",
    "from imblearn.pipeline import make_pipeline\n",
    "pipeline = make_pipeline(RandomOverSampler(random_state=32), CatBoostClassifier(depth=9, \n",
    "                                                                                bootstrap_type= 'Bayesian', \n",
    "                                                                                loss_function = 'MultiClass', \n",
    "                                                                                iterations=80, learning_rate=0.4, l2_leaf_reg=26))\n",
    "pipeline.fit(X_trainer, y_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2 = pipeline.predict(X_tester)\n",
    "print(accuracy_score(y_tester, y_pred2))\n",
    "print(precision_score(y_tester, y_pred2, average = 'macro'))\n",
    "print(recall_score(y_tester, y_pred2, average='macro'))\n",
    "print(f1_score(y_tester, y_pred2, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6254740834386852\n",
      "0.5295210551142089\n",
      "0.5607691006775947\n",
      "0.5411830007069668\n"
     ]
    }
   ],
   "source": [
    "#RandomOverSampler\n",
    "y_pred2 = pipeline.predict(X_tester)\n",
    "print(accuracy_score(y_tester, y_pred2))\n",
    "print(precision_score(y_tester, y_pred2, average = 'macro'))\n",
    "print(recall_score(y_tester, y_pred2, average='macro'))\n",
    "print(f1_score(y_tester, y_pred2, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6343236409608091\n",
      "0.5440021456026725\n",
      "0.5640358371482043\n",
      "0.5496275062157618\n"
     ]
    }
   ],
   "source": [
    "#SVMSMOTE\n",
    "y_pred2 = pipeline.predict(X_tester)\n",
    "print(accuracy_score(y_tester, y_pred2))\n",
    "print(precision_score(y_tester, y_pred2, average = 'macro'))\n",
    "print(recall_score(y_tester, y_pred2, average='macro'))\n",
    "print(f1_score(y_tester, y_pred2, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.618204804045512\n",
      "0.5242842856455125\n",
      "0.564490887377282\n",
      "0.5378716428178781\n"
     ]
    }
   ],
   "source": [
    "#BorderlineSMOTE\n",
    "y_pred2 = pipeline.predict(X_tester)\n",
    "print(accuracy_score(y_tester, y_pred2))\n",
    "print(precision_score(y_tester, y_pred2, average = 'macro'))\n",
    "print(recall_score(y_tester, y_pred2, average='macro'))\n",
    "print(f1_score(y_tester, y_pred2, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.654551201011378\n",
      "0.6587153718983736\n",
      "0.4963123641656133\n",
      "0.5128128953008306\n"
     ]
    }
   ],
   "source": [
    "#KMeansSMOTE\n",
    "y_pred2 = pipeline.predict(X_tester)\n",
    "print(accuracy_score(y_tester, y_pred2))\n",
    "print(precision_score(y_tester, y_pred2, average = 'macro'))\n",
    "print(recall_score(y_tester, y_pred2, average='macro'))\n",
    "print(f1_score(y_tester, y_pred2, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6219974715549936\n",
      "0.5327266928984817\n",
      "0.5775695142430038\n",
      "0.5476057043805225\n"
     ]
    }
   ],
   "source": [
    "#SMOTE\n",
    "y_pred2 = pipeline.predict(X_tester)\n",
    "print(accuracy_score(y_tester, y_pred2))\n",
    "print(precision_score(y_tester, y_pred2, average = 'macro'))\n",
    "print(recall_score(y_tester, y_pred2, average='macro'))\n",
    "print(f1_score(y_tester, y_pred2, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6103034134007586\n",
      "0.5262459074497435\n",
      "0.5765977308632997\n",
      "0.5413008411586501\n"
     ]
    }
   ],
   "source": [
    "#ADSYN\n",
    "y_pred2 = pipeline.predict(X_tester)\n",
    "print(accuracy_score(y_tester, y_pred2))\n",
    "print(precision_score(y_tester, y_pred2, average = 'macro'))\n",
    "print(recall_score(y_tester, y_pred2, average='macro'))\n",
    "print(f1_score(y_tester, y_pred2, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n The best estimator across ALL searched params:\\n\", randm.best_estimator_)\n",
    "    \n",
    "print(\"\\n The best score across ALL searched params:\\n\", randm.best_score_)\n",
    "    \n",
    "print(\"\\n The best parameters across ALL searched params:\\n\", randm.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CatBoostClassifier()\n",
    "parameters = {'depth': list(range(4, 10, 1)),\n",
    "              \n",
    "              'iterations': list(range(10, 100, 10))}\n",
    "    \n",
    "randm = GridSearchCV(estimator=model, param_grid = parameters, cv = 2,  n_jobs=-1)\n",
    "randm.fit(X_trainer, y_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 1.2713026\ttotal: 734ms\tremaining: 58s\n",
      "1:\tlearn: 1.1741230\ttotal: 1.41s\tremaining: 55.1s\n",
      "2:\tlearn: 1.1301406\ttotal: 2.11s\tremaining: 54.3s\n",
      "3:\tlearn: 1.0856734\ttotal: 2.81s\tremaining: 53.3s\n",
      "4:\tlearn: 1.0589595\ttotal: 3.47s\tremaining: 52.1s\n",
      "5:\tlearn: 1.0343295\ttotal: 4.17s\tremaining: 51.4s\n",
      "6:\tlearn: 1.0129892\ttotal: 4.85s\tremaining: 50.6s\n",
      "7:\tlearn: 0.9950448\ttotal: 5.51s\tremaining: 49.6s\n",
      "8:\tlearn: 0.9781915\ttotal: 6.22s\tremaining: 49.1s\n",
      "9:\tlearn: 0.9629398\ttotal: 6.91s\tremaining: 48.4s\n",
      "10:\tlearn: 0.9517231\ttotal: 7.6s\tremaining: 47.7s\n",
      "11:\tlearn: 0.9404595\ttotal: 8.31s\tremaining: 47.1s\n",
      "12:\tlearn: 0.9301228\ttotal: 8.99s\tremaining: 46.3s\n",
      "13:\tlearn: 0.9191849\ttotal: 9.69s\tremaining: 45.7s\n",
      "14:\tlearn: 0.9096659\ttotal: 10.4s\tremaining: 44.9s\n",
      "15:\tlearn: 0.9001949\ttotal: 11.1s\tremaining: 44.3s\n",
      "16:\tlearn: 0.8914413\ttotal: 11.8s\tremaining: 43.7s\n",
      "17:\tlearn: 0.8819735\ttotal: 12.5s\tremaining: 43s\n",
      "18:\tlearn: 0.8743611\ttotal: 13.2s\tremaining: 42.3s\n",
      "19:\tlearn: 0.8650124\ttotal: 13.9s\tremaining: 41.6s\n",
      "20:\tlearn: 0.8582929\ttotal: 14.6s\tremaining: 41s\n",
      "21:\tlearn: 0.8510230\ttotal: 15.3s\tremaining: 40.4s\n",
      "22:\tlearn: 0.8429690\ttotal: 16s\tremaining: 39.7s\n",
      "23:\tlearn: 0.8352786\ttotal: 16.7s\tremaining: 39s\n",
      "24:\tlearn: 0.8290428\ttotal: 17.4s\tremaining: 38.2s\n",
      "25:\tlearn: 0.8224297\ttotal: 18.1s\tremaining: 37.5s\n",
      "26:\tlearn: 0.8154803\ttotal: 18.7s\tremaining: 36.8s\n",
      "27:\tlearn: 0.8096535\ttotal: 19.4s\tremaining: 36.1s\n",
      "28:\tlearn: 0.8053349\ttotal: 20.2s\tremaining: 35.4s\n",
      "29:\tlearn: 0.7984777\ttotal: 20.9s\tremaining: 34.8s\n",
      "30:\tlearn: 0.7932542\ttotal: 21.6s\tremaining: 34.1s\n",
      "31:\tlearn: 0.7888278\ttotal: 22.2s\tremaining: 33.4s\n",
      "32:\tlearn: 0.7839920\ttotal: 22.9s\tremaining: 32.6s\n",
      "33:\tlearn: 0.7780600\ttotal: 23.6s\tremaining: 31.9s\n",
      "34:\tlearn: 0.7719215\ttotal: 24.3s\tremaining: 31.3s\n",
      "35:\tlearn: 0.7673918\ttotal: 25s\tremaining: 30.6s\n",
      "36:\tlearn: 0.7628234\ttotal: 25.7s\tremaining: 29.8s\n",
      "37:\tlearn: 0.7571011\ttotal: 26.3s\tremaining: 29.1s\n",
      "38:\tlearn: 0.7504829\ttotal: 27s\tremaining: 28.4s\n",
      "39:\tlearn: 0.7462167\ttotal: 27.7s\tremaining: 27.7s\n",
      "40:\tlearn: 0.7418545\ttotal: 28.4s\tremaining: 27s\n",
      "41:\tlearn: 0.7360120\ttotal: 29.1s\tremaining: 26.3s\n",
      "42:\tlearn: 0.7312654\ttotal: 29.8s\tremaining: 25.6s\n",
      "43:\tlearn: 0.7256299\ttotal: 30.5s\tremaining: 24.9s\n",
      "44:\tlearn: 0.7212593\ttotal: 31.2s\tremaining: 24.2s\n",
      "45:\tlearn: 0.7178278\ttotal: 31.8s\tremaining: 23.5s\n",
      "46:\tlearn: 0.7141783\ttotal: 32.5s\tremaining: 22.8s\n",
      "47:\tlearn: 0.7098304\ttotal: 33.2s\tremaining: 22.1s\n",
      "48:\tlearn: 0.7058831\ttotal: 34s\tremaining: 21.5s\n",
      "49:\tlearn: 0.7016487\ttotal: 34.6s\tremaining: 20.8s\n",
      "50:\tlearn: 0.6976173\ttotal: 35.3s\tremaining: 20.1s\n",
      "51:\tlearn: 0.6936053\ttotal: 36s\tremaining: 19.4s\n",
      "52:\tlearn: 0.6896012\ttotal: 36.7s\tremaining: 18.7s\n",
      "53:\tlearn: 0.6849858\ttotal: 37.4s\tremaining: 18s\n",
      "54:\tlearn: 0.6814419\ttotal: 38.1s\tremaining: 17.3s\n",
      "55:\tlearn: 0.6784803\ttotal: 38.7s\tremaining: 16.6s\n",
      "56:\tlearn: 0.6736696\ttotal: 39.4s\tremaining: 15.9s\n",
      "57:\tlearn: 0.6707733\ttotal: 40.1s\tremaining: 15.2s\n",
      "58:\tlearn: 0.6686072\ttotal: 40.8s\tremaining: 14.5s\n",
      "59:\tlearn: 0.6645646\ttotal: 41.5s\tremaining: 13.8s\n",
      "60:\tlearn: 0.6601051\ttotal: 42.2s\tremaining: 13.1s\n",
      "61:\tlearn: 0.6566280\ttotal: 42.9s\tremaining: 12.4s\n",
      "62:\tlearn: 0.6529027\ttotal: 43.5s\tremaining: 11.8s\n",
      "63:\tlearn: 0.6493769\ttotal: 44.2s\tremaining: 11.1s\n",
      "64:\tlearn: 0.6458867\ttotal: 44.9s\tremaining: 10.4s\n",
      "65:\tlearn: 0.6424868\ttotal: 45.6s\tremaining: 9.67s\n",
      "66:\tlearn: 0.6383905\ttotal: 46.3s\tremaining: 8.98s\n",
      "67:\tlearn: 0.6352430\ttotal: 47s\tremaining: 8.29s\n",
      "68:\tlearn: 0.6324157\ttotal: 47.7s\tremaining: 7.6s\n",
      "69:\tlearn: 0.6281451\ttotal: 48.4s\tremaining: 6.91s\n",
      "70:\tlearn: 0.6254216\ttotal: 49s\tremaining: 6.22s\n",
      "71:\tlearn: 0.6204397\ttotal: 49.8s\tremaining: 5.53s\n",
      "72:\tlearn: 0.6163307\ttotal: 50.4s\tremaining: 4.84s\n",
      "73:\tlearn: 0.6133688\ttotal: 51.1s\tremaining: 4.14s\n",
      "74:\tlearn: 0.6102715\ttotal: 51.8s\tremaining: 3.45s\n",
      "75:\tlearn: 0.6070691\ttotal: 52.5s\tremaining: 2.76s\n",
      "76:\tlearn: 0.6039860\ttotal: 53.2s\tremaining: 2.07s\n",
      "77:\tlearn: 0.6004809\ttotal: 53.9s\tremaining: 1.38s\n",
      "78:\tlearn: 0.5967645\ttotal: 54.6s\tremaining: 691ms\n",
      "79:\tlearn: 0.5948433\ttotal: 55.3s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2,\n",
       "             estimator=<catboost.core.CatBoostClassifier object at 0x00000299295B69C8>,\n",
       "             n_jobs=-1,\n",
       "             param_grid={'bootstrap_type': ['Bayesian', 'Bernoulli', 'MVS',\n",
       "                                            'Poisson'],\n",
       "                         'loss_function': ['Logloss', 'CrossEntropy',\n",
       "                                           'MultiClassOneVsAll', 'MAPE',\n",
       "                                           'MultiClass']})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "model = CatBoostClassifier(depth=9, iterations=80, learning_rate=0.4, l2_leaf_reg=26)\n",
    "parameters = {'bootstrap_type': ['Bayesian', 'Bernoulli', 'MVS', 'Poisson'],\n",
    "              \n",
    "              'loss_function': ['Logloss', 'CrossEntropy', 'MultiClassOneVsAll', 'MAPE', 'MultiClass']}\n",
    "randm = GridSearchCV(estimator=model, param_grid = parameters, cv = 2,  n_jobs=-1)\n",
    "randm.fit(X_trainer, y_trainer)\n",
    "\n",
    "    # Results from Random Search\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The best estimator across ALL searched params:\n",
      " <catboost.core.CatBoostClassifier object at 0x000002992F1D19C8>\n",
      "\n",
      " The best score across ALL searched params:\n",
      " 0.6530229868900106\n",
      "\n",
      " The best parameters across ALL searched params:\n",
      " {'bootstrap_type': 'Bayesian', 'loss_function': 'MultiClass'}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n The best estimator across ALL searched params:\\n\", randm.best_estimator_)\n",
    "    \n",
    "print(\"\\n The best score across ALL searched params:\\n\", randm.best_score_)\n",
    "    \n",
    "print(\"\\n The best parameters across ALL searched params:\\n\", randm.best_params_)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-d35b0e36cdfd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mp_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_tester\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'macro'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mr_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_tester\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'macro'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mf1_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_tester\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mac_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.float64' object is not callable"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_tester, y_pred)\n",
    "print(precision_score(y_tester, y_pred, average='macro')\n",
    "print(recall_score(y_tester, y_pred, average='macro')\n",
    "f1_score = f1_score(y_tester, y_pred)\n",
    "print(ac_score)\n",
    "print(p_score)\n",
    "print(r_score)\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 1.2713026\ttotal: 669ms\tremaining: 52.8s\n",
      "1:\tlearn: 1.1741230\ttotal: 1.35s\tremaining: 52.5s\n",
      "2:\tlearn: 1.1301406\ttotal: 2.03s\tremaining: 52.1s\n",
      "3:\tlearn: 1.0856734\ttotal: 2.72s\tremaining: 51.7s\n",
      "4:\tlearn: 1.0589595\ttotal: 3.42s\tremaining: 51.3s\n",
      "5:\tlearn: 1.0343295\ttotal: 4.09s\tremaining: 50.5s\n",
      "6:\tlearn: 1.0129892\ttotal: 4.77s\tremaining: 49.8s\n",
      "7:\tlearn: 0.9950448\ttotal: 5.46s\tremaining: 49.1s\n",
      "8:\tlearn: 0.9781915\ttotal: 6.14s\tremaining: 48.5s\n",
      "9:\tlearn: 0.9629398\ttotal: 6.82s\tremaining: 47.8s\n",
      "10:\tlearn: 0.9517231\ttotal: 7.49s\tremaining: 47s\n",
      "11:\tlearn: 0.9404595\ttotal: 8.16s\tremaining: 46.2s\n",
      "12:\tlearn: 0.9301228\ttotal: 8.83s\tremaining: 45.5s\n",
      "13:\tlearn: 0.9191849\ttotal: 9.52s\tremaining: 44.9s\n",
      "14:\tlearn: 0.9096659\ttotal: 10.2s\tremaining: 44.2s\n",
      "15:\tlearn: 0.9001949\ttotal: 10.9s\tremaining: 43.6s\n",
      "16:\tlearn: 0.8914413\ttotal: 11.6s\tremaining: 42.8s\n",
      "17:\tlearn: 0.8819735\ttotal: 12.2s\tremaining: 42.1s\n",
      "18:\tlearn: 0.8743611\ttotal: 12.9s\tremaining: 41.4s\n",
      "19:\tlearn: 0.8650124\ttotal: 13.6s\tremaining: 40.7s\n",
      "20:\tlearn: 0.8582929\ttotal: 14.2s\tremaining: 40s\n",
      "21:\tlearn: 0.8510230\ttotal: 14.9s\tremaining: 39.3s\n",
      "22:\tlearn: 0.8429690\ttotal: 15.6s\tremaining: 38.6s\n",
      "23:\tlearn: 0.8352786\ttotal: 16.3s\tremaining: 37.9s\n",
      "24:\tlearn: 0.8290428\ttotal: 16.9s\tremaining: 37.2s\n",
      "25:\tlearn: 0.8224297\ttotal: 17.6s\tremaining: 36.5s\n",
      "26:\tlearn: 0.8154803\ttotal: 18.3s\tremaining: 35.9s\n",
      "27:\tlearn: 0.8096535\ttotal: 18.9s\tremaining: 35.1s\n",
      "28:\tlearn: 0.8053349\ttotal: 19.6s\tremaining: 34.5s\n",
      "29:\tlearn: 0.7984777\ttotal: 20.3s\tremaining: 33.9s\n",
      "30:\tlearn: 0.7932542\ttotal: 21s\tremaining: 33.1s\n",
      "31:\tlearn: 0.7888278\ttotal: 21.6s\tremaining: 32.5s\n",
      "32:\tlearn: 0.7839920\ttotal: 22.3s\tremaining: 31.8s\n",
      "33:\tlearn: 0.7780600\ttotal: 23s\tremaining: 31.1s\n",
      "34:\tlearn: 0.7719215\ttotal: 23.7s\tremaining: 30.4s\n",
      "35:\tlearn: 0.7673918\ttotal: 24.3s\tremaining: 29.7s\n",
      "36:\tlearn: 0.7628234\ttotal: 25s\tremaining: 29s\n",
      "37:\tlearn: 0.7571011\ttotal: 25.7s\tremaining: 28.4s\n",
      "38:\tlearn: 0.7504829\ttotal: 26.4s\tremaining: 27.7s\n",
      "39:\tlearn: 0.7462167\ttotal: 27s\tremaining: 27s\n",
      "40:\tlearn: 0.7418545\ttotal: 27.7s\tremaining: 26.4s\n",
      "41:\tlearn: 0.7360120\ttotal: 28.4s\tremaining: 25.7s\n",
      "42:\tlearn: 0.7312654\ttotal: 29.1s\tremaining: 25s\n",
      "43:\tlearn: 0.7256299\ttotal: 29.8s\tremaining: 24.4s\n",
      "44:\tlearn: 0.7212593\ttotal: 30.5s\tremaining: 23.7s\n",
      "45:\tlearn: 0.7178278\ttotal: 31.1s\tremaining: 23s\n",
      "46:\tlearn: 0.7141783\ttotal: 31.8s\tremaining: 22.3s\n",
      "47:\tlearn: 0.7098304\ttotal: 32.5s\tremaining: 21.7s\n",
      "48:\tlearn: 0.7058831\ttotal: 33.2s\tremaining: 21s\n",
      "49:\tlearn: 0.7016487\ttotal: 33.9s\tremaining: 20.3s\n",
      "50:\tlearn: 0.6976173\ttotal: 34.6s\tremaining: 19.7s\n",
      "51:\tlearn: 0.6936053\ttotal: 35.3s\tremaining: 19s\n",
      "52:\tlearn: 0.6896012\ttotal: 36s\tremaining: 18.3s\n",
      "53:\tlearn: 0.6849858\ttotal: 36.7s\tremaining: 17.7s\n",
      "54:\tlearn: 0.6814419\ttotal: 37.4s\tremaining: 17s\n",
      "55:\tlearn: 0.6784803\ttotal: 38.1s\tremaining: 16.3s\n",
      "56:\tlearn: 0.6736696\ttotal: 38.8s\tremaining: 15.7s\n",
      "57:\tlearn: 0.6707733\ttotal: 39.5s\tremaining: 15s\n",
      "58:\tlearn: 0.6686072\ttotal: 40.2s\tremaining: 14.3s\n",
      "59:\tlearn: 0.6645646\ttotal: 40.9s\tremaining: 13.6s\n",
      "60:\tlearn: 0.6601051\ttotal: 41.6s\tremaining: 13s\n",
      "61:\tlearn: 0.6566280\ttotal: 42.4s\tremaining: 12.3s\n",
      "62:\tlearn: 0.6529027\ttotal: 43s\tremaining: 11.6s\n",
      "63:\tlearn: 0.6493769\ttotal: 43.7s\tremaining: 10.9s\n",
      "64:\tlearn: 0.6458867\ttotal: 44.4s\tremaining: 10.3s\n",
      "65:\tlearn: 0.6424868\ttotal: 45.1s\tremaining: 9.57s\n",
      "66:\tlearn: 0.6383905\ttotal: 45.8s\tremaining: 8.89s\n",
      "67:\tlearn: 0.6352430\ttotal: 46.5s\tremaining: 8.21s\n",
      "68:\tlearn: 0.6324157\ttotal: 47.2s\tremaining: 7.52s\n",
      "69:\tlearn: 0.6281451\ttotal: 47.9s\tremaining: 6.84s\n",
      "70:\tlearn: 0.6254216\ttotal: 48.6s\tremaining: 6.16s\n",
      "71:\tlearn: 0.6204397\ttotal: 49.3s\tremaining: 5.47s\n",
      "72:\tlearn: 0.6163307\ttotal: 50s\tremaining: 4.79s\n",
      "73:\tlearn: 0.6133688\ttotal: 50.6s\tremaining: 4.11s\n",
      "74:\tlearn: 0.6102715\ttotal: 51.3s\tremaining: 3.42s\n",
      "75:\tlearn: 0.6070691\ttotal: 52s\tremaining: 2.74s\n",
      "76:\tlearn: 0.6039860\ttotal: 52.7s\tremaining: 2.05s\n",
      "77:\tlearn: 0.6004809\ttotal: 53.4s\tremaining: 1.37s\n",
      "78:\tlearn: 0.5967645\ttotal: 54.1s\tremaining: 684ms\n",
      "79:\tlearn: 0.5948433\ttotal: 54.7s\tremaining: 0us\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'accuracy_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-4ee17b34e12e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_trainer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_trainer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_tester\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mac_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_tester\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mp_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mprecision_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_tester\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'macro'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mr_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrecall_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_tester\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'macro'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'accuracy_score' is not defined"
     ]
    }
   ],
   "source": [
    "model = CatBoostClassifier(depth=9, bootstrap_type= 'Bayesian', loss_function = 'MultiClass', iterations=80, learning_rate=0.4, l2_leaf_reg=26)\n",
    "model.fit(X_trainer, y_trainer)\n",
    "y_pred = model.predict(X_tester)\n",
    "ac_score = (accuracy_score(y_tester, y_pred))\n",
    "p_score = (precision_score(y_tester, y_pred, average='macro'))\n",
    "r_score = (recall_score(y_tester, y_pred, average='macro'))\n",
    "f1_score = (f1_score(y_tester, y_pred, average='macro'))\n",
    "print(ac_score)\n",
    "print(p_score)\n",
    "print(r_score)\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 1.2713026\ttotal: 698ms\tremaining: 55.1s\n",
      "1:\tlearn: 1.1741230\ttotal: 1.38s\tremaining: 53.9s\n",
      "2:\tlearn: 1.1301406\ttotal: 2.09s\tremaining: 53.7s\n",
      "3:\tlearn: 1.0856734\ttotal: 2.79s\tremaining: 53s\n",
      "4:\tlearn: 1.0589595\ttotal: 3.48s\tremaining: 52.3s\n",
      "5:\tlearn: 1.0343295\ttotal: 4.18s\tremaining: 51.5s\n",
      "6:\tlearn: 1.0129892\ttotal: 4.88s\tremaining: 50.9s\n",
      "7:\tlearn: 0.9950448\ttotal: 5.57s\tremaining: 50.2s\n",
      "8:\tlearn: 0.9781915\ttotal: 6.26s\tremaining: 49.4s\n",
      "9:\tlearn: 0.9629398\ttotal: 6.96s\tremaining: 48.7s\n",
      "10:\tlearn: 0.9517231\ttotal: 7.65s\tremaining: 48s\n",
      "11:\tlearn: 0.9404595\ttotal: 8.34s\tremaining: 47.2s\n",
      "12:\tlearn: 0.9301228\ttotal: 9.03s\tremaining: 46.5s\n",
      "13:\tlearn: 0.9191849\ttotal: 9.73s\tremaining: 45.9s\n",
      "14:\tlearn: 0.9096659\ttotal: 10.4s\tremaining: 45.2s\n",
      "15:\tlearn: 0.9001949\ttotal: 11.1s\tremaining: 44.4s\n",
      "16:\tlearn: 0.8914413\ttotal: 11.8s\tremaining: 43.7s\n",
      "17:\tlearn: 0.8819735\ttotal: 12.5s\tremaining: 43.1s\n",
      "18:\tlearn: 0.8743611\ttotal: 13.2s\tremaining: 42.3s\n",
      "19:\tlearn: 0.8650124\ttotal: 13.9s\tremaining: 41.7s\n",
      "20:\tlearn: 0.8582929\ttotal: 14.6s\tremaining: 40.9s\n",
      "21:\tlearn: 0.8510230\ttotal: 15.3s\tremaining: 40.3s\n",
      "22:\tlearn: 0.8429690\ttotal: 16s\tremaining: 39.5s\n",
      "23:\tlearn: 0.8352786\ttotal: 16.7s\tremaining: 38.9s\n",
      "24:\tlearn: 0.8290428\ttotal: 17.3s\tremaining: 38.2s\n",
      "25:\tlearn: 0.8224297\ttotal: 18s\tremaining: 37.5s\n",
      "26:\tlearn: 0.8154803\ttotal: 18.7s\tremaining: 36.7s\n",
      "27:\tlearn: 0.8096535\ttotal: 19.4s\tremaining: 36s\n",
      "28:\tlearn: 0.8053349\ttotal: 20.1s\tremaining: 35.3s\n",
      "29:\tlearn: 0.7984777\ttotal: 20.8s\tremaining: 34.7s\n",
      "30:\tlearn: 0.7932542\ttotal: 21.5s\tremaining: 34s\n",
      "31:\tlearn: 0.7888278\ttotal: 22.2s\tremaining: 33.3s\n",
      "32:\tlearn: 0.7839920\ttotal: 22.9s\tremaining: 32.6s\n",
      "33:\tlearn: 0.7780600\ttotal: 23.6s\tremaining: 31.9s\n",
      "34:\tlearn: 0.7719215\ttotal: 24.3s\tremaining: 31.2s\n",
      "35:\tlearn: 0.7673918\ttotal: 25s\tremaining: 30.5s\n",
      "36:\tlearn: 0.7628234\ttotal: 25.7s\tremaining: 29.8s\n",
      "37:\tlearn: 0.7571011\ttotal: 26.3s\tremaining: 29.1s\n",
      "38:\tlearn: 0.7504829\ttotal: 27s\tremaining: 28.4s\n",
      "39:\tlearn: 0.7462167\ttotal: 27.7s\tremaining: 27.7s\n",
      "40:\tlearn: 0.7418545\ttotal: 28.4s\tremaining: 27s\n",
      "41:\tlearn: 0.7360120\ttotal: 29.1s\tremaining: 26.4s\n",
      "42:\tlearn: 0.7312654\ttotal: 29.8s\tremaining: 25.7s\n",
      "43:\tlearn: 0.7256299\ttotal: 30.5s\tremaining: 25s\n",
      "44:\tlearn: 0.7212593\ttotal: 31.2s\tremaining: 24.3s\n",
      "45:\tlearn: 0.7178278\ttotal: 31.9s\tremaining: 23.6s\n",
      "46:\tlearn: 0.7141783\ttotal: 32.6s\tremaining: 22.9s\n",
      "47:\tlearn: 0.7098304\ttotal: 33.3s\tremaining: 22.2s\n",
      "48:\tlearn: 0.7058831\ttotal: 33.9s\tremaining: 21.5s\n",
      "49:\tlearn: 0.7016487\ttotal: 34.6s\tremaining: 20.8s\n",
      "50:\tlearn: 0.6976173\ttotal: 35.3s\tremaining: 20.1s\n",
      "51:\tlearn: 0.6936053\ttotal: 36s\tremaining: 19.4s\n",
      "52:\tlearn: 0.6896012\ttotal: 36.7s\tremaining: 18.7s\n",
      "53:\tlearn: 0.6849858\ttotal: 37.4s\tremaining: 18s\n",
      "54:\tlearn: 0.6814419\ttotal: 38s\tremaining: 17.3s\n",
      "55:\tlearn: 0.6784803\ttotal: 38.7s\tremaining: 16.6s\n",
      "56:\tlearn: 0.6736696\ttotal: 39.4s\tremaining: 15.9s\n",
      "57:\tlearn: 0.6707733\ttotal: 40.1s\tremaining: 15.2s\n",
      "58:\tlearn: 0.6686072\ttotal: 40.8s\tremaining: 14.5s\n",
      "59:\tlearn: 0.6645646\ttotal: 41.5s\tremaining: 13.8s\n",
      "60:\tlearn: 0.6601051\ttotal: 42.2s\tremaining: 13.1s\n",
      "61:\tlearn: 0.6566280\ttotal: 42.9s\tremaining: 12.4s\n",
      "62:\tlearn: 0.6529027\ttotal: 43.5s\tremaining: 11.7s\n",
      "63:\tlearn: 0.6493769\ttotal: 44.2s\tremaining: 11.1s\n",
      "64:\tlearn: 0.6458867\ttotal: 44.9s\tremaining: 10.4s\n",
      "65:\tlearn: 0.6424868\ttotal: 45.6s\tremaining: 9.68s\n",
      "66:\tlearn: 0.6383905\ttotal: 46.3s\tremaining: 8.98s\n",
      "67:\tlearn: 0.6352430\ttotal: 47s\tremaining: 8.29s\n",
      "68:\tlearn: 0.6324157\ttotal: 47.6s\tremaining: 7.59s\n",
      "69:\tlearn: 0.6281451\ttotal: 48.3s\tremaining: 6.91s\n",
      "70:\tlearn: 0.6254216\ttotal: 49s\tremaining: 6.21s\n",
      "71:\tlearn: 0.6204397\ttotal: 49.8s\tremaining: 5.53s\n",
      "72:\tlearn: 0.6163307\ttotal: 50.5s\tremaining: 4.84s\n",
      "73:\tlearn: 0.6133688\ttotal: 51.1s\tremaining: 4.15s\n",
      "74:\tlearn: 0.6102715\ttotal: 51.8s\tremaining: 3.45s\n",
      "75:\tlearn: 0.6070691\ttotal: 52.5s\tremaining: 2.76s\n",
      "76:\tlearn: 0.6039860\ttotal: 53.2s\tremaining: 2.07s\n",
      "77:\tlearn: 0.6004809\ttotal: 53.9s\tremaining: 1.38s\n",
      "78:\tlearn: 0.5967645\ttotal: 54.6s\tremaining: 691ms\n",
      "79:\tlearn: 0.5948433\ttotal: 55.3s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2,\n",
       "             estimator=<catboost.core.CatBoostClassifier object at 0x0000023789970288>,\n",
       "             n_jobs=-1,\n",
       "             param_grid={'l2_leaf_reg': [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22,\n",
       "                                         24, 26, 28],\n",
       "                         'learning_rate': [0.6, 0.1, 0.4, 0.8]})"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "model = CatBoostClassifier(depth=9, iterations=80)\n",
    "parameters = {'learning_rate': [0.6, 0.1, 0.4, 0.8],\n",
    "               'l2_leaf_reg': list(range(2, 30, 2))}\n",
    "    \n",
    "randm = GridSearchCV(estimator=model, param_grid = parameters, cv = 2,  n_jobs=-1)\n",
    "randm.fit(X_trainer, y_trainer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The best estimator across ALL searched params:\n",
      " <catboost.core.CatBoostClassifier object at 0x00000237920CC048>\n",
      "\n",
      " The best score across ALL searched params:\n",
      " 0.6530229868900106\n",
      "\n",
      " The best parameters across ALL searched params:\n",
      " {'l2_leaf_reg': 26, 'learning_rate': 0.4}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n The best estimator across ALL searched params:\\n\", randm.best_estimator_)\n",
    "    \n",
    "print(\"\\n The best score across ALL searched params:\\n\", randm.best_score_)\n",
    "    \n",
    "print(\"\\n The best parameters across ALL searched params:\\n\", randm.best_params_)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The best estimator across ALL searched params:\n",
      " <catboost.core.CatBoostClassifier object at 0x0000023791FA8608>\n",
      "\n",
      " The best score across ALL searched params:\n",
      " 0.6437770235802374\n",
      "\n",
      " The best parameters across ALL searched params:\n",
      " {'depth': 9, 'iterations': 80}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n The best estimator across ALL searched params:\\n\", randm.best_estimator_)\n",
    "    \n",
    "print(\"\\n The best score across ALL searched params:\\n\", randm.best_score_)\n",
    "    \n",
    "print(\"\\n The best parameters across ALL searched params:\\n\", randm.best_params_)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('ProgramData': virtualenv)",
   "language": "python",
   "name": "python37464bitprogramdatavirtualenv2203a48eb30e4608bccee8d0c91a3fd7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
