{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Lenovo X230\\\\Downloads'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "# Customise our plotting settings\n",
    "rcParams['figure.figsize'] = 10, 5\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')\n",
    "sample_data = pd.read_csv('sample_submission.csv')\n",
    "testing = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making copies of original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data.copy()\n",
    "sample = sample_data.copy()\n",
    "test = testing.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15819 entries, 0 to 15818\n",
      "Data columns (total 3 columns):\n",
      "sentiment    15819 non-null int64\n",
      "message      15819 non-null object\n",
      "tweetid      15819 non-null int64\n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 370.9+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()\n",
    "# non-null equals number of entries for all columns, that means no null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for the existence of NaN values in a cell:\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for the existence of NaN values in a cell:\n",
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['sentiment'].value_counts()\n",
    "#the classes are inbalanced, class 1 is more than 50% of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#percentage of class 1\n",
    "len(train[train['sentiment']==1])/len(train) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#percentage of class 2\n",
    "len(train[train['sentiment']==2])/len(train) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#percentage of class 0\n",
    "len(train[train['sentiment']==0])/len(train) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#percentage of class -1\n",
    "len(train[train['sentiment']==-1])/len(train) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BALANCING CLASSES( upsample minority class + downsample majority class)¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sum = train[['sentiment', 'message']].groupby('sentiment').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot by sentimental classes\n",
    "train_sum.sort_values('message', ascending=False).plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As usual, we start by importing our modules\n",
    "from sklearn.utils import resample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4265"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#class size equals to 50% of the majority class(can change it to any number which will give best result)\n",
    "\n",
    "\n",
    "\n",
    "class_size = int(len(train[train['sentiment']==1]))# Upsampling\n",
    "\n",
    "#class_size = int(len(train[train['sentiment']==1])/2) # upsample minority class + downsample majority class\n",
    "\n",
    "#class_size = int(len(train[train['sentiment']==-1])) # DownSampling\n",
    "\n",
    "\n",
    "class_size\n",
    "\n",
    "#uncomment the others to try them out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "one = train[train['sentiment']==1]\n",
    "two = train[train['sentiment']==2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    4265\n",
       "2    3640\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "one_downsampled = resample(one,\n",
    "                          replace=False, # sample without replacement (no need to duplicate observations)\n",
    "                          n_samples=class_size, # match number in minority class\n",
    "                          random_state=27) # reproducible results\n",
    "\n",
    "# Combine downsampled majority class with minority class\n",
    "downsampled = pd.concat([one_downsampled, two])\n",
    "\n",
    "# Check new class counts\n",
    "downsampled['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    4265\n",
       "1    4265\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upsample minority\n",
    "two_upsampled = resample(two,\n",
    "                          replace=True, # sample with replacement (we need to duplicate observations)\n",
    "                          n_samples=class_size, # match number in minority class\n",
    "                          random_state=27) # reproducible results\n",
    "\n",
    "# Combine upsampled minority class with majority class\n",
    "upsampled = pd.concat([two_upsampled, one_downsampled])\n",
    "\n",
    "# Check new class counts\n",
    "upsampled['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero = train[train['sentiment']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    4265\n",
       "1    4265\n",
       "0    4265\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_upsampled = resample(zero,\n",
    "                          replace=True, # sample with replacement (we need to duplicate observations)\n",
    "                          n_samples=class_size, # match number in minority class\n",
    "                          random_state=27) # reproducible results\n",
    "\n",
    "# Combine upsampled minority class with majority class\n",
    "upsampled0 = pd.concat([zero_upsampled, upsampled])\n",
    "\n",
    "# Check new class counts\n",
    "upsampled0['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "minus_one = train[train['sentiment']==-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1    4265\n",
       " 2    4265\n",
       " 1    4265\n",
       " 0    4265\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minus_one_upsampled = resample(minus_one,\n",
    "                          replace=True, # sample with replacement (we need to duplicate observations)\n",
    "                          n_samples=class_size, # match number in minority class\n",
    "                          random_state=27) # reproducible results\n",
    "\n",
    "# Combine upsampled minority class with majority class\n",
    "upsampled_ = pd.concat([minus_one_upsampled, upsampled0])\n",
    "\n",
    "# Check new class counts\n",
    "upsampled_['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>12819</td>\n",
       "      <td>-1</td>\n",
       "      <td>@ConnorDukeSmith climate change is fake #iamright</td>\n",
       "      <td>69529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6687</td>\n",
       "      <td>-1</td>\n",
       "      <td>RT @PolitixGal: When govt controls scientific ...</td>\n",
       "      <td>316086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9172</td>\n",
       "      <td>-1</td>\n",
       "      <td>@DRUDGE_REPORT 'Trump veers off script on clim...</td>\n",
       "      <td>106644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12863</td>\n",
       "      <td>-1</td>\n",
       "      <td>I'm going to start replacing 'climate change' ...</td>\n",
       "      <td>527580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1878</td>\n",
       "      <td>-1</td>\n",
       "      <td>RT @goburch: I just aint believing all that gl...</td>\n",
       "      <td>477516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2856</td>\n",
       "      <td>1</td>\n",
       "      <td>These beautiful paintings turn depressing clim...</td>\n",
       "      <td>642420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6325</td>\n",
       "      <td>1</td>\n",
       "      <td>RT @chunkymark: a housing bubble, a mounting d...</td>\n",
       "      <td>672682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10548</td>\n",
       "      <td>1</td>\n",
       "      <td>RT @AllBirdsWiki: @birdsblooms Animal ag is th...</td>\n",
       "      <td>630990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7929</td>\n",
       "      <td>1</td>\n",
       "      <td>@statesman Scott Pruitt Must Go, and all citiz...</td>\n",
       "      <td>554457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3149</td>\n",
       "      <td>1</td>\n",
       "      <td>RT @SwannyQLD: The imperative of reducing carb...</td>\n",
       "      <td>462669</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17060 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentiment                                            message  tweetid\n",
       "12819         -1  @ConnorDukeSmith climate change is fake #iamright    69529\n",
       "6687          -1  RT @PolitixGal: When govt controls scientific ...   316086\n",
       "9172          -1  @DRUDGE_REPORT 'Trump veers off script on clim...   106644\n",
       "12863         -1  I'm going to start replacing 'climate change' ...   527580\n",
       "1878          -1  RT @goburch: I just aint believing all that gl...   477516\n",
       "...          ...                                                ...      ...\n",
       "2856           1  These beautiful paintings turn depressing clim...   642420\n",
       "6325           1  RT @chunkymark: a housing bubble, a mounting d...   672682\n",
       "10548          1  RT @AllBirdsWiki: @birdsblooms Animal ag is th...   630990\n",
       "7929           1  @statesman Scott Pruitt Must Go, and all citiz...   554457\n",
       "3149           1  RT @SwannyQLD: The imperative of reducing carb...   462669\n",
       "\n",
       "[17060 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=upsampled_ \n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sum = train[['sentiment', 'message']].groupby('sentiment').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot by sentimental classes\n",
    "train_sum.sort_values('message', ascending=False).plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove urls\n",
    "pattern_url = r'http[s]?://(?:[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f][0-9A-Fa-f]))+'\n",
    "subs_url = r'url-web'\n",
    "train['message'] = train['message'].replace(to_replace = pattern_url, value = subs_url, regex = True)\n",
    "test['message'] = test['message'].replace(to_replace = pattern_url, value = subs_url, regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make lower case\n",
    "train['message'] = train['message'].str.lower()\n",
    "test['message'] = test['message'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip out punctuation marks and numerals\n",
    "import string\n",
    "def remove_punctuation_numbers(post):\n",
    "    punc_numbers = string.punctuation + '0123456789'\n",
    "    return ''.join([l for l in post if l not in punc_numbers])\n",
    "\n",
    "train['message'] = train['message'].apply(remove_punctuation_numbers)\n",
    "test['message'] = test['message'].apply(remove_punctuation_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rt politixgal when govt controls scientific research via grant money there is no truth only agenda obama regime pushed global warming'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"message\"].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenisation\n",
    "from nltk.tokenize import word_tokenize, TreebankWordTokenizer\n",
    "tokeniser = TreebankWordTokenizer()\n",
    "train['message'] = train['message'].apply(tokeniser.tokenize)\n",
    "test['message'] = test['message'].apply(tokeniser.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rt',\n",
       " 'politixgal',\n",
       " 'when',\n",
       " 'govt',\n",
       " 'controls',\n",
       " 'scientific',\n",
       " 'research',\n",
       " 'via',\n",
       " 'grant',\n",
       " 'money',\n",
       " 'there',\n",
       " 'is',\n",
       " 'no',\n",
       " 'truth',\n",
       " 'only',\n",
       " 'agenda',\n",
       " 'obama',\n",
       " 'regime',\n",
       " 'pushed',\n",
       " 'global',\n",
       " 'warming']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"message\"].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Lenovo\n",
      "[nltk_data]     X230\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def mbti_lemma(words, lemmatizer):\n",
    "    return [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "train['message'] = train['message'].apply(mbti_lemma, args=(lemmatizer, ))\n",
    "test['message'] = test['message'].apply(mbti_lemma, args=(lemmatizer, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rt',\n",
       " 'politixgal',\n",
       " 'when',\n",
       " 'govt',\n",
       " 'control',\n",
       " 'scientific',\n",
       " 'research',\n",
       " 'via',\n",
       " 'grant',\n",
       " 'money',\n",
       " 'there',\n",
       " 'is',\n",
       " 'no',\n",
       " 'truth',\n",
       " 'only',\n",
       " 'agenda',\n",
       " 'obama',\n",
       " 'regime',\n",
       " 'pushed',\n",
       " 'global',\n",
       " 'warming']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"message\"].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# Removing Stop Words\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))\n",
    "def remove_stop_words(tokens):\n",
    "    return [t for t in tokens if t not in stopwords.words('english')]\n",
    "train['message'] = train['message'].apply(remove_stop_words)\n",
    "test['message'] = test['message'].apply(remove_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rt',\n",
       " 'politixgal',\n",
       " 'govt',\n",
       " 'control',\n",
       " 'scientific',\n",
       " 'research',\n",
       " 'via',\n",
       " 'grant',\n",
       " 'money',\n",
       " 'truth',\n",
       " 'agenda',\n",
       " 'obama',\n",
       " 'regime',\n",
       " 'pushed',\n",
       " 'global',\n",
       " 'warming']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"message\"].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = train['message'].astype(str)\n",
    "y = train['sentiment']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Naïve Bayes:\n",
    "text_clf_nb = Pipeline([('tfidf', TfidfVectorizer(lowercase=False)),\n",
    "                     ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "# Linear SVC:\n",
    "text_clf_lsvc = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                     ('clf', LinearSVC()),\n",
    "])\n",
    "\n",
    "#Logistic regression(one over the rest)\n",
    "text_clf_logReg= Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                          ('clf', LogisticRegression(multi_class='ovr')),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tfidf',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=False, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, use_idf=True,\n",
       "                                 vocabulary=None)),\n",
       "                ('clf',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf_nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form a prediction set\n",
    "predictions = text_clf_nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[803  20  17  11]\n",
      " [ 82 647  74  58]\n",
      " [105  61 506 152]\n",
      " [ 20  12  42 802]]\n"
     ]
    }
   ],
   "source": [
    "# Report the confusion matrix\n",
    "from sklearn import metrics\n",
    "print(metrics.confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.80      0.94      0.86       851\n",
      "           0       0.87      0.75      0.81       861\n",
      "           1       0.79      0.61      0.69       824\n",
      "           2       0.78      0.92      0.84       876\n",
      "\n",
      "    accuracy                           0.81      3412\n",
      "   macro avg       0.81      0.81      0.80      3412\n",
      "weighted avg       0.81      0.81      0.80      3412\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print a classification report\n",
    "print(metrics.classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8083235638921453\n"
     ]
    }
   ],
   "source": [
    "# Print the overall accuracy\n",
    "print(metrics.accuracy_score(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tfidf',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, use_idf=True,\n",
       "                                 vocabulary=None)),\n",
       "                ('clf',\n",
       "                 LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                           fit_intercept=True, intercept_scaling=1,\n",
       "                           loss='squared_hinge', max_iter=1000,\n",
       "                           multi_class='ovr', penalty='l2', random_state=None,\n",
       "                           tol=0.0001, verbose=0))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf_lsvc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form a prediction set\n",
    "predictions = text_clf_lsvc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[823  13   8   7]\n",
      " [ 26 768  44  23]\n",
      " [ 50  76 587 111]\n",
      " [  8  13  42 813]]\n"
     ]
    }
   ],
   "source": [
    "# Report the confusion matrix\n",
    "from sklearn import metrics\n",
    "print(metrics.confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.91      0.97      0.94       851\n",
      "           0       0.88      0.89      0.89       861\n",
      "           1       0.86      0.71      0.78       824\n",
      "           2       0.85      0.93      0.89       876\n",
      "\n",
      "    accuracy                           0.88      3412\n",
      "   macro avg       0.88      0.87      0.87      3412\n",
      "weighted avg       0.88      0.88      0.87      3412\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print a classification report\n",
    "print(metrics.classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8766119577960141\n"
     ]
    }
   ],
   "source": [
    "# Print the overall accuracy\n",
    "print(metrics.accuracy_score(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo X230\\Anaconda3\\ana\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tfidf',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, use_idf=True,\n",
       "                                 vocabulary=None)),\n",
       "                ('clf',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='ovr', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='warn', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf_logReg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form a prediction set\n",
    "predictions = text_clf_logReg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[763  44  25  19]\n",
      " [ 36 704  66  55]\n",
      " [ 64  85 531 144]\n",
      " [ 12  21  51 792]]\n"
     ]
    }
   ],
   "source": [
    "# Report the confusion matrix\n",
    "from sklearn import metrics\n",
    "print(metrics.confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.87      0.90      0.88       851\n",
      "           0       0.82      0.82      0.82       861\n",
      "           1       0.79      0.64      0.71       824\n",
      "           2       0.78      0.90      0.84       876\n",
      "\n",
      "    accuracy                           0.82      3412\n",
      "   macro avg       0.82      0.82      0.81      3412\n",
      "weighted avg       0.82      0.82      0.81      3412\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print a classification report\n",
    "print(metrics.classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8177022274325909\n"
     ]
    }
   ],
   "source": [
    "# Print the overall accuracy\n",
    "print(metrics.accuracy_score(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# submission to kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetid</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>169760</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>35326</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>224985</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>476263</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>872928</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10541</td>\n",
       "      <td>895714</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10542</td>\n",
       "      <td>875167</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10543</td>\n",
       "      <td>78329</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10544</td>\n",
       "      <td>867455</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10545</td>\n",
       "      <td>470892</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10546 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       tweetid  sentiment\n",
       "0       169760          1\n",
       "1        35326          1\n",
       "2       224985          1\n",
       "3       476263          1\n",
       "4       872928          1\n",
       "...        ...        ...\n",
       "10541   895714          1\n",
       "10542   875167          1\n",
       "10543    78329          1\n",
       "10544   867455          1\n",
       "10545   470892          1\n",
       "\n",
       "[10546 rows x 2 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetid</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>169760</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>35326</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>224985</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>476263</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>872928</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10541</td>\n",
       "      <td>895714</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10542</td>\n",
       "      <td>875167</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10543</td>\n",
       "      <td>78329</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10544</td>\n",
       "      <td>867455</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10545</td>\n",
       "      <td>470892</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10546 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       tweetid  sentiment\n",
       "0       169760          2\n",
       "1        35326          1\n",
       "2       224985          1\n",
       "3       476263          1\n",
       "4       872928          0\n",
       "...        ...        ...\n",
       "10541   895714          1\n",
       "10542   875167          1\n",
       "10543    78329          2\n",
       "10544   867455          0\n",
       "10545   470892          2\n",
       "\n",
       "[10546 rows x 2 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = sample.copy()\n",
    "t['sentiment'] = text_clf_lsvc.fit(X_train, y_train).predict(test['message'].astype(str))\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.to_csv('sub5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
