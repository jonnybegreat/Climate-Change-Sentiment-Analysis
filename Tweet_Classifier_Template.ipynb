{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classification ninth attempt.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RperqSUu-vB9",
        "colab_type": "text"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxFbP5UZCpAy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download corpuses required"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZIAxgHP_asi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import packages\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZ4znW3bAP_O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7sSmlnbBIUG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create new columns for : number of characters/tweet,emoticons/tweet,number of emoticons/tweet,list of # per tweet, list of mentions/tweet,number of words/tweet(might need to be done after tokenization),Named Entities"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8an5nHJN-7G6"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cz12OfSC_iaZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove links"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rclm-4as_qQi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenize (experiment with different tokenizers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JT65Uu8x_q1R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert to lowercase"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qm-7dZmy_xkB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# POS tagging (experiment)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XTR5YZxDuKZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Perform NER and add your own values (experiment)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECTPBZC1_yRJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Stemming (experiment)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7LO20V5_yUB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lemmatization (experiment)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dd7uIe6g_yW7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove punctuation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiwA_nJ8_yZK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove numbers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbff3wt3AIAR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create custom stopwords list (add 'RT')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kabJAXU_yeE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fHOroGqr-8W0"
      },
      "source": [
        "# EDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cDrhZNtAYM1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Most common words for all tweets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEAo-iafAc8g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Most common words for tweets per category"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTgm8fRFEP4R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Most common bigrams per category"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ax5B-lUdEbYx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Average word count for each tweet per category"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FQdNqd8EhaB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Average character count for each tweet per category"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nopvt-s7C5ZZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Investigate hashtags"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uH1JRkEBC9vR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Investigate mentions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orfsWtOZDB05",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Investigate Retweets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3oYwXCmFBfx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Investigate Emoticons"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whOQFEgUFfH5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Investigate Clustering"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5WryQdBAoxp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Revise stopwords list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EmnRD-VV_CQU"
      },
      "source": [
        "# Vectorize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpUVGxESGLqu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import vectorizers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57FkJlZaGSKw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tfidf vectorizer for base model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaD24xb_GXow",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Perform gridsearch or vectorizer parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbyRnexdG0Kh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create function to check different vectorizers performance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uy8xFWlb4GYX"
      },
      "source": [
        "# Topic Modelling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SRENWp-c4GYz",
        "colab": {}
      },
      "source": [
        "# LDA clustering - have to use countvectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTpXSPJB-T0G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# NNMF - use Tfidf Vectorizer - do this in conjunction with 1vR model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6w4owLNu-84V"
      },
      "source": [
        "# Balance Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fA-54GbG_zq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use upsampling for base model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMS3JyuHHJ15",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test different balancing techniques"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxmtvOvpHOtB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Understand which data needs to be resampled and what it does to the model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zKqmOoMM-9dd"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AY3xZDE-H3fO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import easy models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J41m-uW-H-5R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fit Models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CFhf765IAxx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check best performing for baseline model (use cross validation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VlMBVu6IE0x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Perform grid search for best parameters for best model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTR1cvhAImhx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Investigate more complex modelling techniqes and implement separately"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4sDx_tV0_LQM"
      },
      "source": [
        "# Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HP_XKd2Itny",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Look at model performance (confusion matrix,accuracy,f1score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPqbvTnxItt_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Figure out what is causing the false positives and false negatives and update model to fix these"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CqJXvLElUt7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aahucuORlU3t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_T9aqXrBlU1R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XodDHXewlUzg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urU14QTwlUxa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bmci07RjlUrc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "n12cm0X9--X0"
      },
      "source": [
        "# Kaggle Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_kQ4goGYDJP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
        "import itertools, string, operator, re, unicodedata, nltk\n",
        "from operator import itemgetter\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import TweetTokenizer, RegexpTokenizer\n",
        "from bs4 import BeautifulSoup\n",
        "import numpy as np\n",
        "from itertools import combinations\n",
        "from gensim.models import Phrases\n",
        "from collections import Counter\n",
        "\n",
        "#Contraction map\n",
        "c_dict = {\n",
        "  \"ain't\": \"am not\",\n",
        "  \"aren't\": \"are not\",\n",
        "  \"can't\": \"cannot\",\n",
        "  \"can't've\": \"cannot have\",\n",
        "  \"'cause\": \"because\",\n",
        "  \"could've\": \"could have\",\n",
        "  \"couldn't\": \"could not\",\n",
        "  \"couldn't've\": \"could not have\",\n",
        "  \"didn't\": \"did not\",\n",
        "  \"doesn't\": \"does not\",\n",
        "  \"don't\": \"do not\",\n",
        "  \"hadn't\": \"had not\",\n",
        "  \"hadn't've\": \"had not have\",\n",
        "  \"hasn't\": \"has not\",\n",
        "  \"haven't\": \"have not\",\n",
        "  \"he'd\": \"he would\",\n",
        "  \"he'd've\": \"he would have\",\n",
        "  \"he'll\": \"he will\",\n",
        "  \"he'll've\": \"he will have\",\n",
        "  \"he's\": \"he is\",\n",
        "  \"how'd\": \"how did\",\n",
        "  \"how'd'y\": \"how do you\",\n",
        "  \"how'll\": \"how will\",\n",
        "  \"how's\": \"how is\",\n",
        "  \"i'd\": \"I would\",\n",
        "  \"i'd've\": \"I would have\",\n",
        "  \"i'll\": \"I will\",\n",
        "  \"i'll've\": \"I will have\",\n",
        "  \"i'm\": \"I am\",\n",
        "  \"i've\": \"I have\",\n",
        "  \"isn't\": \"is not\",\n",
        "  \"it'd\": \"it had\",\n",
        "  \"it'd've\": \"it would have\",\n",
        "  \"it'll\": \"it will\",\n",
        "  \"it'll've\": \"it will have\",\n",
        "  \"it's\": \"it is\",\n",
        "  \"let's\": \"let us\",\n",
        "  \"ma'am\": \"madam\",\n",
        "  \"mayn't\": \"may not\",\n",
        "  \"might've\": \"might have\",\n",
        "  \"mightn't\": \"might not\",\n",
        "  \"mightn't've\": \"might not have\",\n",
        "  \"must've\": \"must have\",\n",
        "  \"mustn't\": \"must not\",\n",
        "  \"mustn't've\": \"must not have\",\n",
        "  \"needn't\": \"need not\",\n",
        "  \"needn't've\": \"need not have\",\n",
        "  \"o'clock\": \"of the clock\",\n",
        "  \"oughtn't\": \"ought not\",\n",
        "  \"oughtn't've\": \"ought not have\",\n",
        "  \"shan't\": \"shall not\",\n",
        "  \"sha'n't\": \"shall not\",\n",
        "  \"shan't've\": \"shall not have\",\n",
        "  \"she'd\": \"she would\",\n",
        "  \"she'd've\": \"she would have\",\n",
        "  \"she'll\": \"she will\",\n",
        "  \"she'll've\": \"she will have\",\n",
        "  \"she's\": \"she is\",\n",
        "  \"should've\": \"should have\",\n",
        "  \"shouldn't\": \"should not\",\n",
        "  \"shouldn't've\": \"should not have\",\n",
        "  \"so've\": \"so have\",\n",
        "  \"so's\": \"so is\",\n",
        "  \"that'd\": \"that would\",\n",
        "  \"that'd've\": \"that would have\",\n",
        "  \"that's\": \"that is\",\n",
        "  \"there'd\": \"there had\",\n",
        "  \"there'd've\": \"there would have\",\n",
        "  \"there's\": \"there is\",\n",
        "  \"they'd\": \"they would\",\n",
        "  \"they'd've\": \"they would have\",\n",
        "  \"they'll\": \"they will\",\n",
        "  \"they'll've\": \"they will have\",\n",
        "  \"they're\": \"they are\",\n",
        "  \"they've\": \"they have\",\n",
        "  \"to've\": \"to have\",\n",
        "  \"wasn't\": \"was not\",\n",
        "  \"we'd\": \"we had\",\n",
        "  \"we'd've\": \"we would have\",\n",
        "  \"we'll\": \"we will\",\n",
        "  \"we'll've\": \"we will have\",\n",
        "  \"we're\": \"we are\",\n",
        "  \"we've\": \"we have\",\n",
        "  \"weren't\": \"were not\",\n",
        "  \"what'll\": \"what will\",\n",
        "  \"what'll've\": \"what will have\",\n",
        "  \"what're\": \"what are\",\n",
        "  \"what's\": \"what is\",\n",
        "  \"what've\": \"what have\",\n",
        "  \"when's\": \"when is\",\n",
        "  \"when've\": \"when have\",\n",
        "  \"where'd\": \"where did\",\n",
        "  \"where's\": \"where is\",\n",
        "  \"where've\": \"where have\",\n",
        "  \"who'll\": \"who will\",\n",
        "  \"who'll've\": \"who will have\",\n",
        "  \"who's\": \"who is\",\n",
        "  \"who've\": \"who have\",\n",
        "  \"why's\": \"why is\",\n",
        "  \"why've\": \"why have\",\n",
        "  \"will've\": \"will have\",\n",
        "  \"won't\": \"will not\",\n",
        "  \"won't've\": \"will not have\",\n",
        "  \"would've\": \"would have\",\n",
        "  \"wouldn't\": \"would not\",\n",
        "  \"wouldn't've\": \"would not have\",\n",
        "  \"y'all\": \"you all\",\n",
        "  \"y'alls\": \"you alls\",\n",
        "  \"y'all'd\": \"you all would\",\n",
        "  \"y'all'd've\": \"you all would have\",\n",
        "  \"y'all're\": \"you all are\",\n",
        "  \"y'all've\": \"you all have\",\n",
        "  \"you'd\": \"you had\",\n",
        "  \"you'd've\": \"you would have\",\n",
        "  \"you'll\": \"you you will\",\n",
        "  \"you'll've\": \"you you will have\",\n",
        "  \"you're\": \"you are\",\n",
        "  \"you've\": \"you have\"\n",
        "}\n",
        "\n",
        "c_re = re.compile('(%s)' % '|'.join(c_dict.keys()))\n",
        "\n",
        "add_stop = ['', ' ', 'say', 's', 'u', 'ap', 'afp', '....', 'n', '\\\\','rt',' ...', '... ','…','¢','â','¬','...','ã',',','¦']\n",
        "\n",
        "stop_words = ENGLISH_STOP_WORDS.union(add_stop)\n",
        "\n",
        "tokenizer = TweetTokenizer()\n",
        "pattern = r\"(?u)\\b\\w\\w+\\b\" \n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "punc = list(set(string.punctuation))\n",
        "\n",
        "def casual_tokenizer(text): #Splits words on white spaces (leaves contractions intact) and splits out trailing punctuation\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    return tokens\n",
        "\n",
        "#Function to replace the nltk pos tags with the corresponding wordnet pos tag to use the wordnet lemmatizer\n",
        "def get_word_net_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return None\n",
        "    \n",
        "def lemma_wordnet(tagged_text):\n",
        "    final = []\n",
        "    for word, tag in tagged_text:\n",
        "        wordnet_tag = get_word_net_pos(tag)\n",
        "        if wordnet_tag is None:\n",
        "            final.append(lemmatizer.lemmatize(word))\n",
        "        else:\n",
        "            final.append(lemmatizer.lemmatize(word, pos=wordnet_tag))\n",
        "    return final\n",
        "\n",
        "def expandContractions(text, c_re=c_re):\n",
        "    def replace(match):\n",
        "        return c_dict[match.group(0)]\n",
        "    return c_re.sub(replace, text)\n",
        "\n",
        "def remove_html(text):\n",
        "    soup = BeautifulSoup(text, \"html5lib\")\n",
        "    tags_del = soup.get_text()\n",
        "    uni = unicodedata.normalize(\"NFKD\", tags_del)\n",
        "    bracket_del = re.sub(r'\\[.*?\\]', '  ', uni)\n",
        "    apostrphe = re.sub('’', \"'\", bracket_del)\n",
        "    string = apostrphe.replace('\\r','  ')\n",
        "    string = string.replace('\\n','  ')\n",
        "    extra_space = re.sub(' +',' ', string)\n",
        "    return extra_space\n",
        "\n",
        "def process_text(text):\n",
        "    soup = BeautifulSoup(text, \"lxml\")\n",
        "    tags_del = soup.get_text()\n",
        "    no_html = re.sub('<[^>]*>', '', tags_del)\n",
        "    tokenized = casual_tokenizer(no_html)\n",
        "    lower = [item.lower() for item in tokenized]\n",
        "    decontract = [expandContractions(item, c_re=c_re) for item in lower]\n",
        "    tagged = nltk.pos_tag(decontract)\n",
        "    lemma = lemma_wordnet(tagged)\n",
        "    no_num = [re.sub('[0-9]+', '', each) for each in lemma]\n",
        "    no_punc = [w for w in no_num if w not in punc]\n",
        "    no_stop = [w for w in no_punc if w not in stop_words]\n",
        "    return no_stop\n",
        "\n",
        "def word_count(text):\n",
        "    return len(str(text).split(' '))\n",
        "\n",
        "def word_freq(clean_text_list, top_n):\n",
        "    \"\"\"\n",
        "    Word Frequency\n",
        "    \"\"\"\n",
        "    flat = [item for sublist in clean_text_list for item in sublist]\n",
        "    with_counts = Counter(flat)\n",
        "    top = with_counts.most_common(top_n)\n",
        "    word = [each[0] for each in top]\n",
        "    num = [each[1] for each in top]\n",
        "    return pd.DataFrame([word, num]).T\n",
        "\n",
        "def word_freq_bigrams(clean_text_list, top_n):\n",
        "    \"\"\"\n",
        "    Word Frequency With Bigrams\n",
        "    \"\"\"\n",
        "    bigram_model = Phrases(clean_text_list, min_count=2, threshold=1)\n",
        "    w_bigrams = list(bigram_model[clean_text_list])\n",
        "    flat_w_bigrams = [item for sublist in w_bigrams for item in sublist]\n",
        "    with_counts = Counter(flat_w_bigrams)\n",
        "    top = with_counts.most_common(top_n)\n",
        "    word = [each[0] for each in top]\n",
        "    num = [each[1] for each in top]\n",
        "    return pd.DataFrame([word, num]).T\n",
        "\n",
        "\n",
        "def bigram_freq(clean_text_list, top_n):\n",
        "    bigram_model = Phrases(clean_text_list, min_count=2, threshold=1)\n",
        "    w_bigrams = list(bigram_model[clean_text_list])\n",
        "    flat_w_bigrams = [item for sublist in w_bigrams for item in sublist]\n",
        "    bigrams = []\n",
        "    for each in flat_w_bigrams:\n",
        "        if '_' in each:\n",
        "            bigrams.append(each)\n",
        "    counts = Counter(bigrams)\n",
        "    top = counts.most_common(top_n)\n",
        "    word = [each[0] for each in top]\n",
        "    num = [each[1] for each in top]\n",
        "    return pd.DataFrame([word, num]).T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xI61v5TyYdcn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "161355d0-7df4-4c84-d175-0972cfc2b91e"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "\n",
        "'''Features'''\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "'''Classifiers'''\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "'''Metrics/Evaluation'''\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc, confusion_matrix\n",
        "from scipy import interp\n",
        "from itertools import cycle\n",
        "\n",
        "'''Plotting'''\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style('darkgrid')\n",
        "\n",
        "'''Display'''\n",
        "from IPython.core.display import display, HTML\n",
        "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.options.display.float_format = '{:,.2f}'.format"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>.container { width:95% !important; }</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgo_T30XY0o-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "6865a8fc-324a-4163-ddbb-63292e213879"
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HWW9ZlCY51X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "72e5beb5-0bf3-4a62-fdf0-9091d51171a1"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCAFe83_YUAA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/jonnybegreat/test-repo/master/twitter_train.csv')\n",
        "test_df = pd.read_csv('https://raw.githubusercontent.com/jonnybegreat/test-repo/master/twitter_test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lx2rEHtfYZiF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "3b6a8ef9-c274-4a04-f620-a8611dc54913"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>message</th>\n",
              "      <th>tweetid</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>PolySciMajor EPA chief doesn't think carbon di...</td>\n",
              "      <td>625221</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>It's not like we lack evidence of anthropogeni...</td>\n",
              "      <td>126103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>RT @RawStory: Researchers say we have three ye...</td>\n",
              "      <td>698562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>#TodayinMaker# WIRED : 2016 was a pivotal year...</td>\n",
              "      <td>573736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>RT @SoyNovioDeTodas: It's 2016, and a racist, ...</td>\n",
              "      <td>466954</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentiment                                            message  tweetid\n",
              "0          1  PolySciMajor EPA chief doesn't think carbon di...   625221\n",
              "1          1  It's not like we lack evidence of anthropogeni...   126103\n",
              "2          2  RT @RawStory: Researchers say we have three ye...   698562\n",
              "3          1  #TodayinMaker# WIRED : 2016 was a pivotal year...   573736\n",
              "4          1  RT @SoyNovioDeTodas: It's 2016, and a racist, ...   466954"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6xM6URMYqIN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "bdbe98cb-a51c-4f1b-a15b-ec79996da562"
      },
      "source": [
        "#Apply the function to preprocess the text. Tokenize, lower, expand contactions, lemmatize, remove punctuation, numbers and stop words\n",
        "df['clean_text'] = df['message'].apply(process_text)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>message</th>\n",
              "      <th>tweetid</th>\n",
              "      <th>clean_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>PolySciMajor EPA chief doesn't think carbon di...</td>\n",
              "      <td>625221</td>\n",
              "      <td>[polyscimajor, epa, chief, does not, think, ca...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>It's not like we lack evidence of anthropogeni...</td>\n",
              "      <td>126103</td>\n",
              "      <td>[it is, like, lack, evidence, anthropogenic, g...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>RT @RawStory: Researchers say we have three ye...</td>\n",
              "      <td>698562</td>\n",
              "      <td>[@rawstory, researcher, year, act, climate, ch...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>#TodayinMaker# WIRED : 2016 was a pivotal year...</td>\n",
              "      <td>573736</td>\n",
              "      <td>[#todayinmaker, wire, pivotal, year, war, clim...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>RT @SoyNovioDeTodas: It's 2016, and a racist, ...</td>\n",
              "      <td>466954</td>\n",
              "      <td>[@soynoviodetodas, it is, racist, sexist, clim...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentiment  ...                                         clean_text\n",
              "0          1  ...  [polyscimajor, epa, chief, does not, think, ca...\n",
              "1          1  ...  [it is, like, lack, evidence, anthropogenic, g...\n",
              "2          2  ...  [@rawstory, researcher, year, act, climate, ch...\n",
              "3          1  ...  [#todayinmaker, wire, pivotal, year, war, clim...\n",
              "4          1  ...  [@soynoviodetodas, it is, racist, sexist, clim...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuGMCBLuYvSk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "outputId": "8124dffc-7495-4728-df12-f2cd22dc4587"
      },
      "source": [
        "#Top 20 most frequent words for all the articles\n",
        "\n",
        "cl_text_list = df['clean_text'].tolist()\n",
        "wf = word_freq(cl_text_list, 20)\n",
        "wf.head(20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>change</td>\n",
              "      <td>12670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>climate</td>\n",
              "      <td>12645</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>global</td>\n",
              "      <td>3781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>warming</td>\n",
              "      <td>2859</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>‚</td>\n",
              "      <td>2256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>trump</td>\n",
              "      <td>1976</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>believe</td>\n",
              "      <td>1309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>think</td>\n",
              "      <td>953</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>’</td>\n",
              "      <td>917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>does not</td>\n",
              "      <td>785</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>warm</td>\n",
              "      <td>729</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>real</td>\n",
              "      <td>718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>world</td>\n",
              "      <td>657</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>it is</td>\n",
              "      <td>637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>she is</td>\n",
              "      <td>635</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>fight</td>\n",
              "      <td>622</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>people</td>\n",
              "      <td>618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>just</td>\n",
              "      <td>606</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>make</td>\n",
              "      <td>565</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>scientist</td>\n",
              "      <td>549</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            0      1\n",
              "0      change  12670\n",
              "1     climate  12645\n",
              "2      global   3781\n",
              "3     warming   2859\n",
              "4           ‚   2256\n",
              "5       trump   1976\n",
              "6     believe   1309\n",
              "7       think    953\n",
              "8           ’    917\n",
              "9    does not    785\n",
              "10       warm    729\n",
              "11       real    718\n",
              "12      world    657\n",
              "13      it is    637\n",
              "14     she is    635\n",
              "15      fight    622\n",
              "16     people    618\n",
              "17       just    606\n",
              "18       make    565\n",
              "19  scientist    549"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdhxBmfZZLQi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "3b30acd5-81be-44a6-9cfa-4cf2b11d48df"
      },
      "source": [
        "#Avg word count by category\n",
        "\n",
        "df['word_count'] = df['message'].apply(word_count)\n",
        "avg_wc = df.groupby('sentiment').mean().reset_index()\n",
        "avg_wc[['sentiment','word_count']]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>word_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1</td>\n",
              "      <td>18.30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>16.48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>18.81</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>15.37</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentiment  word_count\n",
              "0         -1       18.30\n",
              "1          0       16.48\n",
              "2          1       18.81\n",
              "3          2       15.37"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdHiU3WjbWJC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "87881aed-ceeb-4894-9b71-a67a7278e388"
      },
      "source": [
        "df['sentiment'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              " 1    8530\n",
              " 2    3640\n",
              " 0    2353\n",
              "-1    1296\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IHqTyUuZnsF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "7e7349dc-93d6-4af2-dbb8-d2612856b901"
      },
      "source": [
        "#Preparing the dataframes\n",
        "\n",
        "#Splitting the df into the different categories\n",
        "df_negative = df.loc[df['sentiment'] == -1] \n",
        "df_neutral = df.loc[df['sentiment'] == 0]\n",
        "df_positive = df.loc[df['sentiment'] == 1] \n",
        "df_news = df.loc[df['sentiment'] ==2]\n",
        "\n",
        "#Randomly sampling business and sports to create imbalanced classes\n",
        "#df_positive = df_positive.sample(n=3640, random_state=3)\n",
        "#df_neutral = df_neutral.sample(n=1296, random_state=3)\n",
        "#df_news = df_news.sample(n=1296, random_state=3)\n",
        "\n",
        "#Holding out 5 articles from each class for prediction at the end\n",
        "df_negative_holdout = df_negative.iloc[:5]\n",
        "df_neutral_holdout = df_neutral.iloc[:5]\n",
        "df_positive_holdout = df_positive.iloc[:5]\n",
        "df_news_holdout = df_news.iloc[:5]\n",
        "\n",
        "df_negative = df_negative.iloc[5:]\n",
        "df_neutral = df_neutral.iloc[5:]\n",
        "df_positive = df_positive.iloc[5:]\n",
        "df_news = df_news.iloc[5:]\n",
        "\n",
        "#Appending the dfs back together\n",
        "df = pd.concat([df_negative, df_neutral, df_positive, df_news])\n",
        "df_holdout = pd.concat([df_negative_holdout, df_neutral_holdout, df_positive_holdout, df_news_holdout])\n",
        "\n",
        "#Turning the labels into numbers\n",
        "LE = LabelEncoder()\n",
        "df['label_num'] = LE.fit_transform(df['sentiment'])\n",
        "\n",
        "display(df.groupby(['sentiment'])['message'].count())\n",
        "display(df_holdout.groupby(['sentiment'])['message'].count())\n",
        "display(df['sentiment'].unique())\n",
        "display(df['label_num'].unique())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentiment\n",
              "-1    1291\n",
              " 0    2348\n",
              " 1    8525\n",
              " 2    3635\n",
              "Name: message, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentiment\n",
              "-1    5\n",
              " 0    5\n",
              " 1    5\n",
              " 2    5\n",
              "Name: message, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([-1,  0,  1,  2])"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3])"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SI2gOi2hcpDl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "outputId": "c29aa623-e58b-48c3-ab0a-1d5d4d2a86c6"
      },
      "source": [
        "#Top 15 words by category. Taking bigrams into account\n",
        "\n",
        "top_n = 15\n",
        "\n",
        "text_neg = df_negative['clean_text'].tolist()\n",
        "text_neut = df_neutral['clean_text'].tolist()\n",
        "text_pos = df_positive['clean_text'].tolist()\n",
        "text_news = df_news['clean_text'].tolist()\n",
        "\n",
        "neg = word_freq_bigrams(text_neg, top_n=top_n)\n",
        "neut = word_freq_bigrams(text_neut, top_n=top_n)\n",
        "pos = word_freq_bigrams(text_pos, top_n=top_n)\n",
        "news = word_freq_bigrams(text_news, top_n=top_n)\n",
        "\n",
        "df_wf = pd.concat([neg, neut, pos, news], axis=1)\n",
        "cols = ['negative', 'count', 'neutral', 'count', 'positive', 'count', 'news', 'count']\n",
        "df_wf.columns = cols\n",
        "df_wf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>negative</th>\n",
              "      <th>count</th>\n",
              "      <th>neutral</th>\n",
              "      <th>count</th>\n",
              "      <th>positive</th>\n",
              "      <th>count</th>\n",
              "      <th>news</th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>climate_change</td>\n",
              "      <td>551</td>\n",
              "      <td>climate_change</td>\n",
              "      <td>985</td>\n",
              "      <td>climate_change</td>\n",
              "      <td>4339</td>\n",
              "      <td>climate_change</td>\n",
              "      <td>1845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>global_warming</td>\n",
              "      <td>360</td>\n",
              "      <td>global_warming</td>\n",
              "      <td>515</td>\n",
              "      <td>change</td>\n",
              "      <td>1895</td>\n",
              "      <td>change</td>\n",
              "      <td>811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>change</td>\n",
              "      <td>144</td>\n",
              "      <td>change</td>\n",
              "      <td>342</td>\n",
              "      <td>‚</td>\n",
              "      <td>967</td>\n",
              "      <td>global_warming</td>\n",
              "      <td>216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>warming</td>\n",
              "      <td>90</td>\n",
              "      <td>warming</td>\n",
              "      <td>215</td>\n",
              "      <td>global_warming</td>\n",
              "      <td>747</td>\n",
              "      <td>’</td>\n",
              "      <td>184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>global_warm</td>\n",
              "      <td>73</td>\n",
              "      <td>global_warm</td>\n",
              "      <td>137</td>\n",
              "      <td>does not_believe</td>\n",
              "      <td>518</td>\n",
              "      <td>trump</td>\n",
              "      <td>184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>‚</td>\n",
              "      <td>68</td>\n",
              "      <td>‚</td>\n",
              "      <td>131</td>\n",
              "      <td>@stephenschlegel_she is</td>\n",
              "      <td>307</td>\n",
              "      <td>scientist</td>\n",
              "      <td>120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>it is</td>\n",
              "      <td>62</td>\n",
              "      <td>trump</td>\n",
              "      <td>95</td>\n",
              "      <td>think_she is</td>\n",
              "      <td>307</td>\n",
              "      <td>‚</td>\n",
              "      <td>108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>just</td>\n",
              "      <td>54</td>\n",
              "      <td>it is</td>\n",
              "      <td>91</td>\n",
              "      <td>die_husband</td>\n",
              "      <td>307</td>\n",
              "      <td>#climate_change</td>\n",
              "      <td>107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>science</td>\n",
              "      <td>53</td>\n",
              "      <td>just</td>\n",
              "      <td>76</td>\n",
              "      <td>https://t.co/sjofonã¢â‚¬â¦</td>\n",
              "      <td>307</td>\n",
              "      <td>donald_trump</td>\n",
              "      <td>92</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>trump</td>\n",
              "      <td>52</td>\n",
              "      <td>like</td>\n",
              "      <td>74</td>\n",
              "      <td>trump</td>\n",
              "      <td>299</td>\n",
              "      <td>scott_pruitt</td>\n",
              "      <td>89</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>liberal</td>\n",
              "      <td>49</td>\n",
              "      <td>people</td>\n",
              "      <td>55</td>\n",
              "      <td>warming</td>\n",
              "      <td>268</td>\n",
              "      <td>new</td>\n",
              "      <td>82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>warm</td>\n",
              "      <td>44</td>\n",
              "      <td>make</td>\n",
              "      <td>49</td>\n",
              "      <td>denier</td>\n",
              "      <td>246</td>\n",
              "      <td>fight_climate</td>\n",
              "      <td>79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>scientist</td>\n",
              "      <td>43</td>\n",
              "      <td>think</td>\n",
              "      <td>48</td>\n",
              "      <td>real</td>\n",
              "      <td>234</td>\n",
              "      <td>report</td>\n",
              "      <td>78</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>scam</td>\n",
              "      <td>43</td>\n",
              "      <td>real</td>\n",
              "      <td>48</td>\n",
              "      <td>’</td>\n",
              "      <td>213</td>\n",
              "      <td>warming</td>\n",
              "      <td>76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>hoax</td>\n",
              "      <td>38</td>\n",
              "      <td>cause_global</td>\n",
              "      <td>48</td>\n",
              "      <td>believe_climate</td>\n",
              "      <td>198</td>\n",
              "      <td>global_warm</td>\n",
              "      <td>68</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          negative count         neutral  ... count             news count\n",
              "0   climate_change   551  climate_change  ...  4339   climate_change  1845\n",
              "1   global_warming   360  global_warming  ...  1895           change   811\n",
              "2           change   144          change  ...   967   global_warming   216\n",
              "3          warming    90         warming  ...   747                ’   184\n",
              "4      global_warm    73     global_warm  ...   518            trump   184\n",
              "5                ‚    68               ‚  ...   307        scientist   120\n",
              "6            it is    62           trump  ...   307                ‚   108\n",
              "7             just    54           it is  ...   307  #climate_change   107\n",
              "8          science    53            just  ...   307     donald_trump    92\n",
              "9            trump    52            like  ...   299     scott_pruitt    89\n",
              "10         liberal    49          people  ...   268              new    82\n",
              "11            warm    44            make  ...   246    fight_climate    79\n",
              "12       scientist    43           think  ...   234           report    78\n",
              "13            scam    43            real  ...   213          warming    76\n",
              "14            hoax    38    cause_global  ...   198      global_warm    68\n",
              "\n",
              "[15 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxfhMnbgdmvO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#alot less mentions of climage change in negative tweets\n",
        "#RT,'...', random charactars, "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cjGFtM6fSqY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "outputId": "94a6e943-e5ff-49f9-f419-48597aee6d1a"
      },
      "source": [
        "#Top 15 bigrams by category\n",
        "\n",
        "neg_bigrams = bigram_freq(text_neg, top_n = top_n)\n",
        "neut_bigrams = bigram_freq(text_neut, top_n = top_n)\n",
        "pos_bigrams = bigram_freq(text_pos, top_n = top_n)\n",
        "news_bigrams = bigram_freq(text_news, top_n = top_n)\n",
        "\n",
        "df_bigram_wf = pd.concat([neg_bigrams, neut_bigrams, pos_bigrams, news_bigrams], axis=1)\n",
        "df_bigram_wf.columns = cols\n",
        "df_bigram_wf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>negative</th>\n",
              "      <th>count</th>\n",
              "      <th>neutral</th>\n",
              "      <th>count</th>\n",
              "      <th>positive</th>\n",
              "      <th>count</th>\n",
              "      <th>news</th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>climate_change</td>\n",
              "      <td>551</td>\n",
              "      <td>climate_change</td>\n",
              "      <td>985</td>\n",
              "      <td>climate_change</td>\n",
              "      <td>4339</td>\n",
              "      <td>climate_change</td>\n",
              "      <td>1845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>global_warming</td>\n",
              "      <td>360</td>\n",
              "      <td>global_warming</td>\n",
              "      <td>515</td>\n",
              "      <td>global_warming</td>\n",
              "      <td>747</td>\n",
              "      <td>global_warming</td>\n",
              "      <td>216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>global_warm</td>\n",
              "      <td>73</td>\n",
              "      <td>global_warm</td>\n",
              "      <td>137</td>\n",
              "      <td>does not_believe</td>\n",
              "      <td>518</td>\n",
              "      <td>#climate_change</td>\n",
              "      <td>107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>man_make</td>\n",
              "      <td>37</td>\n",
              "      <td>cause_global</td>\n",
              "      <td>48</td>\n",
              "      <td>@stephenschlegel_she is</td>\n",
              "      <td>307</td>\n",
              "      <td>donald_trump</td>\n",
              "      <td>92</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>al_gore</td>\n",
              "      <td>33</td>\n",
              "      <td>�_�</td>\n",
              "      <td>47</td>\n",
              "      <td>think_she is</td>\n",
              "      <td>307</td>\n",
              "      <td>scott_pruitt</td>\n",
              "      <td>89</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>�_�</td>\n",
              "      <td>26</td>\n",
              "      <td>think_global</td>\n",
              "      <td>46</td>\n",
              "      <td>die_husband</td>\n",
              "      <td>307</td>\n",
              "      <td>fight_climate</td>\n",
              "      <td>79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>@realdonaldtrump_concept</td>\n",
              "      <td>22</td>\n",
              "      <td>club_penguin</td>\n",
              "      <td>38</td>\n",
              "      <td>believe_climate</td>\n",
              "      <td>198</td>\n",
              "      <td>global_warm</td>\n",
              "      <td>68</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>create_chinese</td>\n",
              "      <td>22</td>\n",
              "      <td>is not_real</td>\n",
              "      <td>28</td>\n",
              "      <td>fight_climate</td>\n",
              "      <td>184</td>\n",
              "      <td>epa_chief</td>\n",
              "      <td>61</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>order_make</td>\n",
              "      <td>22</td>\n",
              "      <td>believe_climate</td>\n",
              "      <td>27</td>\n",
              "      <td>global_warm</td>\n",
              "      <td>168</td>\n",
              "      <td>trump_climate</td>\n",
              "      <td>61</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>manufacture_non-competitive</td>\n",
              "      <td>22</td>\n",
              "      <td>‚_„</td>\n",
              "      <td>26</td>\n",
              "      <td>@sensanders_president-elect</td>\n",
              "      <td>166</td>\n",
              "      <td>rex_tillerson</td>\n",
              "      <td>53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>man-made_global</td>\n",
              "      <td>20</td>\n",
              "      <td>talk_climate</td>\n",
              "      <td>25</td>\n",
              "      <td>donald_trump</td>\n",
              "      <td>154</td>\n",
              "      <td>white_house</td>\n",
              "      <td>46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>@realdonaldtrump_it is</td>\n",
              "      <td>16</td>\n",
              "      <td>care_global</td>\n",
              "      <td>23</td>\n",
              "      <td>million_people</td>\n",
              "      <td>137</td>\n",
              "      <td>carbon_dioxide</td>\n",
              "      <td>43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>hell_global</td>\n",
              "      <td>16</td>\n",
              "      <td>@ultravlolence_interviewer</td>\n",
              "      <td>20</td>\n",
              "      <td>#climate_change</td>\n",
              "      <td>133</td>\n",
              "      <td>‚_„</td>\n",
              "      <td>42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>°_å</td>\n",
              "      <td>16</td>\n",
              "      <td>warming_melania</td>\n",
              "      <td>20</td>\n",
              "      <td>mr_tã</td>\n",
              "      <td>130</td>\n",
              "      <td>tackle_climate</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>freeze_outside</td>\n",
              "      <td>15</td>\n",
              "      <td>trump_hello</td>\n",
              "      <td>20</td>\n",
              "      <td>‚_„</td>\n",
              "      <td>128</td>\n",
              "      <td>primary_contributor</td>\n",
              "      <td>39</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       negative count  ...                 news count\n",
              "0                climate_change   551  ...       climate_change  1845\n",
              "1                global_warming   360  ...       global_warming   216\n",
              "2                   global_warm    73  ...      #climate_change   107\n",
              "3                      man_make    37  ...         donald_trump    92\n",
              "4                       al_gore    33  ...         scott_pruitt    89\n",
              "5                           �_�    26  ...        fight_climate    79\n",
              "6      @realdonaldtrump_concept    22  ...          global_warm    68\n",
              "7                create_chinese    22  ...            epa_chief    61\n",
              "8                    order_make    22  ...        trump_climate    61\n",
              "9   manufacture_non-competitive    22  ...        rex_tillerson    53\n",
              "10              man-made_global    20  ...          white_house    46\n",
              "11       @realdonaldtrump_it is    16  ...       carbon_dioxide    43\n",
              "12                  hell_global    16  ...                  ‚_„    42\n",
              "13                          °_å    16  ...       tackle_climate    40\n",
              "14               freeze_outside    15  ...  primary_contributor    39\n",
              "\n",
              "[15 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyk0V_NafnIi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#rt donald trump, retweet @stevesgoddard and man make have a strong correlation to negative"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMoGeheegawg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e6e64d80-2c36-4e7b-cee4-ebc371cbdd9c"
      },
      "source": [
        "#Creating the features (tf-idf weights) for the processed text\n",
        "\n",
        "texts = df['clean_text'].astype('str')\n",
        "\n",
        "#tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), \n",
        "#                                   min_df = 2, \n",
        "#                                   max_df = .95)\n",
        "\n",
        "#X = tfidf_vectorizer.fit_transform(texts) #features\n",
        "X = texts\n",
        "y = df['label_num'].values #target\n",
        "\n",
        "print (X.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(15799,)\n",
            "(15799,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yq4LAJ6J2Qsu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import MultinomialNB"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWvRmKcb2hpC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1,random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KH_cGYYc3c5K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EtlWyhd2YH6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LogReg = LogisticRegression()\n",
        "LinSVC = LinearSVC()\n",
        "NB = MultinomialNB()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wERTEhLj2b48",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_clf = Pipeline([('tfidf',TfidfVectorizer(\n",
        "                             min_df=3, \n",
        "                             max_df=0.5, \n",
        "                             ngram_range=(1, 3))),('clf',LinSVC)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqhk0Fzb2iyy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "27190d74-fd07-4ea4-88b0-adfcc59f174a"
      },
      "source": [
        "text_clf.fit(X_train,y_train)\n",
        "predictions = text_clf.predict(X_test)\n",
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "print(confusion_matrix(y_test,predictions))\n",
        "print(classification_report(y_test,predictions))\n",
        "from sklearn import metrics\n",
        "metrics.f1_score(y_test,predictions,average='micro')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 69  21  48   6]\n",
            " [  9  90  98  24]\n",
            " [ 14  54 705  83]\n",
            " [  1  13  86 259]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.48      0.58       144\n",
            "           1       0.51      0.41      0.45       221\n",
            "           2       0.75      0.82      0.79       856\n",
            "           3       0.70      0.72      0.71       359\n",
            "\n",
            "    accuracy                           0.71      1580\n",
            "   macro avg       0.67      0.61      0.63      1580\n",
            "weighted avg       0.70      0.71      0.70      1580\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.710759493670886"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cp5KCFwfgftN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0f75243d-1b3e-45a8-a1fe-698a6fc2f7f3"
      },
      "source": [
        "#Dimenionality reduction. Only using the 100 best features er category\n",
        "\n",
        "lsa = TruncatedSVD(n_components=100, \n",
        "                   n_iter=10, \n",
        "                   random_state=3)\n",
        "\n",
        "X = lsa.fit_transform(X)\n",
        "X.shape\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15799, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4ajCcmAglFN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "1975a20a-f25a-4f49-e32f-fc8fd0457a10"
      },
      "source": [
        "#Preliminary model evaluation using default parameters\n",
        "\n",
        "#Creating a dict of the models\n",
        "model_dict = {'Dummy' : DummyClassifier(random_state=3),\n",
        "              'Stochastic Gradient Descent' : SGDClassifier(random_state=3, loss='log'),\n",
        "              'Random Forest': RandomForestClassifier(random_state=3),\n",
        "              'Decsision Tree': DecisionTreeClassifier(random_state=3),\n",
        "              'AdaBoost': AdaBoostClassifier(random_state=3),\n",
        "              'Gaussian Naive Bayes': GaussianNB(),\n",
        "              'K Nearest Neighbor': KNeighborsClassifier()}\n",
        "\n",
        "#Train test split with stratified sampling for evaluation\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, \n",
        "                                                    y, \n",
        "                                                    test_size = .1, \n",
        "                                                    shuffle = True, \n",
        "                                                    stratify = y, \n",
        "                                                    random_state = 3)\n",
        "\n",
        "#Function to get the scores for each model in a df\n",
        "def model_score_df(model_dict):   \n",
        "    model_name, ac_score_list, p_score_list, r_score_list, f1_score_list = [], [], [], [], []\n",
        "    for k,v in model_dict.items():   \n",
        "        model_name.append(k)\n",
        "        v.fit(X_train, y_train)\n",
        "        y_pred = v.predict(X_test)\n",
        "        ac_score_list.append(accuracy_score(y_test, y_pred))\n",
        "        p_score_list.append(precision_score(y_test, y_pred, average='micro'))\n",
        "        r_score_list.append(recall_score(y_test, y_pred, average='micro'))\n",
        "        f1_score_list.append(f1_score(y_test, y_pred, average='micro'))\n",
        "        model_comparison_df = pd.DataFrame([model_name, ac_score_list, p_score_list, r_score_list, f1_score_list]).T\n",
        "        model_comparison_df.columns = ['model_name', 'accuracy_score', 'precision_score', 'recall_score', 'f1_score']\n",
        "        model_comparison_df = model_comparison_df.sort_values(by='f1_score', ascending=False)\n",
        "    return model_comparison_df\n",
        "\n",
        "model_score_df(model_dict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model_name</th>\n",
              "      <th>accuracy_score</th>\n",
              "      <th>precision_score</th>\n",
              "      <th>recall_score</th>\n",
              "      <th>f1_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Stochastic Gradient Descent</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>K Nearest Neighbor</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>AdaBoost</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Decsision Tree</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Gaussian Naive Bayes</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Dummy</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.36</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    model_name accuracy_score  ... recall_score f1_score\n",
              "2                Random Forest           0.70  ...         0.70     0.70\n",
              "1  Stochastic Gradient Descent           0.65  ...         0.65     0.65\n",
              "6           K Nearest Neighbor           0.64  ...         0.64     0.64\n",
              "4                     AdaBoost           0.64  ...         0.64     0.64\n",
              "3               Decsision Tree           0.58  ...         0.58     0.58\n",
              "5         Gaussian Naive Bayes           0.44  ...         0.44     0.44\n",
              "0                        Dummy           0.36  ...         0.36     0.36\n",
              "\n",
              "[7 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7x_uHuuo7_m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}