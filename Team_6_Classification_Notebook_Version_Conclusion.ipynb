{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Team-6-Classification-Notebook-Version-2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "3uTm6KTx-jFp",
        "aH1r8V74-jFw",
        "QbMM97Xw-jHY",
        "ofKo21OH-jHp",
        "91frBBH6-jHu",
        "Ut_RRUtC-jH9",
        "yr_-oNNO-jIH",
        "IF7xPFUR-jIu",
        "Lgxfg_a1-jJV",
        "wORNfc2y-jJZ",
        "NiYwBmEp-jJZ",
        "P4Qjx8Wk-jJx",
        "KZ5EtgBmSHjV",
        "Z-3d9YiDVSL6",
        "peS01w3zAaEo"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GDljTQyG21zY"
      },
      "source": [
        "# Team 6 CT Classification\n",
        "\n",
        "## Climate Change Sentiment analysis - 2020\n",
        "\n",
        "### Project Description\n",
        "\n",
        "\n",
        ">Many companies are built around lessening one’s environmental impact or carbon footprint. They offer products and services that are environmentally friendly and sustainable, in line with their values and ideals. They would like to determine how people perceive climate change and whether or not they believe it is a real threat. This would add to their market research efforts in gauging how their product/service may be received.\n",
        "\n",
        ">With this context, EDSA is challenging you during the Classification Sprint with the task of creating a Machine Learning model that is able to classify whether or not a person believes in climate change, based on their novel tweet data.\n",
        "\n",
        ">Providing an accurate and robust solution to this task gives companies access to a broad base of consumer sentiment, spanning multiple demographic and geographic categories - thus increasing their insights and informing future marketing strategies.\n",
        "\n",
        "https://www.kaggle.com/c/climate-change-belief-analysis/overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ToQbUN7-jEr",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://images.unsplash.com/photo-1578825141469-690ba22eede0?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1051&q=80)\n",
        "<span>Photo by <a href=\"https://unsplash.com/@markusspiske?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Markus Spiske</a> on <a href=\"/s/photos/business-dead-planet?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Unsplash</a></span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UP8zAruiCcnc"
      },
      "source": [
        "### Notebook Explanation\n",
        "\n",
        "First, we tried 3 base models with no preprocessing of the text to get a benchmark score. We then extracted info from the tweets and added additional columns such as character and words count.\n",
        "\n",
        "The next step was cleaning the tweets. For example, we removed the URLs, the <i>'RT'</i> retweet tags and the mentions. We applied both stemming and lemmatisation normalising methods to test which performs better. Following that, we vectorized the text using 3 methods: Count Vectoriser, TD-IDF and Word2Vec. We added additional columns for each of the combinations of methods and ran each combination through the base models again and compared the performance. This was an iterative process until we got the best performing cleaning method.\n",
        "\n",
        "After deciding on the best cleaning method, we did an Exploratory data analysis (EDA) on the raw tweets to retrieve data about mentions and hashtags. We also did EDA on the categorised processed tweets to extract insights from most the different sentiment labels.\n",
        "\n",
        "Next, we compared base models with processed tweets. We selected the top 3 performing models and used GridSearchCV to find the best parameters to train these models with. We compared the F1 score and selected the best performing model. Below is a model process flow diagram visualising the approach we took to solve this problem.\n",
        "\n",
        "<img src=\"resources/Model Process Flow.png\" />\n",
        "\n",
        "<a id = \"top\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RperqSUu-vB9"
      },
      "source": [
        "# Table of contents\n",
        "\n",
        "1. [Importing packages](#packages) <br><br>\n",
        "\n",
        "2. [Loading and viewing data](#data) <br><br>\n",
        "\n",
        "3. [Data description](#description) <br><br>\n",
        "\n",
        "4. [Sentiment description](#sentiment)<br><br>\n",
        "\n",
        "5. [Data extraction](#extraction)<br><br>\n",
        "\n",
        "6. [Text cleaning](#cleaning) <br><br>\n",
        "\n",
        "7. [Exploratory data analysis](#eda)<br><br>\n",
        "\n",
        "8. [Balancing dataset](#balance) <br><br>\n",
        "\n",
        "9. [Base model](#base) <br><br>\n",
        "\n",
        "10. [Model evaluation & optimisation](#evaluation) <br><br>\n",
        "\n",
        "11. [Conclusion](#conclusion) <br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNP6czEV-jEu",
        "colab_type": "text"
      },
      "source": [
        "# 1. Importing packages <a name=\"packages\"></a>\n",
        "[Return to top](#top) <br><br>\n",
        "\n",
        "The following packages need to be installed.\n",
        "\n",
        "- Spacy - pip install spacy==2.2.4\n",
        "- NTLK - pip indstal nltk==3.5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "4pLys7mY-jEw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "!pip install ipython-autotime\n",
        "\n",
        "%load_ext autotime"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "TFrFUIO--jE3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "9c4b9f39-22cf-4416-ec50-017ee9d10328"
      },
      "source": [
        "# Processing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "\n",
        "# Visualisation\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Spacy packages\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "# NLTK packages\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize,TweetTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "# Sklearn packagesw\n",
        "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.utils import resample\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn import preprocessing\n",
        "from sklearn_pandas import DataFrameMapper\n",
        "\n",
        "# Utils\n",
        "from collections import Counter\n",
        "import itertools, string, operator, re, unicodedata, nltk\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Classifiers\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "# Evaluation\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.feature_extraction.stop_words module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_extraction.text. Anything that cannot be imported from sklearn.feature_extraction.text is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "time: 2.98 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBfY5_XS-jFC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "outputId": "c61efbad-9e7a-451f-cde0-06c7ecae79d2"
      },
      "source": [
        "# Downloads\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        },
        {
          "output_type": "stream",
          "text": [
            "time: 112 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRjZRZP9-jFH",
        "colab_type": "text"
      },
      "source": [
        "# 2. Loading  and viewing data <a name=\"data\"></a>\n",
        "[Return to top](#top) <br><br>\n",
        "\n",
        "\n",
        "- Data loads from Github repository\n",
        "- Training dataset has 15819 entries\n",
        "- Testing dataset has 10546 entries\n",
        "- Viewing the first 5 rows of datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "r-Fkx2_m-jFI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "ef87ffdc-41bd-4708-c46d-6edf730bae29"
      },
      "source": [
        "# Import data\n",
        "train_df = pd.read_csv('https://raw.githubusercontent.com/jonnybegreat/test-repo/master/twitter_train.csv')\n",
        "test_df = pd.read_csv('https://raw.githubusercontent.com/jonnybegreat/test-repo/master/twitter_test.csv')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 906 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TxFbP5UZCpAy",
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "785632c0-df08-4c32-ad34-d1f91be7a8da"
      },
      "source": [
        "# Make copy of train_df assigning to variable df and view the first 5 rows\n",
        "df = train_df.copy()\n",
        "df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>message</th>\n",
              "      <th>tweetid</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>PolySciMajor EPA chief doesn't think carbon di...</td>\n",
              "      <td>625221</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>It's not like we lack evidence of anthropogeni...</td>\n",
              "      <td>126103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>RT @RawStory: Researchers say we have three ye...</td>\n",
              "      <td>698562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>#TodayinMaker# WIRED : 2016 was a pivotal year...</td>\n",
              "      <td>573736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>RT @SoyNovioDeTodas: It's 2016, and a racist, ...</td>\n",
              "      <td>466954</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentiment                                            message  tweetid\n",
              "0          1  PolySciMajor EPA chief doesn't think carbon di...   625221\n",
              "1          1  It's not like we lack evidence of anthropogeni...   126103\n",
              "2          2  RT @RawStory: Researchers say we have three ye...   698562\n",
              "3          1  #TodayinMaker# WIRED : 2016 was a pivotal year...   573736\n",
              "4          1  RT @SoyNovioDeTodas: It's 2016, and a racist, ...   466954"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        },
        {
          "output_type": "stream",
          "text": [
            "time: 26.6 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "wWDK5p_g-jFU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "9ae209d7-f440-471a-b7f1-70a311a7cad6"
      },
      "source": [
        "# View the first 5 rows of test dataset\n",
        "test_df.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>message</th>\n",
              "      <th>tweetid</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Europe will now be looking to China to make su...</td>\n",
              "      <td>169760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Combine this with the polling of staffers re c...</td>\n",
              "      <td>35326</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The scary, unimpeachable evidence that climate...</td>\n",
              "      <td>224985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>@Karoli @morgfair @OsborneInk @dailykos \\nPuti...</td>\n",
              "      <td>476263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>RT @FakeWillMoore: 'Female orgasms cause globa...</td>\n",
              "      <td>872928</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             message  tweetid\n",
              "0  Europe will now be looking to China to make su...   169760\n",
              "1  Combine this with the polling of staffers re c...    35326\n",
              "2  The scary, unimpeachable evidence that climate...   224985\n",
              "3  @Karoli @morgfair @OsborneInk @dailykos \\nPuti...   476263\n",
              "4  RT @FakeWillMoore: 'Female orgasms cause globa...   872928"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "stream",
          "text": [
            "time: 10.6 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTJe69F7-jFd",
        "colab_type": "text"
      },
      "source": [
        "# 3. Data description <a name=\"description\"></a>\n",
        "[Return to top](#top) <br><br>\n",
        "\n",
        "<b>Training data columns (15819 entries):</b>\n",
        "- Sentiment - Labelled sentiment classification\n",
        "- Message - Tweet to analysed\n",
        "- Tweet ID - ID of unique tweet\n",
        "\n",
        "<b>Test data columns (10546 entries):</b>\n",
        "- Message - Tweet to be analysed\n",
        "- Tweet ID - ID of unique tweet\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtU0TOe4-jFe",
        "colab_type": "text"
      },
      "source": [
        "<i>Training data info and data types:</i>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "gvJG7zox-jFf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "outputId": "63d34a7b-baff-44ed-9aee-c4a19fd888f2"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 15819 entries, 0 to 15818\n",
            "Data columns (total 3 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   sentiment  15819 non-null  int64 \n",
            " 1   message    15819 non-null  object\n",
            " 2   tweetid    15819 non-null  int64 \n",
            "dtypes: int64(2), object(1)\n",
            "memory usage: 370.9+ KB\n",
            "time: 16.2 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCfL0ZHg-jFj",
        "colab_type": "text"
      },
      "source": [
        "<i>Testing data info and data types:</i>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "hoTT43ZF-jFk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "outputId": "2c9ef651-0e41-46c4-c450-da1b58b872b3"
      },
      "source": [
        "test_df.info()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10546 entries, 0 to 10545\n",
            "Data columns (total 2 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   message  10546 non-null  object\n",
            " 1   tweetid  10546 non-null  int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 164.9+ KB\n",
            "time: 9.53 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uTm6KTx-jFp",
        "colab_type": "text"
      },
      "source": [
        "# 4. Sentiment description <a name=\"sentiment\"></a>\n",
        "[Return to top](#top) <br><br>\n",
        "\n",
        "The table displays the description of each sentiment category:\n",
        "\n",
        "![alt text](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F2205222%2F8e4d65f2029797e0462b52022451829c%2Fdata.PNG?generation=1590752860255531&alt=media)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jhasVLF-jFp",
        "colab_type": "text"
      },
      "source": [
        "<i>How many tweets are there in each category?</i>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "PnJ899yT-jFq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "43f46924-292e-4339-f54a-4b9586483496"
      },
      "source": [
        "#Bar graph of sentiment count\n",
        "sns.catplot(x = 'sentiment', kind = 'count', edgecolor = '.6',\n",
        "            palette = 'pastel', data = df);"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "time: 260 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chWWSIwe-jFv",
        "colab_type": "text"
      },
      "source": [
        "Majority of the tweets are positive(1) towards climate change. The least amount of tweets are negative (-1) towards climate change. This shows data is unbalanced and can affect our prediction results. Later in this notebook, we will explore resampling methods to balance the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aH1r8V74-jFw",
        "colab_type": "text"
      },
      "source": [
        "# 5. Data extraction <a name=\"extraction\"></a>\n",
        "[Return to top](#top) <br><br>\n",
        "\n",
        "\n",
        "Create new columns of the following features of the tweets:\n",
        "- Tokenise tweet\n",
        "- Categorise as retweet or not\n",
        "- Hashtag extraction and count\n",
        "- Mention extraction and count\n",
        "- Word and character count\n",
        "- Average word length\n",
        "- Stop word count per\n",
        "\n",
        "All of the above methods are applied to the test data as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "wEwoD_An-jFy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "c6d62eb5-1f11-4884-f43d-099aca386fd1"
      },
      "source": [
        "# Create copies of train and test dataframes\n",
        "df_with_metadata = df.copy()\n",
        "test_df_with_metadata = test_df.copy()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 3.61 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "at-icNkv-jF_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "7281c5a8-35e6-403a-ea64-04ee5354b8f9"
      },
      "source": [
        "def hashtag_column(x):\n",
        "    '''\n",
        "    This function extracts hashtags from tweets and\n",
        "    adds them to a new hashtags column.\n",
        "    '''\n",
        "    hashtags = []\n",
        "    new_tag_list = []\n",
        "        \n",
        "    #Find all the items that start with a '#'\n",
        "    for i in x:\n",
        "        ht = re.findall(r\"#(\\w+)\", i)\n",
        "        hashtags.append(ht)         \n",
        "    \n",
        "    #Replace empty tag lists with NaN\n",
        "    for tag in hashtags:\n",
        "        if tag == []:\n",
        "            tag = np.nan\n",
        "        new_tag_list.append(tag)\n",
        "        \n",
        "    return new_tag_list"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 7.97 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "GYO8eYuF-jGE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "b1611916-c8b6-4cd8-f1c1-1b535874f6ca"
      },
      "source": [
        "# Mentions function - NEEDS TO BE EDITED TO INCLUDE MULTIPLE MENTIONS\n",
        "def mention_column(x):\n",
        "    '''\n",
        "    This function extracts mentions from tweets and\n",
        "    add it to a new hashtags column.\n",
        "    '''\n",
        "    mentions = []\n",
        "    new_mention_list = []\n",
        "\n",
        "    #Find all the items that start with a '@'\n",
        "    for i in x:\n",
        "        ht = re.findall(r\"@(\\w+)\", i)\n",
        "        mentions.append(ht)         \n",
        "\n",
        "    #Replace empty mention lists with NaN        \n",
        "    for tag in mentions:\n",
        "        if tag == []:\n",
        "            tag = np.nan\n",
        "        new_mention_list.append(tag)\n",
        "        \n",
        "    return new_mention_list"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 8.72 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "05l6__6z-jGK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "9ebfcf2b-2a09-4e97-8b66-55bf96a94ff2"
      },
      "source": [
        "# Tokenized tweet - apply word_tokenize from NLTK\n",
        "df_with_metadata['message_token'] = df_with_metadata['message'].apply(lambda x: word_tokenize(x))\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 3.55 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "l7dmPPTS-jGP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "8058e8ec-2ed0-4059-de81-514e2cec0e69"
      },
      "source": [
        "# Categorise as retweet or not\n",
        "\n",
        "#New 'retweet' column to return yes if the first 2 characters is 'RT', else return no\n",
        "df_with_metadata['retweet'] = ['yes' if df_with_metadata['message'][i][:2] == 'RT' \n",
        "                               else 'no' for i in range(len(df_with_metadata))]\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 234 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "JMmjAFt--jGS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "cd863743-eff2-4181-e4f6-3e15093561d5"
      },
      "source": [
        "# Create new column 'hashtags' and extract hashtags with hashtag_column function.\n",
        "df_with_metadata['hashtags'] = hashtag_column(df_with_metadata['message'])\n",
        "\n",
        "test_df_with_metadata['hashtags'] = hashtag_column(test_df_with_metadata['message'])\n",
        "\n",
        "#Count how many times a word starts with #\n",
        "df_with_metadata['hashtag_count'] = df_with_metadata['message'].apply(\n",
        "    lambda tweet: len([word for word in tweet.split() if word.startswith('#')]))\n",
        "\n",
        "test_df_with_metadata['hashtag_count'] = test_df_with_metadata['message'].apply(\n",
        "    lambda tweet: len([word for word in tweet.split() if word.startswith('#')]))\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 145 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "3bBicWH0-jGX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "2ee8c917-3d2f-40d1-f652-23d771046068"
      },
      "source": [
        "# Create new column 'mentions' and extract mentions with mention_column function.\n",
        "df_with_metadata['mentions'] = mention_column(df_with_metadata['message'])\n",
        "\n",
        "test_df_with_metadata['mentions'] = mention_column(test_df_with_metadata['message'])\n",
        "\n",
        "#Count how many times a word starts with @\n",
        "df_with_metadata['mention_count'] = df_with_metadata['message'].apply(\n",
        "    lambda tweet: len([word for word in tweet.split() if word.startswith('@')]))\n",
        "\n",
        "test_df_with_metadata['mention_count'] = test_df_with_metadata['message'].apply(\n",
        "    lambda tweet: len([word for word in tweet.split() if word.startswith('@')]))\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 156 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "V5xwmA6--jGa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "2ff87688-4fea-4384-a1a1-929866dfa07b"
      },
      "source": [
        "# Character count\n",
        "df_with_metadata['char_count'] = df_with_metadata['message'].str.len()\n",
        "test_df_with_metadata['char_count'] = test_df_with_metadata['message'].str.len()\n",
        "\n",
        "# Word count\n",
        "df_with_metadata['word_count'] = df_with_metadata['message'].str.split().str.len()\n",
        "test_df_with_metadata['word_count'] = test_df_with_metadata['message'].str.split().str.len()\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 199 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "z8AeZ3te-jGd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "cb0c31f9-f02c-493f-a839-c9ab13ed3652"
      },
      "source": [
        "# Average word length\n",
        "df_with_metadata['avg_word_length'] = df_with_metadata['message'].apply(\n",
        "    lambda tweet: round(sum([len(word) for word in tweet.split()]) / len(tweet.split()),2))\n",
        "\n",
        "test_df_with_metadata['avg_word_length'] = test_df_with_metadata['message'].apply(\n",
        "    lambda tweet: round(sum([len(word) for word in tweet.split()]) / len(tweet.split()),2))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 138 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "hhonJt5W-jGg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "0823c148-eecd-438d-fccb-bba759ea7702"
      },
      "source": [
        "# Stop word count\n",
        "df_with_metadata['stopword_count'] = df_with_metadata['message'].apply(\n",
        "    lambda tweet: len([word for word in tweet.split() if word in STOP_WORDS]))\n",
        "\n",
        "test_df_with_metadata['stopword_count'] = test_df_with_metadata['message'].apply(\n",
        "    lambda tweet: len([word for word in tweet.split() if word in STOP_WORDS]))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 108 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t489YDG--jGj",
        "colab_type": "text"
      },
      "source": [
        "# 6. Text cleaning <a name=\"cleaning\"></a>\n",
        "[Return to top](#top) <br><br>\n",
        "\n",
        "\n",
        "The following text cleaning processes were applied to the tweets:\n",
        "- Upon inspection, there is a common recurrence of the following special character combination: 'Ã¢â‚¬Â¦'\n",
        "- Tokenise text\n",
        "- Make the text lowercase\n",
        "- Expand contracted words\n",
        "- Part of speech tagging\n",
        "- Lemmatising text - replace with base words\n",
        "- Remove numbers\n",
        "- Remove punctuation\n",
        "- Remove stop words \n",
        "\n",
        "Stop words are commonly used words such as 'the', 'a' and 'in'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "Jm97gTI1-jGj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "a8a86239-900f-43f4-c258-a3caa0482470"
      },
      "source": [
        "#Contraction dictionary\n",
        "c_dict = {\n",
        "  \"ain't\": \"am not\",\n",
        "  \"aren't\": \"are not\",\n",
        "  \"can't\": \"cannot\",\n",
        "  \"can't've\": \"cannot have\",\n",
        "  \"'cause\": \"because\",\n",
        "  \"could've\": \"could have\",\n",
        "  \"couldn't\": \"could not\",\n",
        "  \"couldn't've\": \"could not have\",\n",
        "  \"didn't\": \"did not\",\n",
        "  \"doesn't\": \"does not\",\n",
        "  \"don't\": \"do not\",\n",
        "  \"hadn't\": \"had not\",\n",
        "  \"hadn't've\": \"had not have\",\n",
        "  \"hasn't\": \"has not\",\n",
        "  \"haven't\": \"have not\",\n",
        "  \"he'd\": \"he would\",\n",
        "  \"he'd've\": \"he would have\",\n",
        "  \"he'll\": \"he will\",\n",
        "  \"he'll've\": \"he will have\",\n",
        "  \"he's\": \"he is\",\n",
        "  \"how'd\": \"how did\",\n",
        "  \"how'd'y\": \"how do you\",\n",
        "  \"how'll\": \"how will\",\n",
        "  \"how's\": \"how is\",\n",
        "  \"i'd\": \"I would\",\n",
        "  \"i'd've\": \"I would have\",\n",
        "  \"i'll\": \"I will\",\n",
        "  \"i'll've\": \"I will have\",\n",
        "  \"i'm\": \"I am\",\n",
        "  \"i've\": \"I have\",\n",
        "  \"isn't\": \"is not\",\n",
        "  \"it'd\": \"it had\",\n",
        "  \"it'd've\": \"it would have\",\n",
        "  \"it'll\": \"it will\",\n",
        "  \"it'll've\": \"it will have\",\n",
        "  \"it's\": \"it is\",\n",
        "  \"let's\": \"let us\",\n",
        "  \"ma'am\": \"madam\",\n",
        "  \"mayn't\": \"may not\",\n",
        "  \"might've\": \"might have\",\n",
        "  \"mightn't\": \"might not\",\n",
        "  \"mightn't've\": \"might not have\",\n",
        "  \"must've\": \"must have\",\n",
        "  \"mustn't\": \"must not\",\n",
        "  \"mustn't've\": \"must not have\",\n",
        "  \"needn't\": \"need not\",\n",
        "  \"needn't've\": \"need not have\",\n",
        "  \"o'clock\": \"of the clock\",\n",
        "  \"oughtn't\": \"ought not\",\n",
        "  \"oughtn't've\": \"ought not have\",\n",
        "  \"shan't\": \"shall not\",\n",
        "  \"sha'n't\": \"shall not\",\n",
        "  \"shan't've\": \"shall not have\",\n",
        "  \"she'd\": \"she would\",\n",
        "  \"she'd've\": \"she would have\",\n",
        "  \"she'll\": \"she will\",\n",
        "  \"she'll've\": \"she will have\",\n",
        "  \"she's\": \"she is\",\n",
        "  \"should've\": \"should have\",\n",
        "  \"shouldn't\": \"should not\",\n",
        "  \"shouldn't've\": \"should not have\",\n",
        "  \"so've\": \"so have\",\n",
        "  \"so's\": \"so is\",\n",
        "  \"that'd\": \"that would\",\n",
        "  \"that'd've\": \"that would have\",\n",
        "  \"that's\": \"that is\",\n",
        "  \"there'd\": \"there had\",\n",
        "  \"there'd've\": \"there would have\",\n",
        "  \"there's\": \"there is\",\n",
        "  \"they'd\": \"they would\",\n",
        "  \"they'd've\": \"they would have\",\n",
        "  \"they'll\": \"they will\",\n",
        "  \"they'll've\": \"they will have\",\n",
        "  \"they're\": \"they are\",\n",
        "  \"they've\": \"they have\",\n",
        "  \"to've\": \"to have\",\n",
        "  \"wasn't\": \"was not\",\n",
        "  \"we'd\": \"we had\",\n",
        "  \"we'd've\": \"we would have\",\n",
        "  \"we'll\": \"we will\",\n",
        "  \"we'll've\": \"we will have\",\n",
        "  \"we're\": \"we are\",\n",
        "  \"we've\": \"we have\",\n",
        "  \"weren't\": \"were not\",\n",
        "  \"what'll\": \"what will\",\n",
        "  \"what'll've\": \"what will have\",\n",
        "  \"what're\": \"what are\",\n",
        "  \"what's\": \"what is\",\n",
        "  \"what've\": \"what have\",\n",
        "  \"when's\": \"when is\",\n",
        "  \"when've\": \"when have\",\n",
        "  \"where'd\": \"where did\",\n",
        "  \"where's\": \"where is\",\n",
        "  \"where've\": \"where have\",\n",
        "  \"who'll\": \"who will\",\n",
        "  \"who'll've\": \"who will have\",\n",
        "  \"who's\": \"who is\",\n",
        "  \"who've\": \"who have\",\n",
        "  \"why's\": \"why is\",\n",
        "  \"why've\": \"why have\",\n",
        "  \"will've\": \"will have\",\n",
        "  \"won't\": \"will not\",\n",
        "  \"won't've\": \"will not have\",\n",
        "  \"would've\": \"would have\",\n",
        "  \"wouldn't\": \"would not\",\n",
        "  \"wouldn't've\": \"would not have\",\n",
        "  \"y'all\": \"you all\",\n",
        "  \"y'alls\": \"you alls\",\n",
        "  \"y'all'd\": \"you all would\",\n",
        "  \"y'all'd've\": \"you all would have\",\n",
        "  \"y'all're\": \"you all are\",\n",
        "  \"y'all've\": \"you all have\",\n",
        "  \"you'd\": \"you had\",\n",
        "  \"you'd've\": \"you would have\",\n",
        "  \"you'll\": \"you you will\",\n",
        "  \"you'll've\": \"you you will have\",\n",
        "  \"you're\": \"you are\",\n",
        "  \"you've\": \"you have\"\n",
        "}\n",
        "c_re = re.compile('(%s)' % '|'.join(c_dict.keys()))\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 109 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "RTvMJIo_-jGm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 995
        },
        "outputId": "a9f07a5d-5bdd-4a9a-884f-466cc1f9a9ec"
      },
      "source": [
        "# Creating library objects\n",
        "tokenizer = TweetTokenizer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "punc = list(set(string.punctuation))\n",
        "\n",
        "# Adding additional stop words\n",
        "additional_stopwords = ['', ' ', 'say', 's', 'u', 'ap', 'afp', '...',\n",
        "                        'n', '\\\\', 'â', '’', '¢', '…', 'it is', \"do not\"]\n",
        "\n",
        "stop_words = ENGLISH_STOP_WORDS.union(additional_stopwords)\n",
        "\n",
        "# Function to expand contracted words\n",
        "def expandContractions(text, c_re = c_re):\n",
        "    '''\n",
        "    The function replaces contracted words with\n",
        "    their expanded form from c_dict.\n",
        "    '''\n",
        "    def replace(match):\n",
        "        return c_dict[match.group(0)]\n",
        "    return c_re.sub(replace, text)\n",
        "\n",
        "# Function to extract parts of speech\n",
        "\n",
        "def get_word_net_pos(treebank_tag):\n",
        "    '''\n",
        "    Function to return treebank tag with description.\n",
        "    '''\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "    \n",
        "# Function to lemmamtise words\n",
        "\n",
        "def lemma_wordnet(tagged_text):\n",
        "    '''\n",
        "    This function returns the lemmatized base of each\n",
        "    word by using wordnet lemmatizer.\n",
        "    ''' \n",
        "    final = []\n",
        "    for word, tag in tagged_text:\n",
        "        wordnet_tag = get_word_net_pos(tag)\n",
        "        if wordnet_tag is None:\n",
        "            final.append(lemmatizer.lemmatize(word))\n",
        "        else:\n",
        "            final.append(lemmatizer.lemmatize(word, pos = wordnet_tag))\n",
        "    return final\n",
        "\n",
        "\n",
        "# Function to process text\n",
        "\n",
        "def process_text(text):\n",
        "    '''\n",
        "    Function to tokenise text, make all the text lowercase, \n",
        "    expanding contracted words, extracting parts of speech tags,\n",
        "    lemmatising text, and removing numbers, punctuation and stop words.\n",
        "    '''\n",
        "    # Tokenize text using tokenizer\n",
        "    tokenized = tokenizer.tokenize(text)\n",
        "    \n",
        "    # Make each word lowercase\n",
        "    lower = [item.lower() for item in tokenized] \n",
        "    \n",
        "    #Expand contracted words\n",
        "    decontract = [expandContractions(item, c_re = c_re) for item in lower]\n",
        "    \n",
        "    # POS tagging\n",
        "    tagged = nltk.pos_tag(decontract) \n",
        "    \n",
        "    # Lemmatize words using lemma_wordnet function\n",
        "    lemma = lemma_wordnet(tagged) \n",
        "    \n",
        "    # Remove numbers from text\n",
        "    no_num = [re.sub('[0-9]+', '', each) for each in lemma] \n",
        "    \n",
        "    # Remove punctuation if not in punc list\n",
        "    no_punc = [w for w in no_num if w not in punc] \n",
        "    \n",
        "    # Remove stop words is words in stop_words list\n",
        "    no_stop = [w for w in no_punc if w not in stop_words] \n",
        "    \n",
        "    return no_stop # Return processed text\n",
        "\n",
        "# Clean the tweets on training and testing data\n",
        "df_with_metadata['cleaned_text'] = df_with_metadata['message'].apply(process_text)\n",
        "test_df_with_metadata['cleaned_text'] = test_df_with_metadata['message'].apply(process_text)\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/lib/nltk_data'\n**********************************************************************\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-e46c6e79bddf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;31m# Clean the tweets on training and testing data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m \u001b[0mdf_with_metadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cleaned_text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_with_metadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'message'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0mtest_df_with_metadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cleaned_text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_df_with_metadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'message'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   3846\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3847\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3848\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3850\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-e46c6e79bddf>\u001b[0m in \u001b[0;36mprocess_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;31m# Lemmatize words using lemma_wordnet function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0mlemma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlemma_wordnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;31m# Remove numbers from text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-e46c6e79bddf>\u001b[0m in \u001b[0;36mlemma_wordnet\u001b[0;34m(tagged_text)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mfinal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtagged_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mwordnet_tag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_word_net_pos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwordnet_tag\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mfinal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-e46c6e79bddf>\u001b[0m in \u001b[0;36mget_word_net_pos\u001b[0;34m(treebank_tag)\u001b[0m\n\u001b[1;32m     27\u001b[0m     '''\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtreebank_tag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'J'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mADJ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mtreebank_tag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'V'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVERB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/lib/nltk_data'\n**********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "time: 197 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeQmnZge-jGr",
        "colab_type": "text"
      },
      "source": [
        "Let's look at the sentiment and cleaned tweet tokens in comparison with the original tweets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "WQqpY7aX-jGs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Display cleaned tweets\n",
        "df_with_metadata[['message','cleaned_text','sentiment']]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e46miwh5-jGw",
        "colab_type": "text"
      },
      "source": [
        "# 7. Exploratory data analysis <a name=\"eda\"></a>\n",
        "[Return to top](#top) <br><br>\n",
        "\n",
        "\n",
        "We will be exploring our data and drawing information from the original tweets, the cleaned tweets and the data we have extracted.\n",
        "\n",
        "Let's take another look at the columns that were created."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "uveD4k0Q-jGw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# View metadata df\n",
        "df_with_metadata.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqwlCmcy-jG2",
        "colab_type": "text"
      },
      "source": [
        "### Retweets\n",
        "Let's start by looking at the retweets. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "fIAKh7uV-jG3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Bar graph of number of rewteets\n",
        "sns.catplot(x = \"retweet\", kind = \"count\", edgecolor = \".6\",palette = \"pastel\",\n",
        "            data = df_with_metadata);\n",
        "\n",
        "valuecounts = df_with_metadata['retweet'].value_counts()\n",
        "\n",
        "# Print percentage of each value\n",
        "print('Yes: ', round(valuecounts[0]/\n",
        "                     len(df_with_metadata['retweet'])*100,2),'%')\n",
        "print('No: ', round(valuecounts[1]/\n",
        "                    len(df_with_metadata['retweet'])*100,2),'%')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjh5faJk-jG6",
        "colab_type": "text"
      },
      "source": [
        "Just over 60% of these tweets are Retweets! There might be some duplicate tweets. We explore by taking a look at the top 10 most retweeted tweets and how many times they were retweeted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "d4v_EQLK-jG7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#View the top 10 retweeted tweets\n",
        "df_rt_counts = pd.DataFrame(df_with_metadata['message']\n",
        "                            .astype(str).value_counts())\n",
        "df_rt_counts.head(10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "994yXQJF-jG-",
        "colab_type": "text"
      },
      "source": [
        "We have alot of the same tweet occurences so we would assume that the sentiment would be the same for each of them. Let's look at the top duplicate tweet to confirm this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "9BLVil9x-jG_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#View sentiment of tweet with the highest retweet count\n",
        "df_most_duplicated_tweet = pd.DataFrame(df_with_metadata[df_with_metadata['message'] == \"RT @StephenSchlegel: she's thinking about how she's going to die because your husband doesn't believe in climate change https://t.co/SjoFoNÃ¢â‚¬Â¦\"])\n",
        "\n",
        "df_most_duplicated_tweet[['message','sentiment']]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYgQLoqs-jHC",
        "colab_type": "text"
      },
      "source": [
        "There are quite a couple of duplicate tweets with the same sentiment. This might be a problem and lead to over importance of certain categories.\n",
        " \n",
        "There is also a recurrence of the following special character combination: 'Ã¢â‚¬Â¦'. Let's Remove duplicates and this special character set before we continue.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "dvlBLgki-jHD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# View shape of metadata_df\n",
        "df_with_metadata.shape\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "bFDTZZYq-jHI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Removing special character combinations\n",
        "df_with_metadata['message'] = [re.sub('Ã¢â‚¬Â¦', '', i)\n",
        "                               for i in df_with_metadata['message']]\n",
        "\n",
        "print(df_with_metadata['message'][10])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "UUqjnIAP-jHL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Drop duplicated retweeted tweets\n",
        "df_with_metadata.drop_duplicates(subset = \"message\", \n",
        "                     keep = False, inplace = True) \n",
        "\n",
        "# View shape of metadata_df with duplicated removed\n",
        "print(df_with_metadata.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "BSO3KJqR-jHN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# View df with metadata again\n",
        "df_with_metadata.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcPLww2w-jHQ",
        "colab_type": "text"
      },
      "source": [
        "### Hashtags and Mentions\n",
        "\n",
        "We can tell a lot from the sentiment of tweets by looking at the hashtags which are used. Which hashtags appear the most in these tweets?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "H_WIeNhf-jHR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Drop null values from hashtags column\n",
        "df_hashtags = df_with_metadata['hashtags'].dropna()\n",
        "\n",
        "# Join all the text in the list and remove apostrophes\n",
        "all_words_hastags = ' '.join([text for text in df_hashtags.astype(str)])\n",
        "all_words_hastags = all_words_hastags.replace(\"'\", \"\")\n",
        "\n",
        "# Create word cloud\n",
        "wordcloud = WordCloud(width = 800, height = 500, random_state = 21,\n",
        "                      max_font_size = 110).generate(all_words_hastags)\n",
        "\n",
        "# Plot word cloud\n",
        "print('Word cloud of top hashtags')\n",
        "plt.figure(figsize = (10, 7))\n",
        "plt.imshow(wordcloud, interpolation = \"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UUfe9Yr-jHU",
        "colab_type": "text"
      },
      "source": [
        "Looking at popular hashtags across all categories its seems as though the prominent hashtags include: 'Climatechange','climate','ActOnClimate'\n",
        "These are to be expected as this dataset are tweets related to climate change.\n",
        "\n",
        "Other prominent hashtags include: 'Paris Agreement', 'Trump','MAGA' etc. *This makes it seem as though these tweets are **American** and are **politically related**.*\n",
        "\n",
        "The appearance of the hashtags 'Election night' and 'I'm Voting Because' also makes it seem as though these tweets were **sampled from twitter during the 2016 American presidential election.**\n",
        "\n",
        "Let's have a look at mentions to confirm our hypothesis:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "e_WiZACu-jHU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Drop null values from mentions column\n",
        "df_mentions = df_with_metadata['mentions'].dropna()\n",
        "\n",
        "# Join all the text in the list and remove apostrophes\n",
        "all_words_mentions = ' '.join([text for text in df_mentions.astype(str)])\n",
        "all_words_mentions = all_words_mentions.replace(\"'\", \"\")\n",
        "\n",
        "# Create word cloud\n",
        "wordcloud = WordCloud(width = 800, height = 500, random_state = 21,\n",
        "                    max_font_size = 110).generate(all_words_mentions)\n",
        "\n",
        "# Plot word cloud\n",
        "print('Word cloud of top mentions')\n",
        "plt.figure(figsize = (10, 7))\n",
        "plt.imshow(wordcloud, interpolation = \"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Sv8yOFc-jHX",
        "colab_type": "text"
      },
      "source": [
        "There is a much wider spread of mentions with DonalTrump topping the list. The mentions which are prominent include : **'realDonaldTrump', 'POTUS', 'SenSanders', 'CNN'** etc. Which also makes it seem as though these tweets were taking during the election time.\n",
        "One has to be cautious when analyzing mentions as there are two types of main mentions on Twitter :\n",
        "\n",
        "1) **Where a twitter profile is referred to**\n",
        "\n",
        "2) **Where a twitter profile Retweets something**\n",
        "\n",
        "Since a mention occurs every time a tweet is retweeted, it might be worth looking into the mentions of only the retweets if time had allowed.\n",
        "\n",
        "Let's have a look at character count distribution for these tweets:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbMM97Xw-jHY",
        "colab_type": "text"
      },
      "source": [
        "#### Character and word count"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "7UdsekCG-jHZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Display distribution of total characters\n",
        "sns.distplot(df_with_metadata['char_count'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oANTIVjv-jHd",
        "colab_type": "text"
      },
      "source": [
        "We can see from this plot that most of the tweets are using the full amount of characters allowed (140). We will see how these changes in each category. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjC-yvJG-jHg",
        "colab_type": "text"
      },
      "source": [
        "### EDA for categories\n",
        "\n",
        "Let's dig a bit deeper into the tweets in each category to see if we can find out the reason they were classified in this manner as well as seeing if we can identify any similarities.\n",
        "\n",
        "We start by separating the dataframes to analyze each category and looking at the most common mentions and hashtags."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "GYBMgsA7-jHg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create dataframes for each category\n",
        "df_negative_tweets = df_with_metadata[df['sentiment'] == -1]\n",
        "df_neutral_tweets = df_with_metadata[df['sentiment'] == 0]\n",
        "df_positive_tweets = df_with_metadata[df['sentiment'] == 1]\n",
        "df_news_tweets = df_with_metadata[df['sentiment'] == 2]\n",
        "df_negative_tweets.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fk-Mf7GE-jHk",
        "colab_type": "text"
      },
      "source": [
        "#### Most common mentions\n",
        "We look at the most common mentions per category."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "B-AwXwcK-jHl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Drop null values from mentions column in negative tweets\n",
        "df_neg_mentions = df_negative_tweets['mentions'].dropna()\n",
        "\n",
        "# Join all the text in the list and remove apostrophes\n",
        "all_words_mentions_neg = ' '.join([text for text in df_neg_mentions.astype(str)])\n",
        "all_words_mentions_neg = all_words_mentions_neg.replace(\"'\", \"\")\n",
        "\n",
        "# Create word cloud\n",
        "wordcloud1 = WordCloud(width = 800, height = 500, random_state = 21,\n",
        "                       max_font_size = 110).generate(all_words_mentions_neg)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Drop null values from mentions column in neutral tweets\n",
        "df_neut_mentions = df_neutral_tweets['mentions'].dropna()\n",
        "\n",
        "# Join all the text in the list and remove apostrophes\n",
        "all_words_mentions_neut = ' '.join([text for text in df_neut_mentions.astype(str)])\n",
        "all_words_mentions_neut = all_words_mentions_neut.replace(\"'\", \"\")\n",
        "\n",
        "# Create word cloud\n",
        "wordcloud2 = WordCloud(width = 800, height = 500, random_state = 21,\n",
        "                       max_font_size = 110).generate(all_words_mentions_neut)\n",
        "\n",
        "\n",
        "\n",
        "# Drop null values from mentions column in positive tweets\n",
        "df_pos_mentions = df_positive_tweets['mentions'].dropna()\n",
        "\n",
        "# Join all the text in the list and remove apostrophes\n",
        "all_words_mentions_pos = ' '.join([text for text in df_pos_mentions.astype(str)])\n",
        "all_words_mentions_pos = all_words_mentions_pos.replace(\"'\",\"\")\n",
        "\n",
        "# Create word cloud\n",
        "wordcloud3 = WordCloud(width = 800, height = 500, random_state = 21,\n",
        "                       max_font_size = 110).generate(all_words_mentions_pos)\n",
        "\n",
        "\n",
        "\n",
        "# Drop null values from mentions column in news tweets\n",
        "df_news_mentions = df_news_tweets['mentions'].dropna()\n",
        "\n",
        "# Join all the text in the list and remove apostrophes\n",
        "all_words_mentions_news = ' '.join([text for text in df_news_mentions.astype(str)])\n",
        "all_words_mentions_news = all_words_mentions_news.replace(\"'\",\"\")\n",
        "\n",
        "# Create word cloud\n",
        "wordcloud4 = WordCloud(width = 800, height = 500, random_state = 21,\n",
        "                       max_font_size = 110).generate(all_words_mentions_news)\n",
        "\n",
        "\n",
        "\n",
        "# Plot all 4 word clouds to compare\n",
        "fig, axes = plt.subplots(nrows = 2, ncols = 2, figsize = (15, 11))\n",
        "fig.suptitle('Mentions')\n",
        "\n",
        "# Negative mention word cloud\n",
        "axes[0,0].imshow(wordcloud1, interpolation = \"bilinear\")\n",
        "axes[0,0].axis('off')\n",
        "axes[0,0].set_title('NEGATIVE')\n",
        "\n",
        "# Neutral mention word cloud\n",
        "axes[1,0].imshow(wordcloud2, interpolation = \"bilinear\")\n",
        "axes[1,0].axis('off')\n",
        "axes[1,0].set_title('NEUTRAL')\n",
        "\n",
        "# Positive mention word cloud\n",
        "axes[0,1].imshow(wordcloud3, interpolation = \"bilinear\")\n",
        "axes[0,1].axis('off')\n",
        "axes[0,1].set_title('POSITIVE')\n",
        "\n",
        "# News mention word cloud\n",
        "axes[1,1].imshow(wordcloud4, interpolation = \"bilinear\")\n",
        "axes[1,1].axis('off')\n",
        "axes[1,1].set_title('NEWS')\n",
        "fig.tight_layout()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofKo21OH-jHp",
        "colab_type": "text"
      },
      "source": [
        "#### Most common hastags\n",
        "Now we look at the most common hashtags per category."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "VsmIR4Ho-jHp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Drop null values from hastags column in negative tweets\n",
        "df_neg_hashtags = df_negative_tweets['hashtags'].dropna()\n",
        "\n",
        "# Join all the text in the list and remove apostrophes\n",
        "all_words_hashtags_neg = ' '.join([text for text in df_neg_hashtags.astype(str)])\n",
        "all_words_hashtags_neg = all_words_hashtags_neg.replace(\"'\", \"\")\n",
        "\n",
        "#Create word cloud\n",
        "wordcloud1 = WordCloud(width = 800, height = 500, random_state = 21,\n",
        "                       max_font_size = 110).generate(all_words_hashtags_neg)\n",
        "\n",
        "\n",
        "\n",
        "# Drop null values from hastags column in neutral tweets\n",
        "df_neut_hashtags = df_neutral_tweets['hashtags'].dropna()\n",
        "\n",
        "# Join all the text in the list and remove apostrophes\n",
        "all_words_hashtags_neut = ' '.join([text for text in df_neut_hashtags.astype(str)])\n",
        "all_words_hashtags_neut = all_words_hashtags_neut.replace(\"'\", \"\")\n",
        "\n",
        "# Create word cloud\n",
        "wordcloud2 = WordCloud(width = 800, height = 500, random_state = 21,\n",
        "                       max_font_size = 110).generate(all_words_hashtags_neut)\n",
        "\n",
        "\n",
        "\n",
        "# Drop null values from hastags column in postive tweets\n",
        "df_pos_hashtags = df_positive_tweets['hashtags'].dropna()\n",
        "\n",
        "# Join all the text in the list and remove apostrophes\n",
        "all_words_hashtags_pos = ' '.join([text for text in df_pos_hashtags.astype(str)])\n",
        "all_words_hashtags_pos = all_words_hashtags_pos.replace(\"'\", \"\")\n",
        "\n",
        "#Create word cloud\n",
        "wordcloud3 = WordCloud(width = 800, height = 500, random_state = 21,\n",
        "                       max_font_size = 110).generate(all_words_hashtags_pos)\n",
        "\n",
        "\n",
        "\n",
        "# Drop null values from hastags column in news tweets\n",
        "df_news_hashtags = df_news_tweets['hashtags'].dropna()\n",
        "\n",
        "# Join all the text in the list and remove apostrophes\n",
        "all_words_hashtags_news = ' '.join([text for text in df_news_hashtags.astype(str)])\n",
        "all_words_hashtags_news = all_words_hashtags_news.replace(\"'\", \"\")\n",
        "\n",
        "# Create word cloud\n",
        "wordcloud4 = WordCloud(width = 800, height = 500, random_state = 21,\n",
        "                       max_font_size = 110).generate(all_words_hashtags_news)\n",
        "\n",
        "\n",
        "\n",
        "# Plot all 4 word clouds to compare\n",
        "fig, axes = plt.subplots(nrows = 2, ncols = 2, figsize = (15, 11))\n",
        "fig.suptitle('Hashtags')\n",
        "\n",
        "# Negative hashtag word cloud\n",
        "axes[0,0].imshow(wordcloud1, interpolation = \"bilinear\")\n",
        "axes[0,0].axis('off')\n",
        "axes[0,0].set_title('NEGATIVE')\n",
        "\n",
        "# Neutral hashtag word cloud\n",
        "axes[1,0].imshow(wordcloud2, interpolation = \"bilinear\")\n",
        "axes[1,0].axis('off')\n",
        "axes[1,0].set_title('NEUTRAL')\n",
        "\n",
        "# Positive hashtag word cloud\n",
        "axes[0,1].imshow(wordcloud3, interpolation = \"bilinear\")\n",
        "axes[0,1].axis('off')\n",
        "axes[0,1].set_title('POSITIVE')\n",
        "\n",
        "# News hashtag word cloud\n",
        "axes[1,1].imshow(wordcloud4, interpolation = \"bilinear\")\n",
        "axes[1,1].axis('off')\n",
        "axes[1,1].set_title('NEWS')\n",
        "fig.tight_layout()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91frBBH6-jHu",
        "colab_type": "text"
      },
      "source": [
        "#### Most Important Words\n",
        "\n",
        "Let's put together a dataframe of the top words, hashtags and mentions so that we can see what words are influencing each category."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "5SsvAYvn-jHu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Word frequency function\n",
        "def word_freq(clean_text_list, top_n):\n",
        "    '''\n",
        "    This function returns a dataframe with the most common words\n",
        "    in the text and the count of their frequency. It takes in a\n",
        "    list of clean text, and top n frequency.\n",
        "    '''\n",
        "    flat = [item for sublist in clean_text_list for item in sublist]\n",
        "    with_counts = Counter(flat)\n",
        "    top = with_counts.most_common(top_n)\n",
        "    word = [each[0] for each in top]\n",
        "    num = [each[1] for each in top]\n",
        "    return pd.DataFrame([word, num]).T\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "L7DveOo3-jHy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # add borders to dataframes\n",
        "# %%HTML\n",
        "# <style type=\"text/css\">\n",
        "# table.dataframe td, table.dataframe th {\n",
        "#     border: 1px  black solid !important;\n",
        "#   color: black !important;\n",
        "# }\n",
        "# </style>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "do7dNO-b-jH3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use word_freq function to retrieve the most common words\n",
        "\n",
        "# Top 20 most frequent words\n",
        "topn = 20\n",
        "\n",
        "# Word frequency of all sentiments\n",
        "all_words_list = df_with_metadata['cleaned_text'].tolist()\n",
        "all_top = word_freq(all_words_list, topn)\n",
        "\n",
        "# Word frequency of negative sentiments\n",
        "neg_words_list = df_negative_tweets['cleaned_text'].tolist()\n",
        "neg_top = word_freq(neg_words_list, topn)\n",
        "\n",
        "# Word frequency of neutral sentiments\n",
        "neut_words_list = df_neutral_tweets['cleaned_text'].tolist()\n",
        "neut_top = word_freq(neut_words_list, topn)\n",
        "\n",
        "# Word frequency of positive sentiments\n",
        "pos_words_list = df_positive_tweets['cleaned_text'].tolist()\n",
        "pos_top = word_freq(pos_words_list, topn)\n",
        "\n",
        "# Word frequency of news sentiments\n",
        "news_words_list = df_news_tweets['cleaned_text'].tolist()\n",
        "news_top = word_freq(news_words_list, topn)\n",
        "\n",
        "# Create new dataframe from the top words\n",
        "df_top = pd.concat([all_top,neg_top, neut_top, pos_top, news_top], axis = 1)\n",
        "cols = ['All','Count','Negative', 'Count', 'Neutral',\n",
        "        'Count', 'Positive', 'Count', 'News', 'Count']\n",
        "df_top.columns = cols\n",
        "\n",
        "# Return dataframe with top words\n",
        "df_top\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SCzjrWr-jH7",
        "colab_type": "text"
      },
      "source": [
        "As we can see :\n",
        "\n",
        "- for all words : **'climate', 'change','rt', 'global',and 'warming'** all are at the top of the word counts. These are top   occurences throughout all categories.\n",
        "\n",
        "- for negative words : **'science', 'cause','real', and 'scam'** stand out as top words that are distinct to negative.\n",
        "\n",
        "- for news words : **'fight', 'epa','pruit', 'scientist',and 'new'** stand out as top words that are distinct to news.\n",
        "\n",
        "How we address these words in our model is important as they could carry importance in different categories as well as influence predictions.\n",
        "\n",
        "We can see that the positive sentiment has a higher number of occurences for top words due to the imbalance of positive tweets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ut_RRUtC-jH9",
        "colab_type": "text"
      },
      "source": [
        "#### Most Important Hashtags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "2RLvl10B-jH-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use word_freq function to retrieve the most common hastags\n",
        "\n",
        "# Top 20 most common hashtags\n",
        "topn = 20\n",
        "\n",
        "# Hashtag frequency of all sentiments\n",
        "all_words_list_hashtags = df_with_metadata['hashtags'].dropna().tolist()\n",
        "all_top_hashtags = word_freq(all_words_list_hashtags, topn)\n",
        "\n",
        "# Hashtag frequency of negative sentiments\n",
        "neg_words_list_hashtags = df_negative_tweets['hashtags'].dropna().tolist()\n",
        "neg_top_hashtags = word_freq(neg_words_list_hashtags, topn)\n",
        "\n",
        "# Hashtag frequency of neutral sentiments\n",
        "neut_words_list_hashtags = df_neutral_tweets['hashtags'].dropna().tolist()\n",
        "neut_top_hashtags = word_freq(neut_words_list_hashtags, topn)\n",
        "\n",
        "# Hashtag frequency of positive sentiments\n",
        "pos_words_list_hashtags = df_positive_tweets['hashtags'].dropna().tolist()\n",
        "pos_top_hashtags = word_freq(pos_words_list_hashtags, topn)\n",
        "\n",
        "# Hashtag frequency of news sentiments\n",
        "news_words_list_hashtags = df_news_tweets['hashtags'].dropna().tolist()\n",
        "news_top_hashtags = word_freq(news_words_list_hashtags, topn)\n",
        "\n",
        "# Create new dataframe from the top hashtags\n",
        "df_top = pd.concat([all_top_hashtags,neg_top_hashtags,neut_top_hashtags,\n",
        "                    pos_top_hashtags, news_top_hashtags], axis = 1)\n",
        "cols = ['All','Count','Negative', 'Count', 'Neutral',\n",
        "        'Count', 'Positive', 'Count', 'News', 'Count']\n",
        "df_top.columns = cols\n",
        "\n",
        "# Return dataframe with top hashtags\n",
        "df_top\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxZShOHJ-jIB",
        "colab_type": "text"
      },
      "source": [
        "It looks like there are not many repetative hashtags being used in the Negative and the Neutral tweets (probably due to them having the lowest number of samples in the dataset)\n",
        "\n",
        "News and Positive on the other hand have quite a few repeat hashtags. These top hashtags could be used as heavier weighted predictors. Specifically for news."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zKfT17y-jIB",
        "colab_type": "text"
      },
      "source": [
        "#### Most Important Mentions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "OWnj-enm-jIC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use word_freq function to retrieve the most common mentions\n",
        "\n",
        "# Top 20 most common mentions\n",
        "topn = 20\n",
        "\n",
        "# Mention frequency of all sentiments\n",
        "all_words_list_mentions = df_with_metadata['mentions'].dropna().tolist()\n",
        "all_top_mentions = word_freq(all_words_list_mentions, topn)\n",
        "\n",
        "# Mention frequency of negative sentiments\n",
        "neg_words_list_mentions = df_negative_tweets['mentions'].dropna().tolist()\n",
        "neg_top_mentions = word_freq(neg_words_list_mentions, topn)\n",
        "\n",
        "# Mention frequency of neutral sentiments\n",
        "neut_words_list_mentions = df_neutral_tweets['mentions'].dropna().tolist()\n",
        "neut_top_mentions = word_freq(neut_words_list_mentions, topn)\n",
        "\n",
        "# Mention frequency of positive sentiments\n",
        "pos_words_list_mentions = df_positive_tweets['mentions'].dropna().tolist()\n",
        "pos_top_mentions = word_freq(pos_words_list_mentions, topn)\n",
        "\n",
        "# Mention frequency of news sentiments\n",
        "news_words_list_mentions = df_news_tweets['mentions'].dropna().tolist()\n",
        "news_top_mentions = word_freq(news_words_list_mentions, topn)\n",
        "\n",
        "# Create new dataframe from the top hashtags\n",
        "df_top = pd.concat([all_top_mentions,neg_top_mentions, neut_top_mentions,\n",
        "                    pos_top_mentions, news_top_mentions], axis = 1)\n",
        "cols = ['All','Count','Negative', 'Count', 'Neutral',\n",
        "        'Count', 'Positive', 'Count', 'News', 'Count']\n",
        "df_top.columns = cols\n",
        "\n",
        "# Return dataframe with top hashtags\n",
        "df_top\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wT8FzND-jIG",
        "colab_type": "text"
      },
      "source": [
        "From our research - most of these mentions occur in the retweets. Similar to the hashtags, the negative and neutral tweets have low mention ocurrences. It would be a good idea to put importance of these mentions as there are high occurence of certain mentions in different categories. Notably :\n",
        "\n",
        "- @realDonaldTrump at the top of the positive list - probably due to alot of tweets being directed towards him\n",
        "- @thehill and @CNN at the top of the News list which is to be expected and could be used for predictions\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yr_-oNNO-jIH",
        "colab_type": "text"
      },
      "source": [
        "### EDA Summary \n",
        "Let's summarise a few key points of what we have found so far:\n",
        "\n",
        "1) About 60% of these tweets are positive towards climate change - This indicates an imbalanced training dataset.\n",
        "\n",
        "2) This data seems to be taken from Americans around the time of the 2016 US presidential elections.\n",
        "\n",
        "3) @realDonaldTrump is the top mentioned account \n",
        "\n",
        "4) 'Climatechange', 'climate', and 'Trump' are the three most used hashtags\n",
        "\n",
        "5) The full character limit of 140 characters was used in the majority of these tweets.\n",
        "\n",
        "6) Retweets create duplicate data points in the dataset\n",
        "\n",
        "7) Top organisations include renowned climate change activists which could be useful from a business networking perspective"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuTr91AV-jIH",
        "colab_type": "text"
      },
      "source": [
        "# 8. Balancing dataset <a name=\"balance\"></a>\n",
        "\n",
        "[Return to top](#top) <br><br>\n",
        "\n",
        "Unbalanced datasets are characterized by one class in the data set outnumbering the other classes in the data. \n",
        "\n",
        "This is important as machine learning models are accuracy driven. This means that models will try to reduce the error even at the cost of the minority classes. As a result models will tend to see the error as carrying the same cost across all classes in the dataset. That's why balancing our data is important. \n",
        "\n",
        "We have chosen three types of sampling to try:\n",
        "\n",
        "1) Downsample majority classes<br>\n",
        "2) Upsample minority classes<br>\n",
        "3) Downsampling majority classes and Upsampling minority classes\n",
        " \n",
        "\n",
        "But first, lets start by looking at our data in order to see what classes will be affected\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSEFGkj0-jII",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot by sentimental classes\n",
        "sentiment_class_count = df_with_metadata[['sentiment',\n",
        "                                          'message']].groupby('sentiment').count()\n",
        "\n",
        "sentiment_class_count.sort_values('message', ascending=False).plot(kind='bar')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "howfVc4u-jIM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# View dataframe with count of sentiments\n",
        "df_with_metadata[['sentiment', 'message']].groupby('sentiment').count()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KA26ZlU-jIQ",
        "colab_type": "text"
      },
      "source": [
        "As we can see from this overview, the data is heavily imbalanced in favour of the pro(1) class.\n",
        "\n",
        "Knowing this allows us to safely assume that unless the model used is robust, the model will most likely have a hard time identifying our minority classes on unseen data. To fix this we can try downsampling\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUK9qFJA-jIS",
        "colab_type": "text"
      },
      "source": [
        "#### Downsample\n",
        "\n",
        "To downsample our data we need to get the count of our minority class. This tells us the size we should aim for across our dataset. We then only choose enough random observations across our major classes to achieve a balanced dataset.\n",
        "\n",
        "Lets see this in action:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "Cw9IT7WO-jIT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a dataframe for each sentiment\n",
        "pro = df_with_metadata[df_with_metadata['sentiment'] == 1]\n",
        "news = df_with_metadata[df_with_metadata['sentiment'] == 2]\n",
        "neutral = df_with_metadata[df_with_metadata['sentiment'] == 0]\n",
        "anti = df_with_metadata[df_with_metadata['sentiment'] == -1]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "_SfCv8yN-jIW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Downsampling all classes to meet the minority class count - 1204\n",
        "class_size_down = int(len(df_with_metadata[df_with_metadata['sentiment' ] == -1]))\n",
        "print('Downsamplng class size:', class_size_down)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "cFlrXzCV-jIa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Downsampling all classes to meet the minority class count - 1204\n",
        "\n",
        "#Downsample pro - sentiment = 1\n",
        "pro_downsampled = resample(pro,\n",
        "                          replace = False, # Sample without replacement\n",
        "                          n_samples = class_size_down, # Match minority class\n",
        "                          random_state = 27) # Reproducible results\n",
        "\n",
        "#Downsample news - sentiment = 2\n",
        "news_downsampled = resample(news,\n",
        "                          replace = False, # Sample without replacement\n",
        "                          n_samples = class_size_down, # Match minority class\n",
        "                          random_state = 27) # Reproducible results\n",
        "\n",
        "#Downsample neutral - sentiment = 0\n",
        "neutral_downsampled = resample(neutral,\n",
        "                          replace = False, # Sample without replacement\n",
        "                          n_samples = class_size_down, # Match minority class\n",
        "                          random_state = 27) # Reproducible results\n",
        "\n",
        "# Combine downsampled majority class with minority class\n",
        "downsampled = pd.concat([pro_downsampled, news_downsampled,\n",
        "                         neutral_downsampled, anti])\n",
        "\n",
        "# Check new class counts\n",
        "print(downsampled['sentiment'].value_counts())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "In3YOMPc-jId",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Bar graph of balanced sentiment count\n",
        "sns.catplot(x = \"sentiment\", kind = \"count\", edgecolor = \".6\",\n",
        "            palette = \"pastel\",data = downsampled);\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdEdKabl-jIh",
        "colab_type": "text"
      },
      "source": [
        "As we can see our data has been balanced across the dataset. However there is a problem, because of the discrepancy in size between the classes we run the risk of losing valuable information in our downsampled classes. \n",
        "\n",
        "There is another option we can chose from to fix this balancing issue."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c08DEkb8-jIi",
        "colab_type": "text"
      },
      "source": [
        "#### Upsampling\n",
        "\n",
        "There is another option we can choose if we are afraid of losing some of the lexicon offered by our majority classes.\n",
        "\n",
        "Upsampling can be considered the inverse of downsampling. In this case we return the count of our majority class and \"upsample\" the minority classes to this count. To do this we randomly repeat observations across our minority classes to pad the difference.\n",
        "\n",
        "Lets see this in action:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wEfs-8I-jIm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Upsampling- class size is the size of the majority pro(1) class\n",
        "class_size_up = int(len(df_with_metadata[df_with_metadata['sentiment'] == 1]))\n",
        "print('Upsampling class size:', class_size_up)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "yhuVc9wR-jIp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Upsampling minority of all classes to meet the majority class count - 8530\n",
        "\n",
        "#Upsample news - sentiment = 2\n",
        "news_upsampled = resample(news,\n",
        "                          replace = True, # sample without replacement\n",
        "                          n_samples = class_size_up, # match majority class\n",
        "                          random_state=27) # reproducible results\n",
        "\n",
        "#Upsample neutral - sentiment = 0\n",
        "neutral_upsampled = resample(neutral,\n",
        "                          replace = True, # sample without replacement=\n",
        "                          n_samples = class_size_up, # match majority class\n",
        "                          random_state=27) # reproducible results\n",
        "\n",
        "#Upsample anti - sentiment = -1\n",
        "anti_upsampled = resample(anti,\n",
        "                          replace = True, # sample without replacement\n",
        "                          n_samples = class_size_up, # match majority class\n",
        "                          random_state=27) # reproducible results\n",
        "\n",
        "\n",
        "# Combine upsampled minority classes with majority class\n",
        "upsampled = pd.concat([pro, news_upsampled,\n",
        "                       neutral_upsampled, anti_upsampled])\n",
        "\n",
        "# Check new class counts\n",
        "print(upsampled['sentiment'].value_counts())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "xFDgTMpP-jIs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Bar graph of balanced sentiment count\n",
        "sns.catplot(x = \"sentiment\", kind = \"count\", edgecolor = \".6\",\n",
        "            palette = \"pastel\",data = upsampled);\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IF7xPFUR-jIu",
        "colab_type": "text"
      },
      "source": [
        "#### Upsampling minority and downsampling majority\n",
        "\n",
        "As alluded above, as much as  both upsampling and downsampling balance our dataset they come up with other problems. With downsampling we risk losing most of our dataset therefore losing valuable information from the classes that will be downsampled. With upsampling we risk creating a lot of noise in our data since we creating duplicates of the minority classes. Using upsampling means we will have more data of the same observations.\n",
        "\n",
        "Upsampling minority class and downsampling majoriy class is the best of both the above balancing techniques as it finds the middle ground between the two approaches. In order for this approach to work, the class size has to be a value between minority class and majority class size. The class size is set as half the size of the majority class.\n",
        "\n",
        "Lets see this in action:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3STqzlY-jIu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Midsampling - class size is 50% of the majority pro(1) class\n",
        "class_size_mid = int(len(df_with_metadata[df_with_metadata['sentiment']==1])/2)\n",
        "print('Upsampling and downsampling class size:', class_size_mid )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHvIh_lF-jIw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Upsampling minority classes and downsampling majority classes - 4265\n",
        "\n",
        "# Downsample pro - sentiment = 1\n",
        "pro_downsampled = resample(pro,\n",
        "                          replace = False, # sample without replacement\n",
        "                          n_samples = class_size_mid, # half  majority class\n",
        "                          random_state = 27) # reproducible results\n",
        "\n",
        "# Upsample anti - sentiment = -1\n",
        "anti_upsampled = resample(anti,\n",
        "                          replace = True, # sample with replacement\n",
        "                          n_samples = class_size_mid, # half of majority class\n",
        "                          random_state = 27) # reproducible results\n",
        "\n",
        "# Upsample news - sentiment=2\n",
        "news_upsampled = resample(news,\n",
        "                          replace = True, # sample with replacement\n",
        "                          n_samples = class_size_mid, # half of majority class\n",
        "                          random_state = 27) # reproducible results\n",
        "\n",
        "#Upsample neutral - sentiment=0\n",
        "neutral_upsampled = resample(neutral,\n",
        "                          replace = True, # sample with replacement\n",
        "                          n_samples = class_size_mid, # half of majority class\n",
        "                          random_state = 27) # reproducible results\n",
        "\n",
        "# Combine downsampled majority class with minority class\n",
        "upsampled_downsampled = pd.concat([pro_downsampled, anti_upsampled,\n",
        "                                   news_upsampled, neutral_upsampled])\n",
        "\n",
        "# Check new class counts\n",
        "print(upsampled_downsampled['sentiment'].value_counts())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6yCKSEm-jI0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Bar graph of balanced sentiment count\n",
        "sns.catplot(x = \"sentiment\", kind = \"count\", edgecolor = \".6\",\n",
        "            palette = \"pastel\",data = upsampled_downsampled);\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wy08o8MJ-jI4",
        "colab_type": "text"
      },
      "source": [
        "# 9. Base model <a name=\"base\"></a>\n",
        "\n",
        "[Return to top](#top) <br><br>\n",
        "\n",
        "We run a selection of basic model through our raw data to get a benchmark of the f1 score that we will attempt to improve. The base models include:\n",
        "\n",
        "- Logistic regression\n",
        "- Linear SVC\n",
        "- Gaussian Naive Bayes\n",
        "- Decision tree\n",
        "- Random forest\n",
        "\n",
        "We run each of these models on a variety of combinations of text preprocessing and text cleaning methods and have concluded that we get the best results with the following cleaning methods:\n",
        "- Lemmatize text\n",
        "- Do not remove stop words\n",
        "- Keep hashtags, mentions and retweets\n",
        "- Vectorize text with TF-IDF vectorizer\n",
        "- Normalise text using MaxAbsScaler()\n",
        "\n",
        "Let's have another look at the information we have extracted from the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "PS_yjjN3-jI5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Review metadata df\n",
        "df_with_metadata.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "eW1ZtHSu-jI8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create feature_df by filtering columns from metadata\n",
        "feature_df = df_with_metadata.filter([\n",
        "                    'sentiment','cleaned_text', 'retweet',\n",
        "                    'hashtag_count', 'mention_count', 'char_count',\n",
        "                    'word_count', 'avg_word_length', 'stopword_count'\n",
        "                                     ], axis = 1)\n",
        "\n",
        "# Replace yes and no with 1 and 0 in retweet columns\n",
        "feature_df['retweet'] = feature_df['retweet'].map(dict(yes = 1, no = 0))\n",
        "\n",
        "# String text together for vectorizer\n",
        "feature_df['cleaned_text'] = feature_df['cleaned_text'].apply(lambda x:\n",
        "                                                              ' '.join(word for word in x))\n",
        "\n",
        "# View feature_df\n",
        "feature_df.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IinZD1eX-jJB",
        "colab_type": "text"
      },
      "source": [
        "#### Choosing Vectorizer\n",
        "\n",
        "CountVectorizer vs TF-IDFVectorizer\n",
        "\n",
        "CountVectorizer gives you a vector with the number of times each word appears in the document. This leads to a few problems mainly that common words like “a”, ”the”, ”and”, ”an” etc will appear most of the time and other words that carry the topic information of your document will be less frequent. For instance, if you’re training a classifier to identify documents related to AI you don’t want it to learn words like “a” and “the” because they’ll be in every single document (both related to AI and not related documents). Furthermore, the number of occurrences for these non-topic bearing terms will be significantly higher than any other term. This will force them to have the highest weight in the model simply due to their high occurrence and will skew your model.\n",
        "\n",
        "The way to combat this problem is to use TF-IDF. What TF-IDF does is it balances out the term frequency (how often the word appears in the document) with its inverse document frequency (how often the term appears across all documents in the data set). This means that words like “a” and “the” will have very low scores as they’ll appear in all documents in your set. Rarer words like for instance “machine learning” will be very common in just a handful of documents which talk about computer science or AI. TF-IDF will give higher scores to these words and thus they’ll be the ones that the model identifies as important and tries to learn.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "jq7zDgzO-jJB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Vectorize text and keep addtional features with DataFrameMapper\n",
        "data = feature_df\n",
        "\n",
        "# Use TF-IDF vecotirzer on original 'message' text\n",
        "mapper = DataFrameMapper([\n",
        "     ('cleaned_text', TfidfVectorizer(min_df=2, \n",
        "                                 max_df=0.9,strip_accents='unicode',\n",
        "                                 analyzer='word',\n",
        "                                 ngram_range=(1, 2))),\n",
        "     ('retweet', None),\n",
        "     ('hashtag_count', None),\n",
        "     ('mention_count', None),\n",
        "     ('char_count', None),\n",
        "     ('word_count', None),\n",
        "     ('avg_word_length', None),\n",
        " ])\n",
        "\n",
        "# X features\n",
        "X = mapper.fit_transform(data)\n",
        "\n",
        "# Y label\n",
        "y = y = feature_df['sentiment']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "wM-uJtSN-jJE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split dataset with 20% test size\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, \n",
        "                                                    y, \n",
        "                                                    test_size = 0.2,  \n",
        "                                                    random_state = 27)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "zb0LVHH3-jJG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use MaxAbsScaler() to normalise values\n",
        "max_abs_scaler = preprocessing.MaxAbsScaler()\n",
        "\n",
        "# Fit to and transform X_train\n",
        "X_train = max_abs_scaler.fit_transform(X_train)  \n",
        "\n",
        "# Transform X_test\n",
        "X_test = max_abs_scaler.transform(X_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "4VYexpo0-jJI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Names of base models\n",
        "names = ['Logistic Regression',\n",
        "         'Linear SVC', 'Gaussian Naive Bayes',          \n",
        "         'Decision Tree', 'Random Forest']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "5hcnLYQX-jJK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Classifiers models and some hyperparameters\n",
        "classifiers = [\n",
        "    LogisticRegression(max_iter = 200, multi_class = 'ovr', solver = 'lbfgs'), \n",
        "    LinearSVC(),\n",
        "    GaussianNB(),\n",
        "    DecisionTreeClassifier(max_depth = 5),\n",
        "    RandomForestClassifier(max_depth = 5, n_estimators = 10,\n",
        "                           max_features = 1)   \n",
        "]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "xcz60lzM-jJM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create empty lists and dictionaries\n",
        "results = []\n",
        "\n",
        "models = {}\n",
        "confusion = {}\n",
        "class_report = {}\n",
        "\n",
        "# Iterate through names and classifiers list and fit models\n",
        "for name, clf in zip(names, classifiers):    \n",
        "    print ('Fitting {:s} model...'.format(name))\n",
        "    run_time = %timeit -q -o clf.fit(X_train, y_train)\n",
        "    \n",
        "    # Predictions\n",
        "    print ('... predicting')\n",
        "    y_pred = clf.predict(X_train)   \n",
        "    y_pred_test = clf.predict(X_test)\n",
        "    \n",
        "    # Model accuracy\n",
        "    print ('... scoring')\n",
        "    accuracy  = metrics.accuracy_score(y_train, y_pred)\n",
        "    precision = metrics.precision_score(y_train, y_pred, average = 'weighted')\n",
        "    recall    = metrics.recall_score(y_train, y_pred, average = 'weighted')\n",
        "    \n",
        "    f1        = metrics.f1_score(y_train, y_pred, average = 'weighted')    \n",
        "    f1_test   = metrics.f1_score(y_test, y_pred_test, average = 'weighted')    \n",
        "    \n",
        "    # Save the results to dictionaries\n",
        "    models[name] = clf    \n",
        "    confusion[name] = metrics.confusion_matrix(y_train, y_pred)\n",
        "    class_report[name] = metrics.classification_report(y_train, y_pred)\n",
        "    \n",
        "    results.append([name, accuracy, precision, recall,\n",
        "                    f1, f1_test, run_time.best])\n",
        "\n",
        "# Create dataframe of results    \n",
        "results = pd.DataFrame(results, columns = ['Classifier', 'Accuracy',\n",
        "                                           'Precision', 'Recall', 'F1 Train',\n",
        "                                           'F1 Test', 'Train Time'])\n",
        "\n",
        "# Set index as 'Classifier'\n",
        "results.set_index('Classifier', inplace = True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "zbaBq80A-jJO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Return dataframe of result sorted by f1 values\n",
        "results.sort_values('F1 Train', ascending = False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJsPpWSO-jJQ",
        "colab_type": "text"
      },
      "source": [
        "We plot the result in a bar graph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "-a7Ai3_z-jJQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
        "results.sort_values('F1 Train', ascending=False, inplace=True)\n",
        "results.plot(y=['F1 Test'], kind='bar', ax=ax[0], xlim=[0,1.1], ylim=[0.30,0.92])\n",
        "results.plot(y='Train Time', kind='bar', ax=ax[1])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lgxfg_a1-jJV",
        "colab_type": "text"
      },
      "source": [
        "#### Cross Validation\n",
        "\n",
        "We apply cross validation on the models to avoid overfitting our model to the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "YIA-New5-jJV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cross validation returning accurcy and standard deviation\n",
        "cv = []\n",
        "for name, model in models.items():\n",
        "    print ()\n",
        "    print(name)\n",
        "    scores = cross_val_score(model, X, y, cv = 10)\n",
        "    print(\"Accuracy: {:0.2f} (+/- {:0.4f})\".format(scores.mean(), scores.std()))\n",
        "    cv.append([name, scores.mean(), scores.std() ])\n",
        "    \n",
        "cv = pd.DataFrame(cv, columns = ['Model', 'CV_Mean', 'CV_Std_Dev'])\n",
        "cv.set_index('Model', inplace = True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cTlbESJ-jJX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot results from cross validation\n",
        "cv.plot(y = 'CV_Mean', yerr = 'CV_Std_Dev',kind = 'bar', ylim = [0.65, 0.85])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wORNfc2y-jJZ",
        "colab_type": "text"
      },
      "source": [
        "# 10. Model evaluation & optimisation<a name=\"evaluation\"></a>\n",
        "\n",
        "[Return to top](#top) <br><br>\n",
        "\n",
        "From the cross validation results of the base models, we select the top performing models and run the models on our processed text and additional features. The best performing models include:\n",
        "- SVC\n",
        "- Logistic regression\n",
        "\n",
        "#### Gridsearch and Cross Validation on best perfoming models\n",
        "\n",
        "Grid-search is used to find the optimal hyperparameters of a model which results in the most ‘accurate’ predictions.\n",
        "\n",
        "A model hyperparameter is a characteristic of a model that is external to the model and whose value cannot be estimated from data. The value of the hyperparameter has to be set before the learning process begins. For example, c in Support Vector Machines, k in k-Nearest Neighbors, the number of hidden layers in Neural Networks.\n",
        "\n",
        "Cross Validation is a very useful technique for assessing the effectiveness of your models, particularly in cases where you need to mitigate overfitting. It is also of use in determining the hyperparameters of your model, in the sense that which parameters will result in lowest test error. \n",
        "\n",
        "We will perform gridsearch with crossvalidation on the top two most perfoming models so that we can optimize their results by finding the optimal hyperparameters of each model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiYwBmEp-jJZ",
        "colab_type": "text"
      },
      "source": [
        "#### Grid search and cross validation on SVC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "LQa1QtQB-jJa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Grid search and crossvalidation on SVC\n",
        "nfolds = 3\n",
        "\n",
        "Cs = [0.001, 0.01, 0.1, 1, 0,25, 10]\n",
        "gammas = [0.001, 0.01, 0.1, 1,2]\n",
        "\n",
        "param_grid = {\n",
        "    'C'     : Cs,\n",
        "    'gamma' : gammas\n",
        "    }\n",
        "\n",
        "grid_SVM = GridSearchCV(SVC(), param_grid, scoring = 'f1', cv = nfolds)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "nqhHI_6h-jJd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fit optimised SVC\n",
        "grid_SVM.fit(X_train, y_train)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "3RYbrcj6-jJg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Form a prediction set\n",
        "predictions = grid_SVM.predict(X_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "ft-GMMpP-jJp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Report the confusion matrix\n",
        "print(metrics.confusion_matrix(y_test,predictions))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "uE3ck5TF-jJt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print a classification report\n",
        "print(metrics.classification_report(y_test,predictions))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "xFpmyyUg-jJv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print the overall accuracy\n",
        "print(metrics.accuracy_score(y_test,predictions))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4Qjx8Wk-jJx",
        "colab_type": "text"
      },
      "source": [
        "#### Grid search and cross validation on logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "08Ut9-q0-jJx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Grid search and crossvalidation on Logistic Regression \n",
        "nfolds = 3\n",
        "\n",
        "C = np.logspace(0, 4, num = 10)\n",
        "penalty = ['l1', 'l2']\n",
        "solver = ['liblinear', 'saga', 'lbfgs']\n",
        "\n",
        "hyperparameters = dict(C = C, penalty = penalty, solver = solver)\n",
        "\n",
        "logistic = linear_model.LogisticRegression()\n",
        "grid_LogReg = GridSearchCV(logistic, hyperparameters, cv = nfolds)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fGn0lK0-jJ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fit optimised logistic regression\n",
        "grid_LogReg.fit(X_train, y_train)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByDCRQ3z-jJ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Form a prediction set\n",
        "predictions = grid_LogReg.predict(X_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-aCUoup-jJ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Report the confusion matrix\n",
        "print(metrics.confusion_matrix(y_test,predictions))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9pxo3Zf-jKA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print a classification report\n",
        "print(metrics.classification_report(y_test,predictions))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "1uK6sYmE-jKE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print the overall accuracy\n",
        "print(metrics.accuracy_score(y_test,predictions))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTVmx263-jKI",
        "colab_type": "text"
      },
      "source": [
        "# 11. Conclusion<a name=\"conclusion\"></a>\n",
        "\n",
        "[Return to top](#top) <br><br>\n",
        "\n",
        "\n",
        "Here we discuss:\n",
        "- Model description\n",
        "- Model performance\n",
        "- What else we can try\n",
        "- Business case value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZ5EtgBmSHjV",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "#### Model Performance\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNKX-Q3DSHNw",
        "colab_type": "text"
      },
      "source": [
        "Several strategies we attempted to improve model performance, ranging from data precessing techniques to clean the tweets, data balancing strategies, cross validation and grid search for the best values for model hyperparameters.\n",
        "\n",
        "On the whole the models performed better on the uncleaned data. Data balancing strategies yielded  little to no improvement on model performance. A few models that were tried resulted in overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-3d9YiDVSL6",
        "colab_type": "text"
      },
      "source": [
        "#### What else we can try"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ue8PgmIpVXfh",
        "colab_type": "text"
      },
      "source": [
        "Language models and the use of neural networks were two other strategies that we wanted to implement, to see how the performance of model improves with the use of a language model, amnd how a neural network performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peS01w3zAaEo",
        "colab_type": "text"
      },
      "source": [
        "## Business case value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8iszgKIApnX",
        "colab_type": "text"
      },
      "source": [
        "From the above analysis the story that is emerges is fairly clear; the sentiment from the negative class of tweets is that of individuals who consider the science of climate change as being a hoax. Seeing that the debate has also become ideological, it would probably be best to tailor a message to this group that does not emphasize the environmental friendliness and sustainability aspects of the products and services, but rather a message that speaks to product features and price etc, would be the best approach when targeting this group.\n",
        "\n",
        "On the other hand, individuals from the positive class of tweets certainly believe in climate change, it is however not clear whether these individuals in their daily lives necessarily make decisions based on the environmental friendliness and sustainability of the products and services they purchase. Emphasizing a message of environmental friendliness and sustainability within this group, will not negatively impact how the products and services are received.\n",
        "\n",
        "\n",
        "There are organisations that are mentioned in the tweets, a number of which share the same values and ideals when it comes to protecting the environment, who have a substantial membership and following on social media of individuals who share the same values and ideals. The formation of potential partnerships with these organisations could lead to brand exposure with individuals who in their daily lives make conscious decisions with regards to the products and services they purchase.\n",
        "\n",
        "It's our recommendations that the latter strategy of persuing partnerships with like minded organisations will yield the best results, in terms of finding a group of potential customers who share the same values and ideals, and would be likely to purchase your products and services."
      ]
    }
  ]
}